{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Copy of Copy of 113Copy of WCCGAN-GP_bus118.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Vrgk7n61zeH9jFvr6wRCRg9ml79nFYbP",
      "authorship_tag": "ABX9TyPtYDRNCfARu5ok/rLpfDmS"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1zWcmmqR61y"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from matplotlib import pyplot as plt\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from numpy.random import randn\n",
        "from scipy.io import loadmat\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import load_model,Sequential,Model\n",
        "import math\n",
        "import time\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D,Lambda,Concatenate,Dropout\n",
        "import pdb; \n",
        "import scipy\n",
        "from keras.constraints import Constraint\n",
        "from scipy.stats import truncnorm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD1C5EnMl-tU"
      },
      "source": [
        "#Data Preprocessing Functions\n",
        "\n",
        "'''\n",
        "#This is the function to format the generators' cost\n",
        "'''\n",
        "'''\n",
        "#This is the function to format the generators' cost\n",
        "'''\n",
        "def get_gencost(number_of_buses,mat_gen,mat_gencost):\n",
        "  gen_ids = mat_gen[:,0]-1\n",
        "  c1=np.zeros(number_of_buses)\n",
        "  c2=np.zeros(number_of_buses)\n",
        "  j=0\n",
        "  for i in range(0,number_of_buses):\n",
        "    if i in gen_ids:\n",
        "      c1[i] = mat_gencost[j,5]\n",
        "      c2[i] = mat_gencost[j,4]\n",
        "      j=j+1\n",
        "  tf_c1 = tf.convert_to_tensor(c1,dtype='float32')\n",
        "  tf_c2 = tf.convert_to_tensor(c2,dtype='float32')\n",
        "  return tf_c1,tf_c2\n",
        "\n",
        "'''\n",
        "#This is the function to get and format the upper and lower bound \n",
        "#of active&reactive power at each buse\n",
        "'''\n",
        "def get_pq_bound(number_of_buses, mat_gen):\n",
        "  number_of_gens = mat_gen.shape[0]\n",
        "  p_upper = np.zeros(number_of_buses)\n",
        "  q_upper = np.zeros(number_of_buses)\n",
        "  q_lower = np.zeros(number_of_buses)\n",
        "  j=0\n",
        "  for i in range(0,number_of_buses):\n",
        "    if i in (mat_gen[:,0]-1):\n",
        "      #pdb.set_trace()\n",
        "      p_upper[i]=mat_gen[j,8]\n",
        "      q_upper[i]=mat_gen[j,3]\n",
        "      q_lower[i]=mat_gen[j,4]\n",
        "      j=j+1\n",
        "  tf_p_upper = tf.convert_to_tensor(p_upper/baseMVA,dtype='float32')\n",
        "  tf_q_upper = tf.convert_to_tensor(q_upper/baseMVA,dtype='float32')\n",
        "  tf_q_lower = tf.convert_to_tensor(q_lower/baseMVA,dtype='float32')\n",
        "  return tf_p_upper,tf_q_upper,tf_q_lower\n",
        "\n",
        "'''\n",
        "#This is the function to get and format the upper and lower bound \n",
        "#of voltage magnitude&angle at each buse\n",
        "'''\n",
        "def get_vm_va_bound(mat_load):\n",
        "  vm_lower = mat_load['aa'][:,-1]\n",
        "  vm_upper = mat_load['aa'][:,-2]\n",
        "  vm_lower[0]=1\n",
        "  vm_upper[0]=1\n",
        "  va_lower = np.ones(118)*(-180)\n",
        "  va_upper = np.ones(118)*(180)\n",
        "  va_lower[0]=0\n",
        "  va_upper[0]=0\n",
        "  tf_vm_upper = tf.convert_to_tensor(vm_upper,dtype='float32')\n",
        "  tf_vm_lower = tf.convert_to_tensor(vm_lower,dtype='float32')\n",
        "  tf_va_upper = tf.convert_to_tensor(va_upper,dtype='float32')\n",
        "  tf_va_lower = tf.convert_to_tensor(va_lower,dtype='float32')\n",
        "  return tf_vm_upper,tf_vm_lower,tf_va_upper,tf_va_lower"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPmX0AtmSDw4"
      },
      "source": [
        "'''\n",
        "#This is the block to load data from matlab, and cvs dataset\n",
        "#Then process them ready for the neural network training\n",
        "'''\n",
        "#Load grid data from matlab\n",
        "mat_y = loadmat('/content/drive/MyDrive/Phd/opf_gan/Y_bus118.mat')\n",
        "mat_gen = loadmat('/content/drive/MyDrive/Phd/opf_gan/IEEE118_gen.mat')['gen118']\n",
        "mat_load = loadmat('/content/drive/MyDrive/Phd/opf_gan/IEEE118_load.mat')\n",
        "#mat_gencost = pd.read_csv('new_cost.csv').values[:,1:]\n",
        "mat_gencost = loadmat('/content/drive/MyDrive/Phd/opf_gan/case118_gencost.mat')['gencost']\n",
        "\n",
        "#Load dataset\n",
        "condition_part1 = loadmat('/content/drive/MyDrive/Phd/opf_gan/new_case118_feasible_condition_25674.mat')['conditions_list']\n",
        "solution_part1 = loadmat('/content/drive/MyDrive/Phd/opf_gan/new_case118_feasible_points_25674.mat')['datapoints_list']\n",
        "condition_part2 = loadmat('/content/drive/MyDrive/Phd/opf_gan/new_case118_feasible_condition_442.mat')['conditions_list']\n",
        "solution_part2 = loadmat('/content/drive/MyDrive/Phd/opf_gan/new_case118_feasible_points_442.mat')['datapoints_list']\n",
        "condition_part3 = loadmat('/content/drive/MyDrive/Phd/opf_gan/new_case118_feasible_condition_380.mat')['conditions_list']\n",
        "solution_part3 = loadmat('/content/drive/MyDrive/Phd/opf_gan/new_case118_feasible_points_380.mat')['datapoints_list']\n",
        "#condition_part4 = loadmat('/content/drive/MyDrive/Phd/opf_gan/new_case118_feasible_condition_14458.mat')['conditions_list']\n",
        "#solution_part4 = loadmat('/content/drive/MyDrive/Phd/opf_gan/new_case118_feasible_points_14458.mat')['datapoints_list']\n",
        "#condition_part5 = loadmat('/content/drive/MyDrive/Phd/opf_gan/new_case118_feasible_condition_314.mat')['conditions_list']\n",
        "#solution_part5 = loadmat('/content/drive/MyDrive/Phd/opf_gan/new_case118_feasible_points_314.mat',squeeze_me=False)['datapoints_list']\n",
        "\n",
        "opf_condition = loadmat('/content/drive/MyDrive/Phd/opf_gan/case118_opf_condition_19796.mat')['conditions_list']\n",
        "opf_points = loadmat('/content/drive/MyDrive/Phd/opf_gan/case118_opf_condition_19796.mat')['conditions_list']\n",
        "#Data preprocessing\n",
        "Y_bus = mat_y['Y_bus118'].toarray().astype('complex64')\n",
        "baseMVA = 100\n",
        "case118_pload = mat_load['aa'][:,2]/baseMVA\n",
        "case118_qload = mat_load['aa'][:,3]/baseMVA\n",
        "real_conditions = np.concatenate([condition_part1,condition_part2,condition_part3],axis=0)/baseMVA\n",
        "real_points = np.concatenate([solution_part1,solution_part2,solution_part3],axis=0)\n",
        "real_vm = real_points[:,0:118]\n",
        "real_va = real_points[:,118:236]\n",
        "matpower_p = real_points[:,236:290]\n",
        "matpower_q = real_points[:,290:]\n",
        "real_p = np.zeros(real_vm.shape)\n",
        "real_q = np.zeros(real_vm.shape)\n",
        "gen_ids = mat_gen[:,0]-1\n",
        "j=0\n",
        "for i in range(0,118):\n",
        "  if i in gen_ids:\n",
        "    real_p[:,i]=matpower_p[:,j]\n",
        "    real_q[:,i]=matpower_q[:,j]\n",
        "    j=j+1\n",
        "real_p=real_p/baseMVA\n",
        "real_q = real_q/baseMVA\n",
        "opf_real_conditions = opf_condition[0:6000,:]/baseMVA\n",
        "\n",
        "opf_training_conditions = opf_condition[6000:6100,:]/baseMVA\n",
        "\n",
        "tf_p_upper,tf_q_upper,tf_q_lower = get_pq_bound(118, mat_gen)\n",
        "tf_vm_upper,tf_vm_lower,tf_va_upper,tf_va_lower = get_vm_va_bound(mat_load)\n",
        "tf_c1,tf_c2 = get_gencost(118,mat_gen,mat_gencost)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "GZulFcA0pIgF",
        "outputId": "4001c32b-0623-4016-f1a3-8b8214796342"
      },
      "source": [
        "#p_supply_ok\n",
        "tf_c1,tf_c2=get_gencost(118,mat_gen,mat_gencost)\n",
        "cost1=tf.math.multiply(real_p*baseMVA, tf_c1)\n",
        "cost2=tf.math.multiply(K.square(tf.cast(real_p*baseMVA,tf.float32)), tf_c2)\n",
        "suma = tf.reduce_sum(cost1+cost2,axis=1)\n",
        "print(min(suma))\n",
        "plt.hist(suma.numpy(),bins=100)\n",
        "plt.title('prices distribution of training data')\n",
        "plt.xlabel('price')\n",
        "plt.ylabel('# of points')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(59362.766, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '# of points')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf9dacfAXk1L",
        "outputId": "014a44bd-febf-4b84-8a8c-c8d5699c9d84"
      },
      "source": [
        "min(suma)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=13088.887>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G-8jcbJAmTt",
        "outputId": "774ede60-61c7-4a69-f9f2-75ecbbe1f07c"
      },
      "source": [
        "real_p[0,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.60586633e-01, 0.00000000e+00, 0.00000000e+00, 4.95238878e-01,\n",
              "       0.00000000e+00, 8.10118094e-02, 0.00000000e+00, 8.91937939e-01,\n",
              "       0.00000000e+00, 1.20965156e+00, 0.00000000e+00, 3.68768545e-01,\n",
              "       0.00000000e+00, 0.00000000e+00, 3.72576433e-01, 0.00000000e+00,\n",
              "       0.00000000e+00, 8.59869828e-01, 6.89387213e-02, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.30776700e-01,\n",
              "       3.01811163e-01, 2.12642410e+00, 2.63286096e-01, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 1.44331822e-01, 3.79891001e-01,\n",
              "       0.00000000e+00, 6.87674652e-01, 0.00000000e+00, 1.16480197e-01,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.42690935e-01,\n",
              "       0.00000000e+00, 1.00235139e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 4.14558541e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "       8.81365754e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 8.78027462e-01, 3.08465542e-01, 1.74528098e-01,\n",
              "       0.00000000e+00, 0.00000000e+00, 7.01206857e-03, 0.00000000e+00,\n",
              "       9.06316398e-02, 4.60305394e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "       1.35252366e-01, 2.45774900e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       1.02425919e+01, 8.85061567e-01, 0.00000000e+00, 4.37594389e-01,\n",
              "       5.80965688e-01, 5.78086078e-01, 0.00000000e+00, 5.72922829e-02,\n",
              "       4.91226384e-02, 0.00000000e+00, 0.00000000e+00, 1.26921874e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       3.18691634e-01, 0.00000000e+00, 2.49964967e-01, 0.00000000e+00,\n",
              "       2.12289031e+00, 6.59588357e-01, 2.05748069e-01, 2.80764471e-01,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 7.46040680e-01, 5.20458524e-01,\n",
              "       0.00000000e+00, 0.00000000e+00, 3.55160392e-01, 4.08046784e-01,\n",
              "       1.30313154e-01, 0.00000000e+00, 3.07025784e-01, 0.00000000e+00,\n",
              "       0.00000000e+00, 7.20075578e-02, 2.64789019e-01, 6.04188820e-01,\n",
              "       4.96182707e-01, 0.00000000e+00, 0.00000000e+00, 3.27150267e-01,\n",
              "       0.00000000e+00, 0.00000000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDeuxNzfGp_5",
        "outputId": "2c524cf7-5c2a-4c99-e527-5ca46e1050af"
      },
      "source": [
        "print(tf_c1)\n",
        "print(tf_c2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[40.  0.  0. 40.  0. 40.  0. 40.  0. 20.  0. 20.  0.  0. 40.  0.  0. 40.\n",
            " 40.  0.  0.  0.  0. 40. 20. 20. 40.  0.  0.  0. 20. 40.  0. 40.  0. 40.\n",
            "  0.  0.  0. 40.  0. 40.  0.  0.  0. 20.  0.  0. 20.  0.  0.  0.  0. 20.\n",
            " 40. 40.  0.  0. 20.  0. 20. 40.  0.  0. 20. 20.  0.  0. 20. 40.  0. 40.\n",
            " 40. 40.  0. 40. 40.  0.  0. 20.  0.  0.  0.  0. 40.  0. 20.  0. 20. 40.\n",
            " 40. 40.  0.  0.  0.  0.  0.  0. 40. 20.  0.  0. 20. 40. 40.  0. 40.  0.\n",
            "  0. 40. 20. 40. 40.  0.  0. 40.  0.  0.], shape=(118,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.01       0.         0.         0.01       0.         0.01\n",
            " 0.         0.01       0.         0.02222222 0.         0.11764706\n",
            " 0.         0.         0.01       0.         0.         0.01\n",
            " 0.01       0.         0.         0.         0.         0.01\n",
            " 0.04545455 0.03184713 0.01       0.         0.         0.\n",
            " 1.4285715  0.01       0.         0.01       0.         0.01\n",
            " 0.         0.         0.         0.01       0.         0.01\n",
            " 0.         0.         0.         0.5263158  0.         0.\n",
            " 0.04901961 0.         0.         0.         0.         0.20833333\n",
            " 0.01       0.01       0.         0.         0.06451613 0.\n",
            " 0.0625     0.01       0.         0.         0.02557545 0.0255102\n",
            " 0.         0.         0.01936483 0.01       0.         0.01\n",
            " 0.01       0.01       0.         0.01       0.01       0.\n",
            " 0.         0.02096436 0.         0.         0.         0.\n",
            " 0.01       0.         2.5        0.         0.01647446 0.01\n",
            " 0.01       0.01       0.         0.         0.         0.\n",
            " 0.         0.         0.01       0.03968254 0.         0.\n",
            " 0.25       0.01       0.01       0.         0.01       0.\n",
            " 0.         0.01       0.2777778  0.01       0.01       0.\n",
            " 0.         0.01       0.         0.        ], shape=(118,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO2QjsvFVOJn"
      },
      "source": [
        "class ClipConstraint(Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        " \n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn K.clip(weights, -self.clip_value, self.clip_value)\n",
        " \n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}\n",
        "\n",
        "const = ClipConstraint(0.0001)\n",
        "\n",
        "def define_generator(latent_dim):\n",
        "  code = keras.Input(shape=(1,1))\n",
        "  random_noise = keras.Input(shape=(1,latent_dim))\n",
        "  demand = keras.Input(shape=(1,118*2))\n",
        "  concat_layer= Concatenate(axis=-1)([code,random_noise, demand])\n",
        "  hidden = Conv1D(64, (1), activation=None,padding='same')(concat_layer)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  #hidden = Dropout(0.1)(hidden)\n",
        "  hidden = Conv1D(32, (1), activation=None,padding='same')(hidden)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = Conv1D(16, (1), activation=None,padding='same')(hidden)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = Conv1D(8, (1), activation=None,padding='same')(hidden)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  #hidden = Dropout(0.1)(hidden)\n",
        "  hidden = layers.Flatten()(hidden)\n",
        "  g_active = layers.Dense(118,dtype='float32',activation='sigmoid')(hidden)\n",
        "  g_active = tf.math.multiply(g_active,tf_p_upper)\n",
        "  g_reactive = layers.Dense(118,dtype='float32',activation='sigmoid')(hidden)\n",
        "  g_reactive = tf.math.multiply(g_reactive,tf_q_upper-tf_q_lower)\\\n",
        "                                                    +tf_q_lower\n",
        "  bus_vm = layers.Dense(118,dtype='float32',activation='sigmoid')(hidden)\n",
        "  bus_vm = tf.math.multiply(bus_vm,tf_vm_upper-tf_vm_lower)+tf_vm_lower\n",
        "  bus_va = layers.Dense(118,dtype='float32',activation='sigmoid')(hidden)\n",
        "  bus_va = tf.math.multiply(bus_va,tf_va_upper-tf_va_lower)+tf_va_lower\n",
        "  g_model = keras.Model(inputs=[code,random_noise,demand], outputs=[g_active,g_reactive,bus_vm,bus_va])\n",
        "  return g_model\n",
        "\n",
        "def define_qnetwork():\n",
        "  g_active = keras.Input(shape=(1,118))\n",
        "  #cost1 = keras.Input(shape=(118,1))\n",
        "  #cost2 = keras.Input(shape=(118,1))\n",
        "  #concat_layer= Concatenate(axis=1)([cost1,cost2])\n",
        "  hidden = Conv1D(64, (1), activation=None,padding='same')(g_active)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = Conv1D(32, (1), activation=None,padding='same')(hidden)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = Conv1D(8, (1), activation=None,padding='same')(hidden)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = Conv1D(4, (1), activation=None,padding='same')(hidden)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = layers.Flatten()(hidden)\n",
        "  out_layer = layers.Dense(1,dtype='float32',activation='tanh')(hidden)\n",
        "  #define model\n",
        "  q_model = Model(g_active, out_layer)\n",
        "  #q_model = Model([cost1,cost2], out_layer)\n",
        "  return q_model\n",
        "\n",
        "def define_discriminator():\n",
        "  demand = keras.Input(shape=(1,118*2))\n",
        "  g_active = keras.Input(shape=(1,118))\n",
        "  g_reactive = keras.Input(shape=(1,118))\n",
        "  bus_vm = keras.Input(shape=(1,118))\n",
        "  bus_va = keras.Input(shape=(1,118))\n",
        "  concat_layer= Concatenate(axis=-1)([demand,g_active,g_reactive,bus_vm,bus_va])\n",
        "  hidden = Conv1D(64, (1), activation=None,padding='same',kernel_constraint=const)(concat_layer)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  #hidden = layers.MaxPooling1D(pool_size=(3),padding='same')(hidden)\n",
        "  hidden = Conv1D(64, (1), activation=None,padding='same',kernel_constraint=const)(hidden)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = layers.Flatten()(hidden)\n",
        "  out_layer = layers.Dense(2,dtype='float32',activation='linear')(hidden)\n",
        "  #define model\n",
        "  d_model = Model([demand,g_active,g_reactive,bus_vm,bus_va], out_layer)\n",
        "  #compile model\n",
        "  #opt = tf.keras.optimizers.Adam(learning_rate=0.000001, beta_1=0.5)\n",
        "  #d_model.compile(loss='wasserstein_loss', optimizer=opt, metrics=['accuracy'])\n",
        "  return d_model\n",
        "\n",
        "def define_gp_critic():\n",
        "  demand = keras.Input(shape=(1,118*2))\n",
        "  g_active = keras.Input(shape=(1,118))\n",
        "  g_reactive = keras.Input(shape=(1,118))\n",
        "  bus_vm = keras.Input(shape=(1,118))\n",
        "  bus_va = keras.Input(shape=(1,118))\n",
        "  concat_layer= Concatenate(axis=-1)([demand,g_active,g_reactive,bus_vm,bus_va])\n",
        "  #hidden = Conv1D(64, (3), activation=None,padding='same')(concat_layer)\n",
        "  #hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  #hidden = layers.MaxPooling1D(pool_size=(3),padding='same')(hidden)\n",
        "  hidden = Conv1D(128, (1), activation=None,padding='same')(concat_layer)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = Conv1D(32, (1), activation=None,padding='same')(hidden)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = Conv1D(8, (1), activation=None,padding='same')(hidden)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = Conv1D(4, (1), activation=None,padding='same')(hidden)\n",
        "  hidden = tf.keras.layers.LeakyReLU(alpha=0.1)(hidden)\n",
        "  hidden = layers.Flatten()(hidden)\n",
        "  out_layer = layers.Dense(2,dtype='float32',activation='linear')(hidden)\n",
        "  #define model\n",
        "  d_model = Model([demand,g_active,g_reactive,bus_vm,bus_va], out_layer)\n",
        "  return d_model\n",
        "\n",
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "  # make weights in the discriminator not trainable\n",
        "  d_model.trainable = False\n",
        "  [g_active,g_reactive,bus_vm,bus_va] = generator.output\n",
        "  [gen_code, gen_noise, gen_label] = generator.input\n",
        "  gan_output = d_model([tf.reshape(gen_label,[-1,1,118*2]),tf.reshape(g_active,[-1,1,118]),\\\n",
        "                        tf.reshape(g_reactive,[-1,1,118]),tf.reshape(bus_vm,[-1,1,118]),\\\n",
        "                        tf.reshape(bus_va,[-1,1,118])])\n",
        "  # define gan model as taking noise and label and outputting a classification\n",
        "  model = Model([gen_code, gen_noise, gen_label], gan_output)\n",
        "  return model\n",
        "\n",
        "def define_ae(g_model,q_model):\n",
        "  [g_active,g_reactive,bus_vm,bus_va] = g_model.output\n",
        "  [gen_code, gen_noise, gen_label] = g_model.input\n",
        "  #realp_tf = g_active*baseMVA\n",
        "  #cost1=tf.math.multiply(realp_tf, tf_c1)\n",
        "  #cost2=tf.math.multiply(K.square(realp_tf), tf_c2)\n",
        "  q_output = q_model(tf.reshape(g_active,[-1,1,118]))\n",
        "  ae_model = Model([gen_code, gen_noise, gen_label], q_output)\n",
        "  return ae_model\n",
        "\n",
        "def rdm_load_pq_20per(case_pload,case_qload,sample_number):\n",
        "    number_of_bus = case_pload.shape[0]\n",
        "    p_demand = np.zeros([sample_number,number_of_bus])\n",
        "    q_demand = np.zeros([sample_number,number_of_bus])\n",
        "    for i in range(0,number_of_bus):\n",
        "        pi = case_pload[i]\n",
        "        qi = case_qload[i]\n",
        "        if case_pload[i]!=0:\n",
        "          p_mw = np.random.uniform(pi*0.8,pi*1.2,sample_number)\n",
        "          q_mvar = np.random.uniform(qi*0.8,qi*1.2,sample_number)\n",
        "          p_demand[:,i] = p_mw\n",
        "          q_demand[:,i] = q_mvar\n",
        "    return p_demand,q_demand\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "\treturn K.mean(y_true * y_pred)\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  # generate points in the latent space\n",
        "  code_input = np.random.uniform(-1,1,n_samples).reshape([n_samples,1])\n",
        "  x_input = randn(latent_dim * n_samples)\n",
        "  z_input = x_input.reshape(n_samples, latent_dim)\n",
        "  # generate labels\n",
        "  p_demand,q_demand = rdm_load_pq_20per(case118_pload,case118_qload,n_samples)\n",
        "  labels = np.concatenate([p_demand,q_demand],axis=1)\n",
        "  return [code_input,z_input, labels]\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(n_samples):\n",
        "  # choose random instances\n",
        "  ix = np.random.randint(0, 26495, n_samples)\n",
        "  # select images and labels\n",
        "  batch_condition,batch_vm,batch_va,batch_p,batch_q = real_conditions[ix], \\\n",
        "    real_vm[ix], real_va[ix], real_p[ix], real_q[ix]\n",
        "  # generate class labels\n",
        "  y = -np.ones((n_samples, 1))\n",
        "  return batch_condition,batch_vm,batch_va,batch_p,batch_q, y\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tc_input, z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\timages = generator.predict([c_input, z_input, labels_input])\n",
        "\t# create class labels\n",
        "\ty = np.ones((n_samples, 1))\n",
        "\treturn c_input, labels_input,images, y\n",
        "\n",
        "def feasibility_checking(p_demand,q_demand, active_p,reactive_q,vm,va):\n",
        "  generated_size = p_demand.shape[0]\n",
        "  #Step1: calculate power withdraw on each bus\n",
        "  P_out = tf.convert_to_tensor(p_demand-active_p,dtype='float32')\n",
        "  Q_out = tf.convert_to_tensor(q_demand-reactive_q,dtype='float32')\n",
        "  #pdb.set_trace()\n",
        "  #get voltage on each bus\n",
        "  v_r = tf.math.multiply(vm,tf.cos(tf.math.multiply(va,tf.constant(math.pi/180,dtype='float32'))))\n",
        "  v_i = tf.math.multiply(vm,tf.sin(tf.math.multiply(va,tf.constant(math.pi/180,dtype='float32'))))\n",
        "  V = tf.reshape(tf.complex(v_r,v_i),[generated_size,118])\n",
        "\n",
        "  #calculate current\n",
        "  Y_bus_tf = tf.convert_to_tensor(Y_bus)\n",
        "  I = tf.matmul(V,Y_bus_tf)\n",
        "\n",
        "  #calculate power injection on each bus\n",
        "  S_in = tf.math.multiply(V,tf.math.conj(I))\n",
        "  P_in = tf.math.real(S_in)\n",
        "  Q_in = tf.math.imag(S_in)\n",
        "\n",
        "  #evaluate the balance\n",
        "  p_balance = tf.reduce_mean(P_in+P_out,axis=0)\n",
        "  q_balance = tf.reduce_mean(Q_in+Q_out,axis=0)\n",
        "  return p_balance,q_balance\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_test_samples(generator, latent_dim, test_samples):\n",
        "  # generate points in latent space\n",
        "  n_samples = opf_real_conditions_validation.shape[0]\n",
        "  z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
        "  # predict outputs\n",
        "  images = generator.predict([z_input, opf_real_conditions_validation])\n",
        "  # create class labels\n",
        "  y = np.ones((n_samples, 1))\n",
        "  return opf_real_conditions_validation,images, y\n",
        "\n",
        "def optimality_checking(p_opt,q_opt,p_fake,q_fake):\n",
        "  p_mse = K.mean(K.square(p_opt-p_fake))\n",
        "  q_mse = K.mean(K.square(q_opt-q_fake))\n",
        "  return p_mse,q_mse"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hOJaQRzvUmm"
      },
      "source": [
        "latent_dim = 100\n",
        "batch_size = 64\n",
        "half_batch = batch_size//2\n",
        "# create the discriminator\n",
        "#discriminator = load_model('/content/drive/MyDrive/Phd/opf_gan/cgan_discriminator.h5',\\\n",
        " #                          custom_objects={'ClipConstraint': ClipConstraint})\n",
        "#discriminator=define_discriminator()\n",
        "discriminator = define_gp_critic()\n",
        "# create the generator\n",
        "#generator = load_model('/content/drive/MyDrive/Phd/opf_gan/cgan_generator.h5')\n",
        "generator =define_generator(latent_dim)\n",
        "qnetwork = define_qnetwork()\n",
        "# create the gan\n",
        "gan_model = define_gan(generator, discriminator)\n",
        "ae_model = define_ae(generator,qnetwork)\n",
        "loss_d1_list = []\n",
        "loss_d2_list = []\n",
        "loss_g_list = []                                                                              \n",
        "real_acc_list = []\n",
        "fake_acc_list = []\n",
        "dy_dx_list=[]\n",
        "ae_list=[]\n",
        "pbalance_list=[]\n",
        "qbalance_list=[]\n",
        "opt_perr_list=[]\n",
        "opt_qerr_list=[]\n",
        "ae_loss_list=[]\n",
        "gp_list=[]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7IJgd6sZCFG",
        "outputId": "1ed060e1-aced-44a3-e177-1902b9b86648"
      },
      "source": [
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_37 (InputLayer)           [(None, 1, 236)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_38 (InputLayer)           [(None, 1, 118)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_39 (InputLayer)           [(None, 1, 118)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_40 (InputLayer)           [(None, 1, 118)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_41 (InputLayer)           [(None, 1, 118)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 1, 708)       0           input_37[0][0]                   \n",
            "                                                                 input_38[0][0]                   \n",
            "                                                                 input_39[0][0]                   \n",
            "                                                                 input_40[0][0]                   \n",
            "                                                                 input_41[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_49 (Conv1D)              (None, 1, 128)       90752       concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_49 (LeakyReLU)      (None, 1, 128)       0           conv1d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_50 (Conv1D)              (None, 1, 32)        4128        leaky_re_lu_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)      (None, 1, 32)        0           conv1d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_51 (Conv1D)              (None, 1, 8)         264         leaky_re_lu_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)      (None, 1, 8)         0           conv1d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_52 (Conv1D)              (None, 1, 4)         36          leaky_re_lu_51[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)      (None, 1, 4)         0           conv1d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 4)            0           leaky_re_lu_52[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 2)            10          flatten_12[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 95,190\n",
            "Trainable params: 0\n",
            "Non-trainable params: 95,190\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdWiW9p4ut6v",
        "outputId": "20bb1faf-a229-4041-88b9-85158227c45d"
      },
      "source": [
        "iterations = 10000\n",
        "n_critic=20\n",
        "best_pb=0.3\n",
        "best_qb=0.3\n",
        "for i in range(iterations):\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    discriminator.trainable=True\n",
        "    #pdb.set_trace()\n",
        "    #Initializing optimizers:\n",
        "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
        "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
        "    q_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
        "    \n",
        "    '''\n",
        "      Block 1 starts: Training the discriminator for $n_critic times\n",
        "    '''\n",
        "    for _j in range(n_critic):\n",
        "      #Prepare real and fake data\n",
        "      real_batch_demand, real_batch_vm,real_batch_va,real_batch_p,real_batch_q, \\\n",
        "                  real_batch_y = generate_real_samples(half_batch)\n",
        "      [fake_latent_code, fake_latent_noise, _] = \\\n",
        "                  generate_latent_points(latent_dim, half_batch)\n",
        "      [fake_batch_p,fake_batch_q,fake_batch_vm,fake_batch_va] = \\\n",
        "                  generator([tf.reshape(fake_latent_code,[-1,1,1]),\\\n",
        "                  tf.reshape(fake_latent_noise,[-1,1,latent_dim]),\\\n",
        "                  tf.reshape(real_batch_demand,[-1,1,118*2])])\n",
        "      fake_batch_y = np.ones((half_batch, 1))\n",
        "      alpha = K.random_uniform(\n",
        "          shape=[half_batch,1], \n",
        "          minval=0.,\n",
        "          maxval=1.\n",
        "      )\n",
        "      diff_batch_vm = real_batch_vm - fake_batch_vm\n",
        "      diff_batch_va = real_batch_va - fake_batch_va\n",
        "      diff_batch_p = real_batch_p - fake_batch_p\n",
        "      diff_batch_q = real_batch_q - fake_batch_q\n",
        "\n",
        "      inter_batch_vm = real_batch_vm + (alpha*diff_batch_vm)\n",
        "      inter_batch_va = real_batch_va + (alpha*diff_batch_va)\n",
        "      inter_batch_p = real_batch_p + (alpha*diff_batch_p)\n",
        "      inter_batch_q = real_batch_q + (alpha*diff_batch_q)\n",
        "\n",
        "      '''\n",
        "      Calculate the Cost function for discriminator\n",
        "      '''\n",
        "      #Prepare the gradient penalty\n",
        "      inter_d_output = discriminator([tf.reshape(real_batch_demand,[-1,1,118*2]),\\\n",
        "                                tf.reshape(inter_batch_p,[-1,1,118]),\\\n",
        "                                tf.reshape(inter_batch_q,[-1,1,118]),\\\n",
        "                                tf.reshape(inter_batch_vm,[-1,1,118]),\\\n",
        "                                tf.reshape(inter_batch_va,[-1,1,118])])\n",
        "      inter_gradients_p = tape.gradient(inter_d_output, [inter_batch_p,inter_batch_q,inter_batch_vm,inter_batch_va])\n",
        "      rdc_sum = tf.math.reduce_sum(tf.math.reduce_sum(tf.square(inter_gradients_p),axis=0),axis=1)\n",
        "      slopes = K.sqrt(rdc_sum)\n",
        "      gradient_penalty = tf.reduce_mean(tf.square((slopes-1.)))\n",
        "      \n",
        "      #calculate the regular WGAN loss\n",
        "      real_d_output = discriminator([tf.reshape(real_batch_demand,[-1,1,118*2]),\\\n",
        "                                            tf.reshape(real_batch_p,[-1,1,118]),\\\n",
        "                                            tf.reshape(real_batch_q,[-1,1,118]),\\\n",
        "                                            tf.reshape(real_batch_vm,[-1,1,118]),\\\n",
        "                                            tf.reshape(real_batch_va,[-1,1,118])])\n",
        "      fake_d_output = discriminator([tf.reshape(real_batch_demand,[-1,1,118*2]),\\\n",
        "                                            tf.reshape(fake_batch_p,[-1,1,118]),\\\n",
        "                                            tf.reshape(fake_batch_q,[-1,1,118]),\\\n",
        "                                            tf.reshape(fake_batch_vm,[-1,1,118]),\\\n",
        "                                            tf.reshape(fake_batch_va,[-1,1,118])])\n",
        "      d1_loss = K.mean(real_d_output*real_batch_y)\n",
        "      loss_d1_list.append(d1_loss.numpy())\n",
        "      d2_loss = K.mean(fake_d_output*fake_batch_y)\n",
        "      loss_d2_list.append(d2_loss.numpy())\n",
        "      gp_list.append(gradient_penalty.numpy())\n",
        "      d_loss = d1_loss+d2_loss+100*gradient_penalty\n",
        "      d_grads = tape.gradient(d_loss,discriminator.trainable_weights)\n",
        "      d_optimizer.apply_gradients(zip(d_grads, discriminator.trainable_weights))\n",
        "    '''\n",
        "      Block 1 ends.\n",
        "    '''\n",
        "    \n",
        "    '''\n",
        "      Block 2 starts: Training the generator\n",
        "    '''\n",
        "    #Train Generator  \n",
        "    real_batch_demand,_,_,_,_,_ = generate_real_samples(batch_size)\n",
        "    [fake_latent_code, fake_latent_noise, _] = \\\n",
        "                  generate_latent_points(latent_dim, batch_size)\n",
        "    tf_real_batch_demand = tf.keras.backend.variable(real_batch_demand)\n",
        "    tape.watch(tf_real_batch_demand)\n",
        "    [fake_batch_p,fake_batch_q,fake_batch_vm,fake_batch_va] = \\\n",
        "                        generator([tf.reshape(fake_latent_code,[-1,1,1]),\\\n",
        "                        tf.reshape(fake_latent_noise,[-1,1,latent_dim]),\\\n",
        "                        tf.reshape(tf_real_batch_demand,[-1,1,118*2])])\n",
        "    fake_gan_output = discriminator([tf.reshape(real_batch_demand,[-1,1,118*2]),\\\n",
        "                                            tf.reshape(fake_batch_p,[-1,1,118]),\\\n",
        "                                            tf.reshape(fake_batch_q,[-1,1,118]),\\\n",
        "                                            tf.reshape(fake_batch_vm,[-1,1,118]),\\\n",
        "                                            tf.reshape(fake_batch_va,[-1,1,118])])\n",
        "    fake_batch_y = -np.ones((batch_size, 1))\n",
        "    gan_loss = K.mean(fake_gan_output*fake_batch_y)\n",
        "    dy_dx = tf.reduce_mean(tf.square(tape.gradient([fake_batch_p,fake_batch_q,\\\n",
        "                                                    fake_batch_vm,fake_batch_va],\\\n",
        "                                                    tf_real_batch_demand)))\n",
        "    loss_g_list.append(gan_loss.numpy())\n",
        "    dy_dx_list.append(dy_dx.numpy())\n",
        "    discriminator.trainable=False\n",
        "    gan_grads = tape.gradient(gan_loss+100*dy_dx, gan_model.trainable_weights)\n",
        "    g_optimizer.apply_gradients(zip(gan_grads, gan_model.trainable_weights))\n",
        "  \n",
        "    #Training the Q Inference Network\n",
        "    for _k in range(10):\n",
        "      q_output = qnetwork(tf.reshape(fake_batch_p,[-1,1,118]))\n",
        "      q_loss = 10*K.mean(K.square(fake_latent_code-q_output))\n",
        "      ae_loss_list.append(q_loss.numpy())\n",
        "      ae_grads = tape.gradient(q_loss, ae_model.trainable_weights)\n",
        "      q_optimizer.apply_gradients(zip(ae_grads, ae_model.trainable_weights))\n",
        "    '''\n",
        "      Block 2 ends.\n",
        "    '''\n",
        "\n",
        "    print('This is the iter '+str(i)+', the d1 loss is: '+str(d1_loss.numpy())+\\\n",
        "          ', the d2 loss is: '+str(d2_loss.numpy())+\\\n",
        "          ', the g loss is: '+str(gan_loss.numpy())+\\\n",
        "          ', the ae loss is: '+str(q_loss.numpy())+\\\n",
        "          ', the jacobian loss is:'+str(dy_dx.numpy()))\n",
        "  if i%100==0:\n",
        "    generated_size = 6000\n",
        "    c_input, z_input, opf_real_conditions_validation = generate_latent_points(latent_dim, generated_size)\n",
        "    # predict outputs\n",
        "    #c_input_t, z_input_t, _ = generate_latent_points(latent_dim, real_conditions.shape[0])\n",
        "    #[fake_p_t,fake_q_t,fake_vm_t,fake_va_t] = generator.predict([c_input_t, z_input_t, real_conditions])\n",
        "    [fake_p,fake_q,fake_vm,fake_va] = generator.predict([c_input.reshape([-1,1,1]), z_input.reshape([-1,1,latent_dim]),\\\n",
        "                                                         opf_real_conditions_validation.reshape([-1,1,118*2])])\n",
        "    # create class labels\n",
        "    y_fake = np.ones((generated_size, 1))\n",
        "    #pb_training,qb_training=feasibility_checking(real_conditions[:,0:118],real_conditions[:,118:], \\\n",
        "    #                                             fake_p_t,fake_q_t,fake_vm_t,fake_va_t)\n",
        "    pb,qb=feasibility_checking(opf_real_conditions[:,0:118],opf_real_conditions[:,118:], fake_p,fake_q,fake_vm,fake_va)\n",
        "    #po,qo=optimality_checking(opf_real_p_validation*baseMVA,opf_real_q_validation*baseMVA,fake_p*baseMVA,fake_q*baseMVA)\n",
        "    #pb=K.abs(pb)\n",
        "    #qb=K.abs(qb)\n",
        "    pbalance_list.append(K.mean(pb).numpy())\n",
        "    qbalance_list.append(K.mean(qb).numpy())\n",
        "    #print('training:')\n",
        "    #print(K.mean(pb_training).numpy())\n",
        "    #print(K.mean(qb_training).numpy())\n",
        "    #print('validation:')\n",
        "    print(K.mean(pb).numpy())\n",
        "    print(K.mean(qb).numpy())\n",
        "    if (abs(K.mean(pb).numpy())<best_pb) and (abs(K.mean(qb).numpy())<best_qb):\n",
        "      best_pb = abs(K.mean(pb).numpy())\n",
        "      best_qb = abs(K.mean(qb).numpy())\n",
        "      print('We hit the best performance!, the pb='+str(best_pb)+',qb='+str(best_qb))\n",
        "      # save the generator model\n",
        "      generator.save('/content/drive/MyDrive/Phd/opf_gan/118_generator_114c.h5')\n",
        "      discriminator.save('/content/drive/MyDrive/Phd/opf_gan/118_discriminator_114c.h5')\n",
        "      qnetwork.save('/content/drive/MyDrive/Phd/opf_gan/118_qnetwork_114c.h5')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "This is the iter 5098, the d1 loss is: 2806.9375, the d2 loss is: -3284.6875, the g loss is: 3200.6797, the ae loss is: 0.006673227, the jacobian loss is:0.11228012\n",
            "This is the iter 5099, the d1 loss is: 2732.2422, the d2 loss is: -3180.289, the g loss is: 3144.4219, the ae loss is: 0.0061694556, the jacobian loss is:0.089347295\n",
            "This is the iter 5100, the d1 loss is: 2843.4375, the d2 loss is: -3252.2812, the g loss is: 3278.3828, the ae loss is: 0.009126043, the jacobian loss is:0.109736174\n",
            "0.18607065\n",
            "0.6967856\n",
            "This is the iter 5101, the d1 loss is: 2845.4062, the d2 loss is: -3291.7344, the g loss is: 3212.9375, the ae loss is: 0.008255178, the jacobian loss is:0.10155605\n",
            "This is the iter 5102, the d1 loss is: 2924.586, the d2 loss is: -3340.7969, the g loss is: 3319.4375, the ae loss is: 0.007240328, the jacobian loss is:0.11440117\n",
            "This is the iter 5103, the d1 loss is: 2942.75, the d2 loss is: -3369.0156, the g loss is: 3375.2969, the ae loss is: 0.007179129, the jacobian loss is:0.106322795\n",
            "This is the iter 5104, the d1 loss is: 2808.9844, the d2 loss is: -3207.7031, the g loss is: 3264.4297, the ae loss is: 0.008644978, the jacobian loss is:0.07850012\n",
            "This is the iter 5105, the d1 loss is: 2851.0469, the d2 loss is: -3286.914, the g loss is: 3268.3516, the ae loss is: 0.0053858017, the jacobian loss is:0.10270684\n",
            "This is the iter 5106, the d1 loss is: 2733.414, the d2 loss is: -3151.3516, the g loss is: 3204.961, the ae loss is: 0.0067637665, the jacobian loss is:0.09520836\n",
            "This is the iter 5107, the d1 loss is: 2844.1562, the d2 loss is: -3286.4062, the g loss is: 3252.3203, the ae loss is: 0.006598146, the jacobian loss is:0.08732855\n",
            "This is the iter 5108, the d1 loss is: 2813.8203, the d2 loss is: -3249.625, the g loss is: 3268.4531, the ae loss is: 0.010194689, the jacobian loss is:0.10745561\n",
            "This is the iter 5109, the d1 loss is: 2904.6562, the d2 loss is: -3313.1875, the g loss is: 3315.0469, the ae loss is: 0.010812227, the jacobian loss is:0.12993146\n",
            "This is the iter 5110, the d1 loss is: 2721.086, the d2 loss is: -3125.3281, the g loss is: 3102.9531, the ae loss is: 0.005466886, the jacobian loss is:0.09767014\n",
            "This is the iter 5111, the d1 loss is: 2844.4375, the d2 loss is: -3286.9844, the g loss is: 3283.7734, the ae loss is: 0.0076329485, the jacobian loss is:0.12051877\n",
            "This is the iter 5112, the d1 loss is: 2919.539, the d2 loss is: -3303.3828, the g loss is: 3383.664, the ae loss is: 0.006073826, the jacobian loss is:0.110607535\n",
            "This is the iter 5113, the d1 loss is: 2703.5547, the d2 loss is: -3140.9453, the g loss is: 3125.0312, the ae loss is: 0.0056839464, the jacobian loss is:0.09657585\n",
            "This is the iter 5114, the d1 loss is: 2747.5781, the d2 loss is: -3198.6719, the g loss is: 3144.7969, the ae loss is: 0.0051983143, the jacobian loss is:0.10659998\n",
            "This is the iter 5115, the d1 loss is: 2888.2812, the d2 loss is: -3291.5469, the g loss is: 3265.0312, the ae loss is: 0.0059894305, the jacobian loss is:0.09966584\n",
            "This is the iter 5116, the d1 loss is: 2774.6016, the d2 loss is: -3194.289, the g loss is: 3237.625, the ae loss is: 0.0059741987, the jacobian loss is:0.08649937\n",
            "This is the iter 5117, the d1 loss is: 2797.1953, the d2 loss is: -3142.2969, the g loss is: 3237.1172, the ae loss is: 0.0048600785, the jacobian loss is:0.081147484\n",
            "This is the iter 5118, the d1 loss is: 2771.4766, the d2 loss is: -3188.3828, the g loss is: 3250.2734, the ae loss is: 0.009155605, the jacobian loss is:0.0982109\n",
            "This is the iter 5119, the d1 loss is: 2891.461, the d2 loss is: -3322.1016, the g loss is: 3319.5469, the ae loss is: 0.0071559013, the jacobian loss is:0.108155005\n",
            "This is the iter 5120, the d1 loss is: 2917.3672, the d2 loss is: -3309.1797, the g loss is: 3312.2656, the ae loss is: 0.0078062178, the jacobian loss is:0.15816648\n",
            "This is the iter 5121, the d1 loss is: 2820.0938, the d2 loss is: -3211.8281, the g loss is: 3249.4219, the ae loss is: 0.0059157703, the jacobian loss is:0.09200039\n",
            "This is the iter 5122, the d1 loss is: 2784.9688, the d2 loss is: -3211.664, the g loss is: 3277.5312, the ae loss is: 0.006627431, the jacobian loss is:0.10986628\n",
            "This is the iter 5123, the d1 loss is: 2899.5703, the d2 loss is: -3289.6719, the g loss is: 3272.3438, the ae loss is: 0.007183199, the jacobian loss is:0.10652749\n",
            "This is the iter 5124, the d1 loss is: 2779.1953, the d2 loss is: -3229.1875, the g loss is: 3190.9062, the ae loss is: 0.0067004743, the jacobian loss is:0.13713449\n",
            "This is the iter 5125, the d1 loss is: 2783.8516, the d2 loss is: -3214.711, the g loss is: 3218.75, the ae loss is: 0.0073274327, the jacobian loss is:0.08921358\n",
            "This is the iter 5126, the d1 loss is: 2785.086, the d2 loss is: -3217.9922, the g loss is: 3226.1016, the ae loss is: 0.008543806, the jacobian loss is:0.0796572\n",
            "This is the iter 5127, the d1 loss is: 2723.7344, the d2 loss is: -3139.4531, the g loss is: 3145.586, the ae loss is: 0.007551931, the jacobian loss is:0.098246776\n",
            "This is the iter 5128, the d1 loss is: 2804.4375, the d2 loss is: -3198.6797, the g loss is: 3211.7188, the ae loss is: 0.006256588, the jacobian loss is:0.08712785\n",
            "This is the iter 5129, the d1 loss is: 2580.2812, the d2 loss is: -2989.4531, the g loss is: 3020.0156, the ae loss is: 0.0063240184, the jacobian loss is:0.10157382\n",
            "This is the iter 5130, the d1 loss is: 2724.0781, the d2 loss is: -3146.3047, the g loss is: 3183.2188, the ae loss is: 0.0057974095, the jacobian loss is:0.10286197\n",
            "This is the iter 5131, the d1 loss is: 2797.2344, the d2 loss is: -3212.0234, the g loss is: 3235.914, the ae loss is: 0.005606052, the jacobian loss is:0.08830091\n",
            "This is the iter 5132, the d1 loss is: 2726.3438, the d2 loss is: -3177.9531, the g loss is: 3221.4766, the ae loss is: 0.0064433482, the jacobian loss is:0.07861524\n",
            "This is the iter 5133, the d1 loss is: 2827.2969, the d2 loss is: -3273.039, the g loss is: 3249.0312, the ae loss is: 0.00948753, the jacobian loss is:0.12775318\n",
            "This is the iter 5134, the d1 loss is: 2676.414, the d2 loss is: -3039.4375, the g loss is: 3031.4375, the ae loss is: 0.0063298666, the jacobian loss is:0.1182599\n",
            "This is the iter 5135, the d1 loss is: 2742.7656, the d2 loss is: -3164.8594, the g loss is: 3168.9297, the ae loss is: 0.0068680597, the jacobian loss is:0.13350527\n",
            "This is the iter 5136, the d1 loss is: 2810.1406, the d2 loss is: -3212.039, the g loss is: 3236.6953, the ae loss is: 0.005030681, the jacobian loss is:0.0933418\n",
            "This is the iter 5137, the d1 loss is: 2728.7578, the d2 loss is: -3149.3281, the g loss is: 3172.086, the ae loss is: 0.006492254, the jacobian loss is:0.100593686\n",
            "This is the iter 5138, the d1 loss is: 2759.75, the d2 loss is: -3158.2812, the g loss is: 3266.6406, the ae loss is: 0.0076437183, the jacobian loss is:0.09965057\n",
            "This is the iter 5139, the d1 loss is: 2756.0, the d2 loss is: -3181.4844, the g loss is: 3125.8281, the ae loss is: 0.005295937, the jacobian loss is:0.11757835\n",
            "This is the iter 5140, the d1 loss is: 2766.6953, the d2 loss is: -3215.1719, the g loss is: 3204.1172, the ae loss is: 0.0059582004, the jacobian loss is:0.13798565\n",
            "This is the iter 5141, the d1 loss is: 3122.4688, the d2 loss is: -3457.2188, the g loss is: 3440.4375, the ae loss is: 0.007276926, the jacobian loss is:0.10028962\n",
            "This is the iter 5142, the d1 loss is: 3028.539, the d2 loss is: -3426.2422, the g loss is: 3368.9219, the ae loss is: 0.009346981, the jacobian loss is:0.1182094\n",
            "This is the iter 5143, the d1 loss is: 2742.6094, the d2 loss is: -3112.4062, the g loss is: 3195.5, the ae loss is: 0.0068295468, the jacobian loss is:0.10436438\n",
            "This is the iter 5144, the d1 loss is: 2680.8516, the d2 loss is: -3150.664, the g loss is: 3225.6094, the ae loss is: 0.0070467396, the jacobian loss is:0.08438167\n",
            "This is the iter 5145, the d1 loss is: 2672.8594, the d2 loss is: -3063.2578, the g loss is: 3094.25, the ae loss is: 0.0061330795, the jacobian loss is:0.117699064\n",
            "This is the iter 5146, the d1 loss is: 2784.5703, the d2 loss is: -3238.6406, the g loss is: 3264.3281, the ae loss is: 0.0061405813, the jacobian loss is:0.0715993\n",
            "This is the iter 5147, the d1 loss is: 2749.2969, the d2 loss is: -3189.0, the g loss is: 3147.1953, the ae loss is: 0.007464611, the jacobian loss is:0.14854671\n",
            "This is the iter 5148, the d1 loss is: 2690.7188, the d2 loss is: -3098.125, the g loss is: 3099.1016, the ae loss is: 0.0056290263, the jacobian loss is:0.09962856\n",
            "This is the iter 5149, the d1 loss is: 2725.5078, the d2 loss is: -3142.3125, the g loss is: 3141.2656, the ae loss is: 0.0055661495, the jacobian loss is:0.12593372\n",
            "This is the iter 5150, the d1 loss is: 2786.7812, the d2 loss is: -3234.8984, the g loss is: 3226.3047, the ae loss is: 0.008198755, the jacobian loss is:0.12139135\n",
            "This is the iter 5151, the d1 loss is: 2884.6094, the d2 loss is: -3300.3281, the g loss is: 3280.5312, the ae loss is: 0.0037398837, the jacobian loss is:0.08572894\n",
            "This is the iter 5152, the d1 loss is: 2736.9062, the d2 loss is: -3086.8047, the g loss is: 3129.1562, the ae loss is: 0.005030766, the jacobian loss is:0.06987383\n",
            "This is the iter 5153, the d1 loss is: 2779.164, the d2 loss is: -3181.5703, the g loss is: 3192.875, the ae loss is: 0.007517376, the jacobian loss is:0.07968315\n",
            "This is the iter 5154, the d1 loss is: 2885.789, the d2 loss is: -3306.4922, the g loss is: 3289.0781, the ae loss is: 0.00497564, the jacobian loss is:0.07610347\n",
            "This is the iter 5155, the d1 loss is: 2791.3516, the d2 loss is: -3170.7031, the g loss is: 3203.5625, the ae loss is: 0.0056935824, the jacobian loss is:0.10327989\n",
            "This is the iter 5156, the d1 loss is: 2613.3516, the d2 loss is: -3033.4531, the g loss is: 3074.1094, the ae loss is: 0.011540206, the jacobian loss is:0.11829187\n",
            "This is the iter 5157, the d1 loss is: 2819.7812, the d2 loss is: -3249.0938, the g loss is: 3251.2734, the ae loss is: 0.0067845522, the jacobian loss is:0.11008871\n",
            "This is the iter 5158, the d1 loss is: 2808.9219, the d2 loss is: -3206.9297, the g loss is: 3212.6875, the ae loss is: 0.0066754203, the jacobian loss is:0.10824099\n",
            "This is the iter 5159, the d1 loss is: 2823.8516, the d2 loss is: -3245.6719, the g loss is: 3216.6484, the ae loss is: 0.0068396963, the jacobian loss is:0.11042993\n",
            "This is the iter 5160, the d1 loss is: 2861.25, the d2 loss is: -3248.3516, the g loss is: 3256.7656, the ae loss is: 0.008195788, the jacobian loss is:0.072965145\n",
            "This is the iter 5161, the d1 loss is: 2569.375, the d2 loss is: -3038.7969, the g loss is: 3034.289, the ae loss is: 0.0075803846, the jacobian loss is:0.09726573\n",
            "This is the iter 5162, the d1 loss is: 2807.8594, the d2 loss is: -3211.5078, the g loss is: 3182.6094, the ae loss is: 0.0063582906, the jacobian loss is:0.102241054\n",
            "This is the iter 5163, the d1 loss is: 2837.211, the d2 loss is: -3249.3594, the g loss is: 3253.1875, the ae loss is: 0.00575543, the jacobian loss is:0.0981235\n",
            "This is the iter 5164, the d1 loss is: 2884.7031, the d2 loss is: -3240.039, the g loss is: 3284.0625, the ae loss is: 0.008306749, the jacobian loss is:0.13048889\n",
            "This is the iter 5165, the d1 loss is: 2772.3828, the d2 loss is: -3187.4922, the g loss is: 3251.914, the ae loss is: 0.00748707, the jacobian loss is:0.1580456\n",
            "This is the iter 5166, the d1 loss is: 2767.2734, the d2 loss is: -3217.414, the g loss is: 3197.6094, the ae loss is: 0.006990012, the jacobian loss is:0.124650985\n",
            "This is the iter 5167, the d1 loss is: 2698.086, the d2 loss is: -3119.2031, the g loss is: 3074.8906, the ae loss is: 0.0076474133, the jacobian loss is:0.09455773\n",
            "This is the iter 5168, the d1 loss is: 2797.25, the d2 loss is: -3208.9453, the g loss is: 3271.1953, the ae loss is: 0.0067151636, the jacobian loss is:0.086245686\n",
            "This is the iter 5169, the d1 loss is: 2847.625, the d2 loss is: -3214.6016, the g loss is: 3249.461, the ae loss is: 0.008010386, the jacobian loss is:0.0919912\n",
            "This is the iter 5170, the d1 loss is: 2793.5938, the d2 loss is: -3211.3047, the g loss is: 3233.3281, the ae loss is: 0.007311943, the jacobian loss is:0.11310627\n",
            "This is the iter 5171, the d1 loss is: 2825.3438, the d2 loss is: -3245.2734, the g loss is: 3252.9375, the ae loss is: 0.0050288327, the jacobian loss is:0.102386706\n",
            "This is the iter 5172, the d1 loss is: 2582.2656, the d2 loss is: -2999.7656, the g loss is: 3031.75, the ae loss is: 0.006072637, the jacobian loss is:0.118419774\n",
            "This is the iter 5173, the d1 loss is: 2823.7344, the d2 loss is: -3236.5156, the g loss is: 3230.2969, the ae loss is: 0.0058063357, the jacobian loss is:0.123647965\n",
            "This is the iter 5174, the d1 loss is: 2767.9219, the d2 loss is: -3176.461, the g loss is: 3231.9688, the ae loss is: 0.007284442, the jacobian loss is:0.09312549\n",
            "This is the iter 5175, the d1 loss is: 2866.7812, the d2 loss is: -3289.1094, the g loss is: 3278.0156, the ae loss is: 0.006761009, the jacobian loss is:0.1320084\n",
            "This is the iter 5176, the d1 loss is: 2833.8984, the d2 loss is: -3214.8516, the g loss is: 3150.7969, the ae loss is: 0.008664718, the jacobian loss is:0.1367417\n",
            "This is the iter 5177, the d1 loss is: 2926.9062, the d2 loss is: -3311.3203, the g loss is: 3307.1406, the ae loss is: 0.007397861, the jacobian loss is:0.093775086\n",
            "This is the iter 5178, the d1 loss is: 2844.375, the d2 loss is: -3253.7266, the g loss is: 3247.3438, the ae loss is: 0.0052747093, the jacobian loss is:0.08385334\n",
            "This is the iter 5179, the d1 loss is: 2767.8047, the d2 loss is: -3163.5469, the g loss is: 3221.7188, the ae loss is: 0.006041132, the jacobian loss is:0.14029427\n",
            "This is the iter 5180, the d1 loss is: 2812.164, the d2 loss is: -3222.7969, the g loss is: 3241.8906, the ae loss is: 0.0074958764, the jacobian loss is:0.112224795\n",
            "This is the iter 5181, the d1 loss is: 2935.7031, the d2 loss is: -3371.6953, the g loss is: 3297.5078, the ae loss is: 0.0077058757, the jacobian loss is:0.09797993\n",
            "This is the iter 5182, the d1 loss is: 2906.0312, the d2 loss is: -3348.3516, the g loss is: 3345.3594, the ae loss is: 0.0066956477, the jacobian loss is:0.12560426\n",
            "This is the iter 5183, the d1 loss is: 2811.1406, the d2 loss is: -3224.0781, the g loss is: 3191.1016, the ae loss is: 0.004857487, the jacobian loss is:0.13767727\n",
            "This is the iter 5184, the d1 loss is: 2766.5312, the d2 loss is: -3201.8906, the g loss is: 3237.2266, the ae loss is: 0.00716082, the jacobian loss is:0.0744334\n",
            "This is the iter 5185, the d1 loss is: 3034.875, the d2 loss is: -3414.5625, the g loss is: 3450.6719, the ae loss is: 0.0050292886, the jacobian loss is:0.12668556\n",
            "This is the iter 5186, the d1 loss is: 2860.0, the d2 loss is: -3264.7734, the g loss is: 3268.4453, the ae loss is: 0.0058354135, the jacobian loss is:0.09674407\n",
            "This is the iter 5187, the d1 loss is: 2692.9922, the d2 loss is: -3122.4219, the g loss is: 3132.8281, the ae loss is: 0.0070350245, the jacobian loss is:0.12230656\n",
            "This is the iter 5188, the d1 loss is: 2737.7266, the d2 loss is: -3143.7969, the g loss is: 3093.586, the ae loss is: 0.0069935843, the jacobian loss is:0.10770968\n",
            "This is the iter 5189, the d1 loss is: 2814.1953, the d2 loss is: -3209.7578, the g loss is: 3226.2969, the ae loss is: 0.009078609, the jacobian loss is:0.1272739\n",
            "This is the iter 5190, the d1 loss is: 2892.6094, the d2 loss is: -3292.9766, the g loss is: 3323.6719, the ae loss is: 0.007894386, the jacobian loss is:0.09056333\n",
            "This is the iter 5191, the d1 loss is: 3142.7578, the d2 loss is: -3540.4922, the g loss is: 3542.6094, the ae loss is: 0.008446727, the jacobian loss is:0.11599473\n",
            "This is the iter 5192, the d1 loss is: 2902.2422, the d2 loss is: -3296.0469, the g loss is: 3273.5469, the ae loss is: 0.007738627, the jacobian loss is:0.09882638\n",
            "This is the iter 5193, the d1 loss is: 2889.125, the d2 loss is: -3281.9219, the g loss is: 3240.1484, the ae loss is: 0.005050695, the jacobian loss is:0.11846198\n",
            "This is the iter 5194, the d1 loss is: 2815.4453, the d2 loss is: -3217.7969, the g loss is: 3236.9219, the ae loss is: 0.0064197797, the jacobian loss is:0.08673584\n",
            "This is the iter 5195, the d1 loss is: 2895.1562, the d2 loss is: -3331.5469, the g loss is: 3290.8672, the ae loss is: 0.01000742, the jacobian loss is:0.10517219\n",
            "This is the iter 5196, the d1 loss is: 2877.664, the d2 loss is: -3292.75, the g loss is: 3315.4922, the ae loss is: 0.005606569, the jacobian loss is:0.12420133\n",
            "This is the iter 5197, the d1 loss is: 3088.0312, the d2 loss is: -3484.0703, the g loss is: 3491.4531, the ae loss is: 0.00476884, the jacobian loss is:0.10050882\n",
            "This is the iter 5198, the d1 loss is: 2681.5625, the d2 loss is: -3060.5, the g loss is: 3070.1875, the ae loss is: 0.0071419207, the jacobian loss is:0.08814061\n",
            "This is the iter 5199, the d1 loss is: 2816.2812, the d2 loss is: -3221.6953, the g loss is: 3241.9297, the ae loss is: 0.006195045, the jacobian loss is:0.11315711\n",
            "This is the iter 5200, the d1 loss is: 2846.289, the d2 loss is: -3271.0469, the g loss is: 3259.7578, the ae loss is: 0.0062306807, the jacobian loss is:0.11342608\n",
            "0.18050879\n",
            "0.6684412\n",
            "This is the iter 5201, the d1 loss is: 2868.461, the d2 loss is: -3306.6094, the g loss is: 3271.625, the ae loss is: 0.006025676, the jacobian loss is:0.101680286\n",
            "This is the iter 5202, the d1 loss is: 2882.25, the d2 loss is: -3304.8203, the g loss is: 3297.9375, the ae loss is: 0.008592866, the jacobian loss is:0.111264825\n",
            "This is the iter 5203, the d1 loss is: 2838.7422, the d2 loss is: -3263.914, the g loss is: 3211.4297, the ae loss is: 0.01069572, the jacobian loss is:0.13346618\n",
            "This is the iter 5204, the d1 loss is: 2943.2656, the d2 loss is: -3342.3125, the g loss is: 3297.0938, the ae loss is: 0.006399576, the jacobian loss is:0.12732157\n",
            "This is the iter 5205, the d1 loss is: 2843.5156, the d2 loss is: -3255.2969, the g loss is: 3237.25, the ae loss is: 0.005950401, the jacobian loss is:0.11586178\n",
            "This is the iter 5206, the d1 loss is: 2920.6172, the d2 loss is: -3335.625, the g loss is: 3343.8516, the ae loss is: 0.0066563217, the jacobian loss is:0.10645284\n",
            "This is the iter 5207, the d1 loss is: 2814.8828, the d2 loss is: -3217.3906, the g loss is: 3234.586, the ae loss is: 0.006722077, the jacobian loss is:0.09806955\n",
            "This is the iter 5208, the d1 loss is: 2797.164, the d2 loss is: -3218.9062, the g loss is: 3217.414, the ae loss is: 0.006959532, the jacobian loss is:0.097452015\n",
            "This is the iter 5209, the d1 loss is: 2743.1406, the d2 loss is: -3140.7266, the g loss is: 3242.7266, the ae loss is: 0.004440826, the jacobian loss is:0.08691984\n",
            "This is the iter 5210, the d1 loss is: 2797.3281, the d2 loss is: -3122.664, the g loss is: 3192.8281, the ae loss is: 0.0047244355, the jacobian loss is:0.082955495\n",
            "This is the iter 5211, the d1 loss is: 2745.3594, the d2 loss is: -3178.9688, the g loss is: 3234.7031, the ae loss is: 0.008392043, the jacobian loss is:0.1264742\n",
            "This is the iter 5212, the d1 loss is: 2817.3906, the d2 loss is: -3208.7266, the g loss is: 3224.5, the ae loss is: 0.008208527, the jacobian loss is:0.10402479\n",
            "This is the iter 5213, the d1 loss is: 2749.1016, the d2 loss is: -3138.0156, the g loss is: 3197.5234, the ae loss is: 0.003740775, the jacobian loss is:0.090751894\n",
            "This is the iter 5214, the d1 loss is: 2918.875, the d2 loss is: -3309.8203, the g loss is: 3346.5625, the ae loss is: 0.0075818063, the jacobian loss is:0.085368045\n",
            "This is the iter 5215, the d1 loss is: 2870.8438, the d2 loss is: -3253.625, the g loss is: 3282.1953, the ae loss is: 0.007076137, the jacobian loss is:0.11345721\n",
            "This is the iter 5216, the d1 loss is: 2863.0156, the d2 loss is: -3295.7578, the g loss is: 3267.6406, the ae loss is: 0.0064721126, the jacobian loss is:0.10341022\n",
            "This is the iter 5217, the d1 loss is: 2827.6797, the d2 loss is: -3221.0078, the g loss is: 3194.5781, the ae loss is: 0.004675467, the jacobian loss is:0.087877296\n",
            "This is the iter 5218, the d1 loss is: 2685.0312, the d2 loss is: -3150.375, the g loss is: 3123.3516, the ae loss is: 0.008263831, the jacobian loss is:0.12560399\n",
            "This is the iter 5219, the d1 loss is: 2714.3047, the d2 loss is: -3177.1172, the g loss is: 3207.7188, the ae loss is: 0.008064136, the jacobian loss is:0.11020182\n",
            "This is the iter 5220, the d1 loss is: 3088.0312, the d2 loss is: -3506.9844, the g loss is: 3471.5938, the ae loss is: 0.007866365, the jacobian loss is:0.10730467\n",
            "This is the iter 5221, the d1 loss is: 2984.1953, the d2 loss is: -3398.7578, the g loss is: 3363.8047, the ae loss is: 0.006340649, the jacobian loss is:0.1151476\n",
            "This is the iter 5222, the d1 loss is: 2897.1953, the d2 loss is: -3280.6172, the g loss is: 3340.539, the ae loss is: 0.00773286, the jacobian loss is:0.08198767\n",
            "This is the iter 5223, the d1 loss is: 2796.1406, the d2 loss is: -3251.3906, the g loss is: 3230.336, the ae loss is: 0.005947461, the jacobian loss is:0.13768688\n",
            "This is the iter 5224, the d1 loss is: 2623.3203, the d2 loss is: -2967.1172, the g loss is: 3075.4219, the ae loss is: 0.01018596, the jacobian loss is:0.09892566\n",
            "This is the iter 5225, the d1 loss is: 2862.4844, the d2 loss is: -3246.1172, the g loss is: 3230.7734, the ae loss is: 0.007028429, the jacobian loss is:0.11055613\n",
            "This is the iter 5226, the d1 loss is: 2902.2812, the d2 loss is: -3278.9688, the g loss is: 3261.5312, the ae loss is: 0.0052155885, the jacobian loss is:0.119950555\n",
            "This is the iter 5227, the d1 loss is: 2798.8281, the d2 loss is: -3236.3906, the g loss is: 3268.375, the ae loss is: 0.0071190307, the jacobian loss is:0.10363089\n",
            "This is the iter 5228, the d1 loss is: 2859.7812, the d2 loss is: -3322.0469, the g loss is: 3300.0469, the ae loss is: 0.0073596733, the jacobian loss is:0.09079503\n",
            "This is the iter 5229, the d1 loss is: 2812.961, the d2 loss is: -3267.3281, the g loss is: 3235.2266, the ae loss is: 0.0066562598, the jacobian loss is:0.11541347\n",
            "This is the iter 5230, the d1 loss is: 2883.9844, the d2 loss is: -3313.8125, the g loss is: 3306.9531, the ae loss is: 0.0063113472, the jacobian loss is:0.0906066\n",
            "This is the iter 5231, the d1 loss is: 2831.125, the d2 loss is: -3224.375, the g loss is: 3171.4844, the ae loss is: 0.007392429, the jacobian loss is:0.14172344\n",
            "This is the iter 5232, the d1 loss is: 2759.1562, the d2 loss is: -3191.6406, the g loss is: 3171.6406, the ae loss is: 0.006697717, the jacobian loss is:0.08376981\n",
            "This is the iter 5233, the d1 loss is: 2787.1797, the d2 loss is: -3159.1094, the g loss is: 3083.25, the ae loss is: 0.008620733, the jacobian loss is:0.12547475\n",
            "This is the iter 5234, the d1 loss is: 2785.125, the d2 loss is: -3221.1016, the g loss is: 3208.7812, the ae loss is: 0.0062522176, the jacobian loss is:0.08127264\n",
            "This is the iter 5235, the d1 loss is: 2921.3516, the d2 loss is: -3308.1875, the g loss is: 3264.375, the ae loss is: 0.0061435974, the jacobian loss is:0.07718251\n",
            "This is the iter 5236, the d1 loss is: 2807.4688, the d2 loss is: -3200.8203, the g loss is: 3232.789, the ae loss is: 0.008492095, the jacobian loss is:0.112535074\n",
            "This is the iter 5237, the d1 loss is: 2894.75, the d2 loss is: -3271.1719, the g loss is: 3321.164, the ae loss is: 0.009274436, the jacobian loss is:0.09844972\n",
            "This is the iter 5238, the d1 loss is: 2776.9688, the d2 loss is: -3175.4531, the g loss is: 3194.5, the ae loss is: 0.006747821, the jacobian loss is:0.113395564\n",
            "This is the iter 5239, the d1 loss is: 2855.1484, the d2 loss is: -3250.7188, the g loss is: 3250.9531, the ae loss is: 0.00485838, the jacobian loss is:0.08057606\n",
            "This is the iter 5240, the d1 loss is: 2783.9297, the d2 loss is: -3229.1406, the g loss is: 3188.2344, the ae loss is: 0.0072588576, the jacobian loss is:0.09167749\n",
            "This is the iter 5241, the d1 loss is: 2896.2578, the d2 loss is: -3284.7969, the g loss is: 3303.3281, the ae loss is: 0.0061086356, the jacobian loss is:0.1061731\n",
            "This is the iter 5242, the d1 loss is: 2863.5234, the d2 loss is: -3284.3672, the g loss is: 3289.9922, the ae loss is: 0.0070843664, the jacobian loss is:0.106364004\n",
            "This is the iter 5243, the d1 loss is: 2750.5938, the d2 loss is: -3135.8516, the g loss is: 3217.5625, the ae loss is: 0.004308574, the jacobian loss is:0.09758088\n",
            "This is the iter 5244, the d1 loss is: 2787.961, the d2 loss is: -3202.914, the g loss is: 3203.2266, the ae loss is: 0.010935815, the jacobian loss is:0.10482044\n",
            "This is the iter 5245, the d1 loss is: 2871.9062, the d2 loss is: -3245.5156, the g loss is: 3291.4375, the ae loss is: 0.0069254627, the jacobian loss is:0.10005372\n",
            "This is the iter 5246, the d1 loss is: 2821.3672, the d2 loss is: -3182.1562, the g loss is: 3197.8203, the ae loss is: 0.006485151, the jacobian loss is:0.07844555\n",
            "This is the iter 5247, the d1 loss is: 2963.1484, the d2 loss is: -3359.2031, the g loss is: 3370.9062, the ae loss is: 0.006968518, the jacobian loss is:0.104285255\n",
            "This is the iter 5248, the d1 loss is: 2916.4922, the d2 loss is: -3331.9062, the g loss is: 3335.5547, the ae loss is: 0.0074691456, the jacobian loss is:0.113081284\n",
            "This is the iter 5249, the d1 loss is: 2923.039, the d2 loss is: -3272.0781, the g loss is: 3330.5312, the ae loss is: 0.008145204, the jacobian loss is:0.10439902\n",
            "This is the iter 5250, the d1 loss is: 2888.414, the d2 loss is: -3325.5312, the g loss is: 3266.3438, the ae loss is: 0.0073119495, the jacobian loss is:0.07457154\n",
            "This is the iter 5251, the d1 loss is: 2799.2188, the d2 loss is: -3184.7188, the g loss is: 3216.664, the ae loss is: 0.009552055, the jacobian loss is:0.10701477\n",
            "This is the iter 5252, the d1 loss is: 2729.711, the d2 loss is: -3149.1094, the g loss is: 3100.1484, the ae loss is: 0.009878462, the jacobian loss is:0.10918273\n",
            "This is the iter 5253, the d1 loss is: 2868.5469, the d2 loss is: -3253.5547, the g loss is: 3278.164, the ae loss is: 0.006682051, the jacobian loss is:0.16053525\n",
            "This is the iter 5254, the d1 loss is: 2744.0234, the d2 loss is: -3187.8672, the g loss is: 3213.539, the ae loss is: 0.006939551, the jacobian loss is:0.09287735\n",
            "This is the iter 5255, the d1 loss is: 2916.625, the d2 loss is: -3306.6406, the g loss is: 3303.4688, the ae loss is: 0.0063157454, the jacobian loss is:0.12547415\n",
            "This is the iter 5256, the d1 loss is: 2851.586, the d2 loss is: -3279.875, the g loss is: 3265.6406, the ae loss is: 0.0062470837, the jacobian loss is:0.10634966\n",
            "This is the iter 5257, the d1 loss is: 2953.0156, the d2 loss is: -3346.711, the g loss is: 3288.0234, the ae loss is: 0.0064322725, the jacobian loss is:0.11725502\n",
            "This is the iter 5258, the d1 loss is: 2808.5625, the d2 loss is: -3267.9062, the g loss is: 3253.211, the ae loss is: 0.008625459, the jacobian loss is:0.08260629\n",
            "This is the iter 5259, the d1 loss is: 2954.0078, the d2 loss is: -3403.539, the g loss is: 3373.2188, the ae loss is: 0.0055077183, the jacobian loss is:0.101447456\n",
            "This is the iter 5260, the d1 loss is: 2953.4688, the d2 loss is: -3349.3594, the g loss is: 3340.9844, the ae loss is: 0.0072054947, the jacobian loss is:0.085405976\n",
            "This is the iter 5261, the d1 loss is: 2921.1016, the d2 loss is: -3358.5938, the g loss is: 3339.461, the ae loss is: 0.006690386, the jacobian loss is:0.11403348\n",
            "This is the iter 5262, the d1 loss is: 2900.4062, the d2 loss is: -3279.539, the g loss is: 3341.2422, the ae loss is: 0.0063075684, the jacobian loss is:0.11037758\n",
            "This is the iter 5263, the d1 loss is: 2744.4688, the d2 loss is: -3132.1562, the g loss is: 3107.7734, the ae loss is: 0.008903552, the jacobian loss is:0.12736362\n",
            "This is the iter 5264, the d1 loss is: 2882.3047, the d2 loss is: -3281.414, the g loss is: 3238.2734, the ae loss is: 0.005682351, the jacobian loss is:0.095638685\n",
            "This is the iter 5265, the d1 loss is: 2820.1797, the d2 loss is: -3229.039, the g loss is: 3251.8125, the ae loss is: 0.0069011366, the jacobian loss is:0.14079165\n",
            "This is the iter 5266, the d1 loss is: 2873.8203, the d2 loss is: -3276.0469, the g loss is: 3297.5781, the ae loss is: 0.0073775314, the jacobian loss is:0.09703247\n",
            "This is the iter 5267, the d1 loss is: 2927.375, the d2 loss is: -3389.0547, the g loss is: 3335.5625, the ae loss is: 0.008179732, the jacobian loss is:0.10929379\n",
            "This is the iter 5268, the d1 loss is: 2974.539, the d2 loss is: -3401.6953, the g loss is: 3428.7578, the ae loss is: 0.008573298, the jacobian loss is:0.08894672\n",
            "This is the iter 5269, the d1 loss is: 2870.4922, the d2 loss is: -3285.8594, the g loss is: 3315.0547, the ae loss is: 0.006131749, the jacobian loss is:0.096427806\n",
            "This is the iter 5270, the d1 loss is: 2879.3828, the d2 loss is: -3290.9219, the g loss is: 3344.7188, the ae loss is: 0.006781645, the jacobian loss is:0.0987655\n",
            "This is the iter 5271, the d1 loss is: 2984.4219, the d2 loss is: -3342.7344, the g loss is: 3288.6719, the ae loss is: 0.0057836426, the jacobian loss is:0.10795819\n",
            "This is the iter 5272, the d1 loss is: 2961.25, the d2 loss is: -3352.4219, the g loss is: 3320.6875, the ae loss is: 0.0074677444, the jacobian loss is:0.101281375\n",
            "This is the iter 5273, the d1 loss is: 2899.3438, the d2 loss is: -3342.6875, the g loss is: 3382.2578, the ae loss is: 0.0061430214, the jacobian loss is:0.12980662\n",
            "This is the iter 5274, the d1 loss is: 2894.4297, the d2 loss is: -3262.9844, the g loss is: 3327.5703, the ae loss is: 0.0072014933, the jacobian loss is:0.093036346\n",
            "This is the iter 5275, the d1 loss is: 2967.4688, the d2 loss is: -3339.9688, the g loss is: 3343.9688, the ae loss is: 0.007885578, the jacobian loss is:0.07302212\n",
            "This is the iter 5276, the d1 loss is: 2921.4531, the d2 loss is: -3335.375, the g loss is: 3322.8125, the ae loss is: 0.0066365544, the jacobian loss is:0.08765708\n",
            "This is the iter 5277, the d1 loss is: 2960.0156, the d2 loss is: -3390.5781, the g loss is: 3350.6719, the ae loss is: 0.0066379434, the jacobian loss is:0.088670924\n",
            "This is the iter 5278, the d1 loss is: 2910.9062, the d2 loss is: -3347.7344, the g loss is: 3350.4453, the ae loss is: 0.0046601626, the jacobian loss is:0.110164754\n",
            "This is the iter 5279, the d1 loss is: 2927.875, the d2 loss is: -3311.0547, the g loss is: 3337.0781, the ae loss is: 0.009615813, the jacobian loss is:0.10805277\n",
            "This is the iter 5280, the d1 loss is: 2884.9453, the d2 loss is: -3335.0, the g loss is: 3381.1484, the ae loss is: 0.007935151, the jacobian loss is:0.106377296\n",
            "This is the iter 5281, the d1 loss is: 2944.1719, the d2 loss is: -3338.8516, the g loss is: 3270.8516, the ae loss is: 0.005735706, the jacobian loss is:0.119358286\n",
            "This is the iter 5282, the d1 loss is: 3026.711, the d2 loss is: -3405.539, the g loss is: 3349.0781, the ae loss is: 0.0071947966, the jacobian loss is:0.10150644\n",
            "This is the iter 5283, the d1 loss is: 2849.3438, the d2 loss is: -3294.289, the g loss is: 3264.5234, the ae loss is: 0.0058733183, the jacobian loss is:0.1272945\n",
            "This is the iter 5284, the d1 loss is: 2890.9375, the d2 loss is: -3298.75, the g loss is: 3255.2422, the ae loss is: 0.0090306895, the jacobian loss is:0.10718107\n",
            "This is the iter 5285, the d1 loss is: 2930.0, the d2 loss is: -3358.1406, the g loss is: 3338.7266, the ae loss is: 0.00602382, the jacobian loss is:0.10626067\n",
            "This is the iter 5286, the d1 loss is: 2994.4531, the d2 loss is: -3349.961, the g loss is: 3311.3594, the ae loss is: 0.0064579686, the jacobian loss is:0.08739722\n",
            "This is the iter 5287, the d1 loss is: 2773.6094, the d2 loss is: -3146.711, the g loss is: 3101.289, the ae loss is: 0.005663567, the jacobian loss is:0.11381521\n",
            "This is the iter 5288, the d1 loss is: 2836.1875, the d2 loss is: -3248.1406, the g loss is: 3278.1562, the ae loss is: 0.004934832, the jacobian loss is:0.11299092\n",
            "This is the iter 5289, the d1 loss is: 2801.2344, the d2 loss is: -3232.625, the g loss is: 3232.2422, the ae loss is: 0.0060341964, the jacobian loss is:0.07990239\n",
            "This is the iter 5290, the d1 loss is: 3001.5312, the d2 loss is: -3359.8438, the g loss is: 3391.5312, the ae loss is: 0.0071009826, the jacobian loss is:0.07553702\n",
            "This is the iter 5291, the d1 loss is: 2880.2812, the d2 loss is: -3282.2344, the g loss is: 3278.6484, the ae loss is: 0.0084165335, the jacobian loss is:0.09468544\n",
            "This is the iter 5292, the d1 loss is: 2949.6406, the d2 loss is: -3316.3281, the g loss is: 3315.9219, the ae loss is: 0.0109080225, the jacobian loss is:0.10251747\n",
            "This is the iter 5293, the d1 loss is: 2831.586, the d2 loss is: -3257.4297, the g loss is: 3286.625, the ae loss is: 0.0098956805, the jacobian loss is:0.09173221\n",
            "This is the iter 5294, the d1 loss is: 2927.3125, the d2 loss is: -3275.5703, the g loss is: 3286.2656, the ae loss is: 0.006312771, the jacobian loss is:0.08270483\n",
            "This is the iter 5295, the d1 loss is: 2933.914, the d2 loss is: -3294.9844, the g loss is: 3280.3438, the ae loss is: 0.006924186, the jacobian loss is:0.11331109\n",
            "This is the iter 5296, the d1 loss is: 2913.5, the d2 loss is: -3306.7344, the g loss is: 3308.0078, the ae loss is: 0.0065446356, the jacobian loss is:0.09600295\n",
            "This is the iter 5297, the d1 loss is: 2942.086, the d2 loss is: -3321.5938, the g loss is: 3338.1484, the ae loss is: 0.0057536, the jacobian loss is:0.08873445\n",
            "This is the iter 5298, the d1 loss is: 2929.375, the d2 loss is: -3349.7188, the g loss is: 3359.1562, the ae loss is: 0.007046709, the jacobian loss is:0.121343434\n",
            "This is the iter 5299, the d1 loss is: 2874.7656, the d2 loss is: -3249.625, the g loss is: 3247.7656, the ae loss is: 0.0046335487, the jacobian loss is:0.106060624\n",
            "This is the iter 5300, the d1 loss is: 2802.9453, the d2 loss is: -3216.6562, the g loss is: 3267.5938, the ae loss is: 0.006609671, the jacobian loss is:0.09331312\n",
            "0.19009106\n",
            "0.6975589\n",
            "This is the iter 5301, the d1 loss is: 2886.789, the d2 loss is: -3217.0234, the g loss is: 3187.0625, the ae loss is: 0.0061612786, the jacobian loss is:0.111163944\n",
            "This is the iter 5302, the d1 loss is: 2789.9531, the d2 loss is: -3198.7031, the g loss is: 3238.1719, the ae loss is: 0.005286363, the jacobian loss is:0.08503914\n",
            "This is the iter 5303, the d1 loss is: 2913.4219, the d2 loss is: -3261.3438, the g loss is: 3210.3203, the ae loss is: 0.0063541876, the jacobian loss is:0.13950226\n",
            "This is the iter 5304, the d1 loss is: 2851.211, the d2 loss is: -3241.0, the g loss is: 3202.164, the ae loss is: 0.008925075, the jacobian loss is:0.1026084\n",
            "This is the iter 5305, the d1 loss is: 3051.789, the d2 loss is: -3430.3125, the g loss is: 3409.7969, the ae loss is: 0.0072312783, the jacobian loss is:0.101070486\n",
            "This is the iter 5306, the d1 loss is: 2758.836, the d2 loss is: -3168.2344, the g loss is: 3161.1094, the ae loss is: 0.0067447913, the jacobian loss is:0.12295561\n",
            "This is the iter 5307, the d1 loss is: 2979.7969, the d2 loss is: -3399.414, the g loss is: 3363.3438, the ae loss is: 0.0073197847, the jacobian loss is:0.119589776\n",
            "This is the iter 5308, the d1 loss is: 2699.2969, the d2 loss is: -3089.7188, the g loss is: 3112.711, the ae loss is: 0.005993817, the jacobian loss is:0.08405265\n",
            "This is the iter 5309, the d1 loss is: 2858.3594, the d2 loss is: -3222.7266, the g loss is: 3280.7812, the ae loss is: 0.0064114337, the jacobian loss is:0.1147146\n",
            "This is the iter 5310, the d1 loss is: 2871.375, the d2 loss is: -3237.961, the g loss is: 3271.3828, the ae loss is: 0.008076537, the jacobian loss is:0.0896898\n",
            "This is the iter 5311, the d1 loss is: 2983.7734, the d2 loss is: -3328.6406, the g loss is: 3316.375, the ae loss is: 0.0054048514, the jacobian loss is:0.10862259\n",
            "This is the iter 5312, the d1 loss is: 2710.2344, the d2 loss is: -3105.3047, the g loss is: 3112.9531, the ae loss is: 0.0063404115, the jacobian loss is:0.08487082\n",
            "This is the iter 5313, the d1 loss is: 2852.2656, the d2 loss is: -3285.6719, the g loss is: 3306.8438, the ae loss is: 0.008303105, the jacobian loss is:0.106291525\n",
            "This is the iter 5314, the d1 loss is: 3031.9531, the d2 loss is: -3407.664, the g loss is: 3446.5234, the ae loss is: 0.006345328, the jacobian loss is:0.12679164\n",
            "This is the iter 5315, the d1 loss is: 2911.875, the d2 loss is: -3337.914, the g loss is: 3333.6172, the ae loss is: 0.009101565, the jacobian loss is:0.08627824\n",
            "This is the iter 5316, the d1 loss is: 2915.7031, the d2 loss is: -3354.5469, the g loss is: 3305.4531, the ae loss is: 0.010101726, the jacobian loss is:0.09042153\n",
            "This is the iter 5317, the d1 loss is: 2713.9531, the d2 loss is: -3115.6719, the g loss is: 3129.625, the ae loss is: 0.006189198, the jacobian loss is:0.11037861\n",
            "This is the iter 5318, the d1 loss is: 2849.7578, the d2 loss is: -3220.2188, the g loss is: 3203.5938, the ae loss is: 0.008741944, the jacobian loss is:0.12643282\n",
            "This is the iter 5319, the d1 loss is: 2931.5156, the d2 loss is: -3296.8828, the g loss is: 3325.7344, the ae loss is: 0.0060624927, the jacobian loss is:0.09202834\n",
            "This is the iter 5320, the d1 loss is: 2981.664, the d2 loss is: -3385.6484, the g loss is: 3362.5547, the ae loss is: 0.00646131, the jacobian loss is:0.104652904\n",
            "This is the iter 5321, the d1 loss is: 2869.1875, the d2 loss is: -3250.5469, the g loss is: 3271.461, the ae loss is: 0.007226552, the jacobian loss is:0.12071713\n",
            "This is the iter 5322, the d1 loss is: 2899.4375, the d2 loss is: -3276.6172, the g loss is: 3217.336, the ae loss is: 0.008126201, the jacobian loss is:0.10117667\n",
            "This is the iter 5323, the d1 loss is: 2904.8906, the d2 loss is: -3338.7578, the g loss is: 3281.4453, the ae loss is: 0.0048403624, the jacobian loss is:0.11119817\n",
            "This is the iter 5324, the d1 loss is: 2946.1094, the d2 loss is: -3337.7344, the g loss is: 3354.875, the ae loss is: 0.007567568, the jacobian loss is:0.09685078\n",
            "This is the iter 5325, the d1 loss is: 2810.836, the d2 loss is: -3206.461, the g loss is: 3315.1172, the ae loss is: 0.0061205816, the jacobian loss is:0.09068505\n",
            "This is the iter 5326, the d1 loss is: 3027.9688, the d2 loss is: -3391.9453, the g loss is: 3382.4375, the ae loss is: 0.007211822, the jacobian loss is:0.102041684\n",
            "This is the iter 5327, the d1 loss is: 2879.1484, the d2 loss is: -3246.6875, the g loss is: 3286.8047, the ae loss is: 0.006511042, the jacobian loss is:0.093670376\n",
            "This is the iter 5328, the d1 loss is: 3018.5938, the d2 loss is: -3433.2734, the g loss is: 3409.0625, the ae loss is: 0.004108347, the jacobian loss is:0.07818875\n",
            "This is the iter 5329, the d1 loss is: 2994.5, the d2 loss is: -3365.3906, the g loss is: 3380.3516, the ae loss is: 0.007281159, the jacobian loss is:0.11401795\n",
            "This is the iter 5330, the d1 loss is: 2962.875, the d2 loss is: -3358.6172, the g loss is: 3348.2656, the ae loss is: 0.006387775, the jacobian loss is:0.08188746\n",
            "This is the iter 5331, the d1 loss is: 2951.0625, the d2 loss is: -3328.125, the g loss is: 3289.2969, the ae loss is: 0.0059429565, the jacobian loss is:0.1180547\n",
            "This is the iter 5332, the d1 loss is: 3039.7031, the d2 loss is: -3395.2812, the g loss is: 3356.4375, the ae loss is: 0.008797281, the jacobian loss is:0.13012387\n",
            "This is the iter 5333, the d1 loss is: 3280.0469, the d2 loss is: -3683.25, the g loss is: 3672.5312, the ae loss is: 0.009180302, the jacobian loss is:0.11827609\n",
            "This is the iter 5334, the d1 loss is: 3027.3516, the d2 loss is: -3422.461, the g loss is: 3480.4844, the ae loss is: 0.006899611, the jacobian loss is:0.12374981\n",
            "This is the iter 5335, the d1 loss is: 2892.086, the d2 loss is: -3275.5, the g loss is: 3235.8906, the ae loss is: 0.008559529, the jacobian loss is:0.11052904\n",
            "This is the iter 5336, the d1 loss is: 3019.8594, the d2 loss is: -3392.3906, the g loss is: 3441.2969, the ae loss is: 0.00773903, the jacobian loss is:0.08042103\n",
            "This is the iter 5337, the d1 loss is: 3000.789, the d2 loss is: -3371.7812, the g loss is: 3417.1094, the ae loss is: 0.005946831, the jacobian loss is:0.106098175\n",
            "This is the iter 5338, the d1 loss is: 3079.8438, the d2 loss is: -3454.7266, the g loss is: 3376.7422, the ae loss is: 0.007883368, the jacobian loss is:0.10778605\n",
            "This is the iter 5339, the d1 loss is: 2953.164, the d2 loss is: -3347.9453, the g loss is: 3325.1719, the ae loss is: 0.005804605, the jacobian loss is:0.1170057\n",
            "This is the iter 5340, the d1 loss is: 2951.5469, the d2 loss is: -3308.164, the g loss is: 3279.6484, the ae loss is: 0.0055205235, the jacobian loss is:0.07612448\n",
            "This is the iter 5341, the d1 loss is: 2976.7969, the d2 loss is: -3329.9375, the g loss is: 3286.8906, the ae loss is: 0.007190874, the jacobian loss is:0.12112141\n",
            "This is the iter 5342, the d1 loss is: 2838.2812, the d2 loss is: -3260.086, the g loss is: 3328.9531, the ae loss is: 0.008023975, the jacobian loss is:0.11412048\n",
            "This is the iter 5343, the d1 loss is: 2961.789, the d2 loss is: -3380.0078, the g loss is: 3361.7578, the ae loss is: 0.006276346, the jacobian loss is:0.10473683\n",
            "This is the iter 5344, the d1 loss is: 3022.5, the d2 loss is: -3394.625, the g loss is: 3455.8828, the ae loss is: 0.009520071, the jacobian loss is:0.12377689\n",
            "This is the iter 5345, the d1 loss is: 2983.8281, the d2 loss is: -3375.6172, the g loss is: 3346.0938, the ae loss is: 0.006673556, the jacobian loss is:0.093550384\n",
            "This is the iter 5346, the d1 loss is: 2982.625, the d2 loss is: -3326.0156, the g loss is: 3289.5781, the ae loss is: 0.0052457866, the jacobian loss is:0.098013796\n",
            "This is the iter 5347, the d1 loss is: 2983.4766, the d2 loss is: -3381.375, the g loss is: 3408.4219, the ae loss is: 0.0061459565, the jacobian loss is:0.094322465\n",
            "This is the iter 5348, the d1 loss is: 2724.4766, the d2 loss is: -3092.5547, the g loss is: 3075.1406, the ae loss is: 0.006588433, the jacobian loss is:0.092718385\n",
            "This is the iter 5349, the d1 loss is: 2879.086, the d2 loss is: -3270.8828, the g loss is: 3238.3672, the ae loss is: 0.0063554873, the jacobian loss is:0.08552731\n",
            "This is the iter 5350, the d1 loss is: 3168.289, the d2 loss is: -3607.2344, the g loss is: 3590.0312, the ae loss is: 0.006695265, the jacobian loss is:0.08842449\n",
            "This is the iter 5351, the d1 loss is: 3042.4453, the d2 loss is: -3416.7656, the g loss is: 3413.0625, the ae loss is: 0.006326572, the jacobian loss is:0.1119873\n",
            "This is the iter 5352, the d1 loss is: 3021.5156, the d2 loss is: -3413.086, the g loss is: 3468.0703, the ae loss is: 0.006774857, the jacobian loss is:0.07879372\n",
            "This is the iter 5353, the d1 loss is: 2962.2188, the d2 loss is: -3321.5469, the g loss is: 3325.6094, the ae loss is: 0.006805048, the jacobian loss is:0.14330047\n",
            "This is the iter 5354, the d1 loss is: 3031.3672, the d2 loss is: -3446.4766, the g loss is: 3409.5938, the ae loss is: 0.0053706625, the jacobian loss is:0.10153142\n",
            "This is the iter 5355, the d1 loss is: 3037.4375, the d2 loss is: -3435.1562, the g loss is: 3371.5, the ae loss is: 0.0045885337, the jacobian loss is:0.12684143\n",
            "This is the iter 5356, the d1 loss is: 2923.7656, the d2 loss is: -3329.0781, the g loss is: 3318.3594, the ae loss is: 0.0029889853, the jacobian loss is:0.07749466\n",
            "This is the iter 5357, the d1 loss is: 3045.461, the d2 loss is: -3456.1016, the g loss is: 3441.1797, the ae loss is: 0.0073379423, the jacobian loss is:0.0970515\n",
            "This is the iter 5358, the d1 loss is: 3137.2188, the d2 loss is: -3566.0703, the g loss is: 3529.5156, the ae loss is: 0.006050245, the jacobian loss is:0.11309808\n",
            "This is the iter 5359, the d1 loss is: 3134.0781, the d2 loss is: -3505.789, the g loss is: 3422.5703, the ae loss is: 0.0064207795, the jacobian loss is:0.10659211\n",
            "This is the iter 5360, the d1 loss is: 3073.586, the d2 loss is: -3494.1094, the g loss is: 3406.9688, the ae loss is: 0.008677315, the jacobian loss is:0.13852155\n",
            "This is the iter 5361, the d1 loss is: 2996.1797, the d2 loss is: -3428.6562, the g loss is: 3418.8906, the ae loss is: 0.0060158307, the jacobian loss is:0.121624\n",
            "This is the iter 5362, the d1 loss is: 2992.7266, the d2 loss is: -3357.9844, the g loss is: 3339.25, the ae loss is: 0.0079376735, the jacobian loss is:0.10272519\n",
            "This is the iter 5363, the d1 loss is: 3028.5156, the d2 loss is: -3398.6562, the g loss is: 3367.7969, the ae loss is: 0.006363411, the jacobian loss is:0.13548973\n",
            "This is the iter 5364, the d1 loss is: 3100.414, the d2 loss is: -3452.2969, the g loss is: 3434.2812, the ae loss is: 0.008168651, the jacobian loss is:0.13173339\n",
            "This is the iter 5365, the d1 loss is: 3014.5078, the d2 loss is: -3387.9062, the g loss is: 3348.6719, the ae loss is: 0.0055846316, the jacobian loss is:0.10743705\n",
            "This is the iter 5366, the d1 loss is: 2915.0, the d2 loss is: -3266.7656, the g loss is: 3317.4688, the ae loss is: 0.008164319, the jacobian loss is:0.08709891\n",
            "This is the iter 5367, the d1 loss is: 3007.9062, the d2 loss is: -3387.3516, the g loss is: 3460.5625, the ae loss is: 0.007582918, the jacobian loss is:0.12902355\n",
            "This is the iter 5368, the d1 loss is: 3051.3906, the d2 loss is: -3427.5, the g loss is: 3361.664, the ae loss is: 0.006431794, the jacobian loss is:0.09554173\n",
            "This is the iter 5369, the d1 loss is: 3086.7031, the d2 loss is: -3466.0312, the g loss is: 3434.711, the ae loss is: 0.0074480213, the jacobian loss is:0.110149704\n",
            "This is the iter 5370, the d1 loss is: 3075.2344, the d2 loss is: -3454.2188, the g loss is: 3463.6094, the ae loss is: 0.007822826, the jacobian loss is:0.09653687\n",
            "This is the iter 5371, the d1 loss is: 3034.7812, the d2 loss is: -3436.6094, the g loss is: 3399.4844, the ae loss is: 0.0049626436, the jacobian loss is:0.08128148\n",
            "This is the iter 5372, the d1 loss is: 3085.8828, the d2 loss is: -3494.5625, the g loss is: 3472.9062, the ae loss is: 0.0059865327, the jacobian loss is:0.099968135\n",
            "This is the iter 5373, the d1 loss is: 2887.8594, the d2 loss is: -3255.5547, the g loss is: 3197.539, the ae loss is: 0.007033106, the jacobian loss is:0.13046025\n",
            "This is the iter 5374, the d1 loss is: 2831.1562, the d2 loss is: -3214.0703, the g loss is: 3289.3906, the ae loss is: 0.008648386, the jacobian loss is:0.10563475\n",
            "This is the iter 5375, the d1 loss is: 2998.0625, the d2 loss is: -3402.4922, the g loss is: 3412.9531, the ae loss is: 0.0061256695, the jacobian loss is:0.10300755\n",
            "This is the iter 5376, the d1 loss is: 3046.7656, the d2 loss is: -3407.5703, the g loss is: 3394.8906, the ae loss is: 0.0074819573, the jacobian loss is:0.17977566\n",
            "This is the iter 5377, the d1 loss is: 2925.289, the d2 loss is: -3371.3047, the g loss is: 3286.0078, the ae loss is: 0.004891963, the jacobian loss is:0.106155425\n",
            "This is the iter 5378, the d1 loss is: 2979.2188, the d2 loss is: -3417.625, the g loss is: 3410.4219, the ae loss is: 0.006044875, the jacobian loss is:0.09046762\n",
            "This is the iter 5379, the d1 loss is: 2931.4453, the d2 loss is: -3339.6953, the g loss is: 3350.2344, the ae loss is: 0.008437491, the jacobian loss is:0.10320788\n",
            "This is the iter 5380, the d1 loss is: 3052.9531, the d2 loss is: -3417.1172, the g loss is: 3409.586, the ae loss is: 0.0083652055, the jacobian loss is:0.10510071\n",
            "This is the iter 5381, the d1 loss is: 3029.9531, the d2 loss is: -3381.7344, the g loss is: 3389.3203, the ae loss is: 0.0075355237, the jacobian loss is:0.10069197\n",
            "This is the iter 5382, the d1 loss is: 3055.1484, the d2 loss is: -3444.0625, the g loss is: 3473.7734, the ae loss is: 0.007681871, the jacobian loss is:0.106269516\n",
            "This is the iter 5383, the d1 loss is: 2944.6094, the d2 loss is: -3302.289, the g loss is: 3396.4688, the ae loss is: 0.007169004, the jacobian loss is:0.108028665\n",
            "This is the iter 5384, the d1 loss is: 2840.4453, the d2 loss is: -3238.8516, the g loss is: 3253.125, the ae loss is: 0.0054538203, the jacobian loss is:0.09640432\n",
            "This is the iter 5385, the d1 loss is: 3002.4297, the d2 loss is: -3378.1016, the g loss is: 3397.7812, the ae loss is: 0.0054561337, the jacobian loss is:0.0924203\n",
            "This is the iter 5386, the d1 loss is: 3157.5469, the d2 loss is: -3529.2344, the g loss is: 3531.461, the ae loss is: 0.008176506, the jacobian loss is:0.08933543\n",
            "This is the iter 5387, the d1 loss is: 3027.586, the d2 loss is: -3422.7031, the g loss is: 3403.5703, the ae loss is: 0.006665523, the jacobian loss is:0.11725091\n",
            "This is the iter 5388, the d1 loss is: 2964.9688, the d2 loss is: -3365.0, the g loss is: 3449.961, the ae loss is: 0.007764685, the jacobian loss is:0.09427433\n",
            "This is the iter 5389, the d1 loss is: 3041.0, the d2 loss is: -3404.0469, the g loss is: 3382.6172, the ae loss is: 0.008319373, the jacobian loss is:0.10250146\n",
            "This is the iter 5390, the d1 loss is: 3045.2578, the d2 loss is: -3425.6719, the g loss is: 3382.6875, the ae loss is: 0.0068298085, the jacobian loss is:0.118074596\n",
            "This is the iter 5391, the d1 loss is: 2987.1016, the d2 loss is: -3370.3906, the g loss is: 3333.4219, the ae loss is: 0.008354253, the jacobian loss is:0.12474423\n",
            "This is the iter 5392, the d1 loss is: 3137.0312, the d2 loss is: -3506.8516, the g loss is: 3495.3438, the ae loss is: 0.0073503447, the jacobian loss is:0.12869452\n",
            "This is the iter 5393, the d1 loss is: 3117.8047, the d2 loss is: -3466.0156, the g loss is: 3466.9453, the ae loss is: 0.0039222143, the jacobian loss is:0.089685194\n",
            "This is the iter 5394, the d1 loss is: 2938.039, the d2 loss is: -3346.1719, the g loss is: 3339.1016, the ae loss is: 0.0064672483, the jacobian loss is:0.11478299\n",
            "This is the iter 5395, the d1 loss is: 3015.4844, the d2 loss is: -3400.711, the g loss is: 3457.4766, the ae loss is: 0.005129893, the jacobian loss is:0.09176403\n",
            "This is the iter 5396, the d1 loss is: 2793.414, the d2 loss is: -3213.0938, the g loss is: 3179.6719, the ae loss is: 0.007244456, the jacobian loss is:0.09017251\n",
            "This is the iter 5397, the d1 loss is: 2900.9375, the d2 loss is: -3301.4844, the g loss is: 3330.3281, the ae loss is: 0.0064480393, the jacobian loss is:0.10993292\n",
            "This is the iter 5398, the d1 loss is: 2996.8594, the d2 loss is: -3388.6953, the g loss is: 3435.0312, the ae loss is: 0.007880627, the jacobian loss is:0.19837502\n",
            "This is the iter 5399, the d1 loss is: 3004.7656, the d2 loss is: -3368.289, the g loss is: 3341.4297, the ae loss is: 0.008073635, the jacobian loss is:0.122281946\n",
            "This is the iter 5400, the d1 loss is: 3005.1875, the d2 loss is: -3395.1406, the g loss is: 3406.1562, the ae loss is: 0.0081267515, the jacobian loss is:0.117676474\n",
            "0.1975561\n",
            "0.72322667\n",
            "This is the iter 5401, the d1 loss is: 2968.8984, the d2 loss is: -3369.8828, the g loss is: 3382.0469, the ae loss is: 0.006139469, the jacobian loss is:0.108370505\n",
            "This is the iter 5402, the d1 loss is: 3040.3828, the d2 loss is: -3422.211, the g loss is: 3420.2344, the ae loss is: 0.0077973725, the jacobian loss is:0.11160466\n",
            "This is the iter 5403, the d1 loss is: 3035.289, the d2 loss is: -3386.0469, the g loss is: 3394.5, the ae loss is: 0.0071213245, the jacobian loss is:0.113850385\n",
            "This is the iter 5404, the d1 loss is: 3019.9453, the d2 loss is: -3409.4531, the g loss is: 3425.4375, the ae loss is: 0.00828268, the jacobian loss is:0.107638046\n",
            "This is the iter 5405, the d1 loss is: 3093.539, the d2 loss is: -3458.125, the g loss is: 3461.0625, the ae loss is: 0.0067317416, the jacobian loss is:0.11472759\n",
            "This is the iter 5406, the d1 loss is: 3088.3906, the d2 loss is: -3503.5312, the g loss is: 3505.2188, the ae loss is: 0.008985534, the jacobian loss is:0.12327754\n",
            "This is the iter 5407, the d1 loss is: 3074.9766, the d2 loss is: -3400.9219, the g loss is: 3383.0234, the ae loss is: 0.0081072375, the jacobian loss is:0.12626687\n",
            "This is the iter 5408, the d1 loss is: 2795.3984, the d2 loss is: -3200.8516, the g loss is: 3186.3984, the ae loss is: 0.005533944, the jacobian loss is:0.116017714\n",
            "This is the iter 5409, the d1 loss is: 3022.5078, the d2 loss is: -3414.539, the g loss is: 3423.5938, the ae loss is: 0.005926227, the jacobian loss is:0.071614645\n",
            "This is the iter 5410, the d1 loss is: 2992.9375, the d2 loss is: -3362.3438, the g loss is: 3314.2734, the ae loss is: 0.0035907433, the jacobian loss is:0.10623935\n",
            "This is the iter 5411, the d1 loss is: 3161.1953, the d2 loss is: -3493.2266, the g loss is: 3502.6562, the ae loss is: 0.0054871193, the jacobian loss is:0.09121123\n",
            "This is the iter 5412, the d1 loss is: 2804.7031, the d2 loss is: -3176.7812, the g loss is: 3242.5781, the ae loss is: 0.0069554425, the jacobian loss is:0.11493838\n",
            "This is the iter 5413, the d1 loss is: 3061.3984, the d2 loss is: -3425.1719, the g loss is: 3429.5547, the ae loss is: 0.0067720786, the jacobian loss is:0.12549108\n",
            "This is the iter 5414, the d1 loss is: 3049.4844, the d2 loss is: -3438.9531, the g loss is: 3361.9688, the ae loss is: 0.0055670403, the jacobian loss is:0.09524746\n",
            "This is the iter 5415, the d1 loss is: 2971.75, the d2 loss is: -3323.7031, the g loss is: 3342.0625, the ae loss is: 0.006451336, the jacobian loss is:0.09147347\n",
            "This is the iter 5416, the d1 loss is: 3081.6875, the d2 loss is: -3441.625, the g loss is: 3466.8906, the ae loss is: 0.008222546, the jacobian loss is:0.10788317\n",
            "This is the iter 5417, the d1 loss is: 2956.586, the d2 loss is: -3337.9062, the g loss is: 3354.664, the ae loss is: 0.005072929, the jacobian loss is:0.09127137\n",
            "This is the iter 5418, the d1 loss is: 3008.5547, the d2 loss is: -3376.1719, the g loss is: 3414.2812, the ae loss is: 0.007079835, the jacobian loss is:0.09798622\n",
            "This is the iter 5419, the d1 loss is: 2966.8984, the d2 loss is: -3316.289, the g loss is: 3273.4297, the ae loss is: 0.0059974603, the jacobian loss is:0.11181772\n",
            "This is the iter 5420, the d1 loss is: 2905.2031, the d2 loss is: -3266.8281, the g loss is: 3261.9844, the ae loss is: 0.0060243485, the jacobian loss is:0.08661776\n",
            "This is the iter 5421, the d1 loss is: 2808.1875, the d2 loss is: -3222.4844, the g loss is: 3260.6406, the ae loss is: 0.0073069003, the jacobian loss is:0.11207429\n",
            "This is the iter 5422, the d1 loss is: 2832.2812, the d2 loss is: -3231.4844, the g loss is: 3224.0547, the ae loss is: 0.0036239868, the jacobian loss is:0.08674738\n",
            "This is the iter 5423, the d1 loss is: 3084.9062, the d2 loss is: -3447.9375, the g loss is: 3491.0703, the ae loss is: 0.0059855906, the jacobian loss is:0.11005287\n",
            "This is the iter 5424, the d1 loss is: 2992.961, the d2 loss is: -3381.8516, the g loss is: 3367.75, the ae loss is: 0.00869664, the jacobian loss is:0.104660526\n",
            "This is the iter 5425, the d1 loss is: 3118.0625, the d2 loss is: -3459.8203, the g loss is: 3408.0469, the ae loss is: 0.006447802, the jacobian loss is:0.09851409\n",
            "This is the iter 5426, the d1 loss is: 2989.9922, the d2 loss is: -3360.375, the g loss is: 3324.9922, the ae loss is: 0.0077407896, the jacobian loss is:0.21349412\n",
            "This is the iter 5427, the d1 loss is: 3050.5938, the d2 loss is: -3400.5, the g loss is: 3388.0, the ae loss is: 0.005392087, the jacobian loss is:0.12883586\n",
            "This is the iter 5428, the d1 loss is: 2963.2188, the d2 loss is: -3363.5469, the g loss is: 3336.3203, the ae loss is: 0.010313446, the jacobian loss is:0.1203133\n",
            "This is the iter 5429, the d1 loss is: 3012.5938, the d2 loss is: -3353.664, the g loss is: 3385.039, the ae loss is: 0.007837657, the jacobian loss is:0.12535758\n",
            "This is the iter 5430, the d1 loss is: 3108.7344, the d2 loss is: -3492.8203, the g loss is: 3500.3516, the ae loss is: 0.006766494, the jacobian loss is:0.080214955\n",
            "This is the iter 5431, the d1 loss is: 2834.8281, the d2 loss is: -3208.1172, the g loss is: 3241.9062, the ae loss is: 0.0071104374, the jacobian loss is:0.11860713\n",
            "This is the iter 5432, the d1 loss is: 2978.8672, the d2 loss is: -3345.3906, the g loss is: 3307.9688, the ae loss is: 0.007822867, the jacobian loss is:0.24564347\n",
            "This is the iter 5433, the d1 loss is: 3009.1719, the d2 loss is: -3368.0938, the g loss is: 3349.4844, the ae loss is: 0.006684934, the jacobian loss is:0.09845367\n",
            "This is the iter 5434, the d1 loss is: 2975.8281, the d2 loss is: -3343.4297, the g loss is: 3321.914, the ae loss is: 0.011207947, the jacobian loss is:0.1085186\n",
            "This is the iter 5435, the d1 loss is: 3030.8047, the d2 loss is: -3366.4375, the g loss is: 3408.4297, the ae loss is: 0.008197401, the jacobian loss is:0.108967796\n",
            "This is the iter 5436, the d1 loss is: 3015.7188, the d2 loss is: -3382.6016, the g loss is: 3430.3984, the ae loss is: 0.008115474, the jacobian loss is:0.091949776\n",
            "This is the iter 5437, the d1 loss is: 3016.875, the d2 loss is: -3409.461, the g loss is: 3397.6094, the ae loss is: 0.008100865, the jacobian loss is:0.12302911\n",
            "This is the iter 5438, the d1 loss is: 3053.3125, the d2 loss is: -3428.5547, the g loss is: 3385.25, the ae loss is: 0.006950577, the jacobian loss is:0.1024465\n",
            "This is the iter 5439, the d1 loss is: 2941.5547, the d2 loss is: -3285.7266, the g loss is: 3309.7344, the ae loss is: 0.0045809736, the jacobian loss is:0.17115039\n",
            "This is the iter 5440, the d1 loss is: 2876.8906, the d2 loss is: -3230.1484, the g loss is: 3137.5156, the ae loss is: 0.008683066, the jacobian loss is:0.10345766\n",
            "This is the iter 5441, the d1 loss is: 2967.3438, the d2 loss is: -3314.5625, the g loss is: 3360.6094, the ae loss is: 0.0043623527, the jacobian loss is:0.08520136\n",
            "This is the iter 5442, the d1 loss is: 2901.0234, the d2 loss is: -3330.4219, the g loss is: 3363.7656, the ae loss is: 0.010201456, the jacobian loss is:0.122155\n",
            "This is the iter 5443, the d1 loss is: 3137.5781, the d2 loss is: -3475.2344, the g loss is: 3443.289, the ae loss is: 0.005343123, the jacobian loss is:0.10539682\n",
            "This is the iter 5444, the d1 loss is: 2832.2422, the d2 loss is: -3195.875, the g loss is: 3149.6797, the ae loss is: 0.006350048, the jacobian loss is:0.10197724\n",
            "This is the iter 5445, the d1 loss is: 2857.164, the d2 loss is: -3253.7031, the g loss is: 3233.0234, the ae loss is: 0.005716074, the jacobian loss is:0.12677854\n",
            "This is the iter 5446, the d1 loss is: 3014.0703, the d2 loss is: -3369.8594, the g loss is: 3349.0547, the ae loss is: 0.008878453, the jacobian loss is:0.09439355\n",
            "This is the iter 5447, the d1 loss is: 2813.7812, the d2 loss is: -3197.2266, the g loss is: 3244.7344, the ae loss is: 0.009182135, the jacobian loss is:0.09570992\n",
            "This is the iter 5448, the d1 loss is: 2993.2734, the d2 loss is: -3387.1562, the g loss is: 3354.7812, the ae loss is: 0.0061476557, the jacobian loss is:0.07329354\n",
            "This is the iter 5449, the d1 loss is: 3023.4219, the d2 loss is: -3359.625, the g loss is: 3393.75, the ae loss is: 0.007081749, the jacobian loss is:0.12297392\n",
            "This is the iter 5450, the d1 loss is: 2996.6172, the d2 loss is: -3390.1484, the g loss is: 3320.8438, the ae loss is: 0.007982101, the jacobian loss is:0.10051952\n",
            "This is the iter 5451, the d1 loss is: 2954.8672, the d2 loss is: -3330.8672, the g loss is: 3331.5, the ae loss is: 0.0054787197, the jacobian loss is:0.10945365\n",
            "This is the iter 5452, the d1 loss is: 2986.1484, the d2 loss is: -3346.664, the g loss is: 3279.7969, the ae loss is: 0.006934348, the jacobian loss is:0.09335598\n",
            "This is the iter 5453, the d1 loss is: 2952.586, the d2 loss is: -3316.1094, the g loss is: 3370.3594, the ae loss is: 0.0051249075, the jacobian loss is:0.11146367\n",
            "This is the iter 5454, the d1 loss is: 2718.7969, the d2 loss is: -3071.1016, the g loss is: 3103.0469, the ae loss is: 0.008466911, the jacobian loss is:0.094411105\n",
            "This is the iter 5455, the d1 loss is: 2856.1328, the d2 loss is: -3245.039, the g loss is: 3293.1328, the ae loss is: 0.007329165, the jacobian loss is:0.120020285\n",
            "This is the iter 5456, the d1 loss is: 2908.5, the d2 loss is: -3291.0312, the g loss is: 3323.5078, the ae loss is: 0.008297924, the jacobian loss is:0.09736808\n",
            "This is the iter 5457, the d1 loss is: 2867.0703, the d2 loss is: -3239.9453, the g loss is: 3212.4062, the ae loss is: 0.00667499, the jacobian loss is:0.08448684\n",
            "This is the iter 5458, the d1 loss is: 2948.3281, the d2 loss is: -3294.789, the g loss is: 3294.2422, the ae loss is: 0.008027441, the jacobian loss is:0.40013194\n",
            "This is the iter 5459, the d1 loss is: 2944.1406, the d2 loss is: -3295.3828, the g loss is: 3336.75, the ae loss is: 0.006600288, the jacobian loss is:0.106332146\n",
            "This is the iter 5460, the d1 loss is: 2949.2969, the d2 loss is: -3327.6484, the g loss is: 3292.9062, the ae loss is: 0.009444594, the jacobian loss is:0.10226958\n",
            "This is the iter 5461, the d1 loss is: 2819.0781, the d2 loss is: -3239.6562, the g loss is: 3225.125, the ae loss is: 0.007514618, the jacobian loss is:0.10411664\n",
            "This is the iter 5462, the d1 loss is: 3078.7812, the d2 loss is: -3413.1094, the g loss is: 3440.1172, the ae loss is: 0.0059162993, the jacobian loss is:0.12233144\n",
            "This is the iter 5463, the d1 loss is: 2981.7188, the d2 loss is: -3343.4531, the g loss is: 3413.8125, the ae loss is: 0.008958881, the jacobian loss is:0.09299959\n",
            "This is the iter 5464, the d1 loss is: 3067.5703, the d2 loss is: -3449.1328, the g loss is: 3348.8672, the ae loss is: 0.009596408, the jacobian loss is:0.12316659\n",
            "This is the iter 5465, the d1 loss is: 3070.3203, the d2 loss is: -3418.711, the g loss is: 3361.7656, the ae loss is: 0.0070209326, the jacobian loss is:0.10078398\n",
            "This is the iter 5466, the d1 loss is: 3105.4375, the d2 loss is: -3479.6328, the g loss is: 3503.5, the ae loss is: 0.008545456, the jacobian loss is:0.09225682\n",
            "This is the iter 5467, the d1 loss is: 2993.2578, the d2 loss is: -3347.836, the g loss is: 3340.1094, the ae loss is: 0.004632161, the jacobian loss is:0.111181095\n",
            "This is the iter 5468, the d1 loss is: 2977.1719, the d2 loss is: -3325.875, the g loss is: 3359.5234, the ae loss is: 0.008800885, the jacobian loss is:0.09829307\n",
            "This is the iter 5469, the d1 loss is: 3019.875, the d2 loss is: -3385.6016, the g loss is: 3369.4062, the ae loss is: 0.0076139485, the jacobian loss is:0.11245312\n",
            "This is the iter 5470, the d1 loss is: 3008.2188, the d2 loss is: -3369.6953, the g loss is: 3322.2031, the ae loss is: 0.00815328, the jacobian loss is:0.119194575\n",
            "This is the iter 5471, the d1 loss is: 3061.4375, the d2 loss is: -3410.2031, the g loss is: 3409.7422, the ae loss is: 0.0064466037, the jacobian loss is:0.09983912\n",
            "This is the iter 5472, the d1 loss is: 2950.164, the d2 loss is: -3321.6406, the g loss is: 3345.875, the ae loss is: 0.006023273, the jacobian loss is:0.08900195\n",
            "This is the iter 5473, the d1 loss is: 3097.5781, the d2 loss is: -3418.5234, the g loss is: 3396.6094, the ae loss is: 0.007965387, the jacobian loss is:0.11540587\n",
            "This is the iter 5474, the d1 loss is: 3011.125, the d2 loss is: -3368.1562, the g loss is: 3324.3281, the ae loss is: 0.009629432, the jacobian loss is:0.100848295\n",
            "This is the iter 5475, the d1 loss is: 3000.6016, the d2 loss is: -3339.5703, the g loss is: 3350.289, the ae loss is: 0.0077786157, the jacobian loss is:0.10997237\n",
            "This is the iter 5476, the d1 loss is: 2956.3438, the d2 loss is: -3384.6172, the g loss is: 3398.7578, the ae loss is: 0.010384083, the jacobian loss is:0.12714335\n",
            "This is the iter 5477, the d1 loss is: 3028.4297, the d2 loss is: -3395.9062, the g loss is: 3388.8438, the ae loss is: 0.0066539207, the jacobian loss is:0.13254547\n",
            "This is the iter 5478, the d1 loss is: 2940.461, the d2 loss is: -3284.1719, the g loss is: 3294.5938, the ae loss is: 0.0060664816, the jacobian loss is:0.11118404\n",
            "This is the iter 5479, the d1 loss is: 3056.375, the d2 loss is: -3449.875, the g loss is: 3442.7969, the ae loss is: 0.005995035, the jacobian loss is:0.097615525\n",
            "This is the iter 5480, the d1 loss is: 2937.4453, the d2 loss is: -3272.125, the g loss is: 3237.1484, the ae loss is: 0.0067737764, the jacobian loss is:0.13142794\n",
            "This is the iter 5481, the d1 loss is: 2968.2578, the d2 loss is: -3342.211, the g loss is: 3300.039, the ae loss is: 0.0066256127, the jacobian loss is:0.124225445\n",
            "This is the iter 5482, the d1 loss is: 3020.5156, the d2 loss is: -3399.7734, the g loss is: 3376.5, the ae loss is: 0.00388188, the jacobian loss is:0.11910799\n",
            "This is the iter 5483, the d1 loss is: 3061.2656, the d2 loss is: -3401.8438, the g loss is: 3394.5703, the ae loss is: 0.0071224608, the jacobian loss is:0.09709484\n",
            "This is the iter 5484, the d1 loss is: 2923.8828, the d2 loss is: -3302.2344, the g loss is: 3338.7578, the ae loss is: 0.007914828, the jacobian loss is:0.12490119\n",
            "This is the iter 5485, the d1 loss is: 2994.1328, the d2 loss is: -3353.3906, the g loss is: 3382.9375, the ae loss is: 0.006224541, the jacobian loss is:0.10769822\n",
            "This is the iter 5486, the d1 loss is: 3031.8672, the d2 loss is: -3384.8438, the g loss is: 3382.2422, the ae loss is: 0.008637198, the jacobian loss is:0.10072674\n",
            "This is the iter 5487, the d1 loss is: 3039.2812, the d2 loss is: -3387.1719, the g loss is: 3327.5547, the ae loss is: 0.006152867, the jacobian loss is:0.103097625\n",
            "This is the iter 5488, the d1 loss is: 3072.8047, the d2 loss is: -3421.6719, the g loss is: 3444.0781, the ae loss is: 0.0064119967, the jacobian loss is:0.10452974\n",
            "This is the iter 5489, the d1 loss is: 3041.4688, the d2 loss is: -3405.914, the g loss is: 3363.875, the ae loss is: 0.0066129426, the jacobian loss is:0.12174955\n",
            "This is the iter 5490, the d1 loss is: 2986.7656, the d2 loss is: -3364.375, the g loss is: 3370.3828, the ae loss is: 0.0077089285, the jacobian loss is:0.10308489\n",
            "This is the iter 5491, the d1 loss is: 2865.7188, the d2 loss is: -3224.1875, the g loss is: 3315.9375, the ae loss is: 0.0055906866, the jacobian loss is:0.10912338\n",
            "This is the iter 5492, the d1 loss is: 2973.7188, the d2 loss is: -3357.6172, the g loss is: 3308.5781, the ae loss is: 0.004514544, the jacobian loss is:0.12781808\n",
            "This is the iter 5493, the d1 loss is: 3025.3438, the d2 loss is: -3365.6484, the g loss is: 3352.5625, the ae loss is: 0.005930753, the jacobian loss is:0.1202651\n",
            "This is the iter 5494, the d1 loss is: 2987.8281, the d2 loss is: -3322.7656, the g loss is: 3337.789, the ae loss is: 0.0059994953, the jacobian loss is:0.08461679\n",
            "This is the iter 5495, the d1 loss is: 3024.6328, the d2 loss is: -3429.6328, the g loss is: 3445.164, the ae loss is: 0.008682376, the jacobian loss is:0.09920868\n",
            "This is the iter 5496, the d1 loss is: 3112.2734, the d2 loss is: -3467.9297, the g loss is: 3408.0547, the ae loss is: 0.0056503545, the jacobian loss is:0.10452048\n",
            "This is the iter 5497, the d1 loss is: 3022.2422, the d2 loss is: -3406.6797, the g loss is: 3407.875, the ae loss is: 0.011551493, the jacobian loss is:0.09960506\n",
            "This is the iter 5498, the d1 loss is: 3089.6172, the d2 loss is: -3423.6797, the g loss is: 3452.8828, the ae loss is: 0.0065907114, the jacobian loss is:0.12961036\n",
            "This is the iter 5499, the d1 loss is: 3051.3438, the d2 loss is: -3470.3047, the g loss is: 3439.625, the ae loss is: 0.0050950116, the jacobian loss is:0.114306755\n",
            "This is the iter 5500, the d1 loss is: 2977.4922, the d2 loss is: -3374.1719, the g loss is: 3372.1016, the ae loss is: 0.010041492, the jacobian loss is:0.11498662\n",
            "0.20303704\n",
            "0.7432338\n",
            "This is the iter 5501, the d1 loss is: 3117.664, the d2 loss is: -3469.4062, the g loss is: 3410.75, the ae loss is: 0.0055498886, the jacobian loss is:0.13464363\n",
            "This is the iter 5502, the d1 loss is: 3069.8906, the d2 loss is: -3441.6875, the g loss is: 3469.4453, the ae loss is: 0.0050320313, the jacobian loss is:0.10114818\n",
            "This is the iter 5503, the d1 loss is: 2823.289, the d2 loss is: -3165.8594, the g loss is: 3192.5, the ae loss is: 0.0061234953, the jacobian loss is:0.11005741\n",
            "This is the iter 5504, the d1 loss is: 2881.7969, the d2 loss is: -3219.8047, the g loss is: 3238.7188, the ae loss is: 0.008961979, the jacobian loss is:0.09609293\n",
            "This is the iter 5505, the d1 loss is: 2915.4219, the d2 loss is: -3289.8281, the g loss is: 3281.0938, the ae loss is: 0.0075831767, the jacobian loss is:0.10548059\n",
            "This is the iter 5506, the d1 loss is: 2840.7188, the d2 loss is: -3219.0625, the g loss is: 3172.5938, the ae loss is: 0.0061919494, the jacobian loss is:0.1281507\n",
            "This is the iter 5507, the d1 loss is: 3001.2031, the d2 loss is: -3352.5547, the g loss is: 3324.125, the ae loss is: 0.0066734264, the jacobian loss is:0.10158055\n",
            "This is the iter 5508, the d1 loss is: 3043.5312, the d2 loss is: -3428.289, the g loss is: 3405.0469, the ae loss is: 0.0074266386, the jacobian loss is:0.09261112\n",
            "This is the iter 5509, the d1 loss is: 3081.7344, the d2 loss is: -3451.1719, the g loss is: 3409.8984, the ae loss is: 0.007965268, the jacobian loss is:0.117408104\n",
            "This is the iter 5510, the d1 loss is: 3029.125, the d2 loss is: -3401.7656, the g loss is: 3408.6406, the ae loss is: 0.009494322, the jacobian loss is:0.11991017\n",
            "This is the iter 5511, the d1 loss is: 3034.9297, the d2 loss is: -3406.7031, the g loss is: 3431.4688, the ae loss is: 0.0063979495, the jacobian loss is:0.11032454\n",
            "This is the iter 5512, the d1 loss is: 3043.6328, the d2 loss is: -3410.0703, the g loss is: 3413.5234, the ae loss is: 0.0055909664, the jacobian loss is:0.13372634\n",
            "This is the iter 5513, the d1 loss is: 2945.6719, the d2 loss is: -3363.6875, the g loss is: 3380.7969, the ae loss is: 0.009308973, the jacobian loss is:0.08593935\n",
            "This is the iter 5514, the d1 loss is: 2914.664, the d2 loss is: -3298.539, the g loss is: 3260.5, the ae loss is: 0.0070359753, the jacobian loss is:0.09610819\n",
            "This is the iter 5515, the d1 loss is: 2950.586, the d2 loss is: -3313.6328, the g loss is: 3295.5156, the ae loss is: 0.007489008, the jacobian loss is:0.091741905\n",
            "This is the iter 5516, the d1 loss is: 2866.1562, the d2 loss is: -3219.8047, the g loss is: 3208.5781, the ae loss is: 0.005722958, the jacobian loss is:0.10253706\n",
            "This is the iter 5517, the d1 loss is: 2922.8281, the d2 loss is: -3270.125, the g loss is: 3284.1562, the ae loss is: 0.006268539, the jacobian loss is:0.11849381\n",
            "This is the iter 5518, the d1 loss is: 3064.1562, the d2 loss is: -3414.4922, the g loss is: 3447.0156, the ae loss is: 0.0080279, the jacobian loss is:0.085102476\n",
            "This is the iter 5519, the d1 loss is: 3134.8984, the d2 loss is: -3445.9297, the g loss is: 3437.2656, the ae loss is: 0.007460599, the jacobian loss is:0.1535181\n",
            "This is the iter 5520, the d1 loss is: 3093.0156, the d2 loss is: -3444.2656, the g loss is: 3410.0625, the ae loss is: 0.008321859, the jacobian loss is:0.10384587\n",
            "This is the iter 5521, the d1 loss is: 3045.8125, the d2 loss is: -3409.7578, the g loss is: 3389.3047, the ae loss is: 0.004819967, the jacobian loss is:0.12037048\n",
            "This is the iter 5522, the d1 loss is: 3008.289, the d2 loss is: -3346.836, the g loss is: 3369.6406, the ae loss is: 0.006218861, the jacobian loss is:0.09766157\n",
            "This is the iter 5523, the d1 loss is: 2837.8984, the d2 loss is: -3199.7734, the g loss is: 3138.6484, the ae loss is: 0.0061805495, the jacobian loss is:0.09248251\n",
            "This is the iter 5524, the d1 loss is: 2907.5938, the d2 loss is: -3326.75, the g loss is: 3297.4062, the ae loss is: 0.0077206166, the jacobian loss is:0.10664715\n",
            "This is the iter 5525, the d1 loss is: 2941.4688, the d2 loss is: -3295.5078, the g loss is: 3344.3125, the ae loss is: 0.0074890023, the jacobian loss is:0.08552511\n",
            "This is the iter 5526, the d1 loss is: 2905.289, the d2 loss is: -3226.9844, the g loss is: 3385.2188, the ae loss is: 0.005754648, the jacobian loss is:0.10212497\n",
            "This is the iter 5527, the d1 loss is: 2931.2422, the d2 loss is: -3312.3438, the g loss is: 3325.1875, the ae loss is: 0.0072111683, the jacobian loss is:0.09986054\n",
            "This is the iter 5528, the d1 loss is: 2969.9531, the d2 loss is: -3321.4453, the g loss is: 3305.1719, the ae loss is: 0.0056729317, the jacobian loss is:0.11094713\n",
            "This is the iter 5529, the d1 loss is: 2900.5469, the d2 loss is: -3303.9375, the g loss is: 3324.7422, the ae loss is: 0.005390751, the jacobian loss is:0.2565074\n",
            "This is the iter 5530, the d1 loss is: 3004.125, the d2 loss is: -3366.2812, the g loss is: 3367.7266, the ae loss is: 0.0064605656, the jacobian loss is:0.07869267\n",
            "This is the iter 5531, the d1 loss is: 2895.6875, the d2 loss is: -3255.6406, the g loss is: 3282.9375, the ae loss is: 0.008448472, the jacobian loss is:0.11993409\n",
            "This is the iter 5532, the d1 loss is: 3005.2031, the d2 loss is: -3379.6719, the g loss is: 3359.4766, the ae loss is: 0.007739245, the jacobian loss is:0.12392118\n",
            "This is the iter 5533, the d1 loss is: 3018.0781, the d2 loss is: -3373.7656, the g loss is: 3350.9688, the ae loss is: 0.00678509, the jacobian loss is:0.110569775\n",
            "This is the iter 5534, the d1 loss is: 3005.0234, the d2 loss is: -3392.3672, the g loss is: 3377.5547, the ae loss is: 0.0070012687, the jacobian loss is:0.10317127\n",
            "This is the iter 5535, the d1 loss is: 2875.9062, the d2 loss is: -3227.4922, the g loss is: 3209.1953, the ae loss is: 0.0060778945, the jacobian loss is:0.1343863\n",
            "This is the iter 5536, the d1 loss is: 3050.2422, the d2 loss is: -3402.1406, the g loss is: 3343.375, the ae loss is: 0.00462899, the jacobian loss is:0.086769335\n",
            "This is the iter 5537, the d1 loss is: 2742.8906, the d2 loss is: -3073.6484, the g loss is: 3081.875, the ae loss is: 0.0071339104, the jacobian loss is:0.118959785\n",
            "This is the iter 5538, the d1 loss is: 2869.1719, the d2 loss is: -3190.4844, the g loss is: 3231.6094, the ae loss is: 0.009708285, the jacobian loss is:0.14265895\n",
            "This is the iter 5539, the d1 loss is: 3000.836, the d2 loss is: -3334.5, the g loss is: 3308.7969, the ae loss is: 0.009384737, the jacobian loss is:0.11276969\n",
            "This is the iter 5540, the d1 loss is: 2747.4062, the d2 loss is: -3090.0469, the g loss is: 3091.414, the ae loss is: 0.0071256612, the jacobian loss is:0.12059981\n",
            "This is the iter 5541, the d1 loss is: 2878.7031, the d2 loss is: -3187.3281, the g loss is: 3212.0156, the ae loss is: 0.009633649, the jacobian loss is:0.14644827\n",
            "This is the iter 5542, the d1 loss is: 2930.6016, the d2 loss is: -3252.4922, the g loss is: 3290.3203, the ae loss is: 0.006872182, the jacobian loss is:0.112423055\n",
            "This is the iter 5543, the d1 loss is: 2999.2266, the d2 loss is: -3345.7656, the g loss is: 3321.3516, the ae loss is: 0.0066142944, the jacobian loss is:0.11793084\n",
            "This is the iter 5544, the d1 loss is: 2947.1172, the d2 loss is: -3292.9219, the g loss is: 3310.2422, the ae loss is: 0.0076644914, the jacobian loss is:0.10446043\n",
            "This is the iter 5545, the d1 loss is: 2980.6562, the d2 loss is: -3321.914, the g loss is: 3280.0938, the ae loss is: 0.0057473863, the jacobian loss is:0.14922372\n",
            "This is the iter 5546, the d1 loss is: 2896.1953, the d2 loss is: -3240.1406, the g loss is: 3196.8594, the ae loss is: 0.008714072, the jacobian loss is:0.13210653\n",
            "This is the iter 5547, the d1 loss is: 2912.5234, the d2 loss is: -3248.3281, the g loss is: 3236.539, the ae loss is: 0.007040655, the jacobian loss is:0.13832985\n",
            "This is the iter 5548, the d1 loss is: 2960.289, the d2 loss is: -3318.2188, the g loss is: 3304.9062, the ae loss is: 0.0081956545, the jacobian loss is:0.097250044\n",
            "This is the iter 5549, the d1 loss is: 2913.8984, the d2 loss is: -3287.8828, the g loss is: 3320.25, the ae loss is: 0.0065925973, the jacobian loss is:0.10526551\n",
            "This is the iter 5550, the d1 loss is: 2775.2656, the d2 loss is: -3171.7969, the g loss is: 3173.0469, the ae loss is: 0.006234783, the jacobian loss is:0.14610432\n",
            "This is the iter 5551, the d1 loss is: 2692.0312, the d2 loss is: -3032.3125, the g loss is: 3018.9297, the ae loss is: 0.0097782165, the jacobian loss is:0.14182955\n",
            "This is the iter 5552, the d1 loss is: 2918.5312, the d2 loss is: -3269.3281, the g loss is: 3237.7266, the ae loss is: 0.0070147174, the jacobian loss is:0.10095585\n",
            "This is the iter 5553, the d1 loss is: 2934.6094, the d2 loss is: -3274.8906, the g loss is: 3366.0156, the ae loss is: 0.005989616, the jacobian loss is:0.10426216\n",
            "This is the iter 5554, the d1 loss is: 2907.2344, the d2 loss is: -3259.2969, the g loss is: 3226.789, the ae loss is: 0.0058375373, the jacobian loss is:0.093331724\n",
            "This is the iter 5555, the d1 loss is: 2958.7344, the d2 loss is: -3244.914, the g loss is: 3304.789, the ae loss is: 0.0084391255, the jacobian loss is:0.121789716\n",
            "This is the iter 5556, the d1 loss is: 3093.1094, the d2 loss is: -3486.3125, the g loss is: 3492.1953, the ae loss is: 0.0078008603, the jacobian loss is:0.14591359\n",
            "This is the iter 5557, the d1 loss is: 3073.0156, the d2 loss is: -3392.3281, the g loss is: 3334.3906, the ae loss is: 0.0078011095, the jacobian loss is:0.15652926\n",
            "This is the iter 5558, the d1 loss is: 2926.1016, the d2 loss is: -3291.4062, the g loss is: 3300.8438, the ae loss is: 0.00822083, the jacobian loss is:0.09386791\n",
            "This is the iter 5559, the d1 loss is: 2933.5, the d2 loss is: -3225.1562, the g loss is: 3335.3203, the ae loss is: 0.005939449, the jacobian loss is:0.11349025\n",
            "This is the iter 5560, the d1 loss is: 3024.3984, the d2 loss is: -3353.5469, the g loss is: 3357.711, the ae loss is: 0.006496238, the jacobian loss is:0.27471378\n",
            "This is the iter 5561, the d1 loss is: 2949.914, the d2 loss is: -3273.8594, the g loss is: 3281.6562, the ae loss is: 0.00569964, the jacobian loss is:0.09578994\n",
            "This is the iter 5562, the d1 loss is: 2892.3281, the d2 loss is: -3248.8281, the g loss is: 3245.1719, the ae loss is: 0.010371194, the jacobian loss is:0.28406194\n",
            "This is the iter 5563, the d1 loss is: 2937.5156, the d2 loss is: -3262.7266, the g loss is: 3271.9453, the ae loss is: 0.009628948, the jacobian loss is:0.10756478\n",
            "This is the iter 5564, the d1 loss is: 2972.2812, the d2 loss is: -3319.5078, the g loss is: 3319.7578, the ae loss is: 0.008052002, the jacobian loss is:0.12939966\n",
            "This is the iter 5565, the d1 loss is: 2888.3438, the d2 loss is: -3227.6562, the g loss is: 3273.5781, the ae loss is: 0.006705697, the jacobian loss is:0.10428708\n",
            "This is the iter 5566, the d1 loss is: 2989.6484, the d2 loss is: -3366.3984, the g loss is: 3322.2344, the ae loss is: 0.008026786, the jacobian loss is:0.25554225\n",
            "This is the iter 5567, the d1 loss is: 2893.2969, the d2 loss is: -3241.2656, the g loss is: 3342.211, the ae loss is: 0.0071800756, the jacobian loss is:0.11363786\n",
            "This is the iter 5568, the d1 loss is: 3030.836, the d2 loss is: -3372.086, the g loss is: 3377.0547, the ae loss is: 0.010635005, the jacobian loss is:0.10030057\n",
            "This is the iter 5569, the d1 loss is: 2944.5078, the d2 loss is: -3307.961, the g loss is: 3285.9531, the ae loss is: 0.0045417906, the jacobian loss is:0.10658352\n",
            "This is the iter 5570, the d1 loss is: 2975.5469, the d2 loss is: -3322.8047, the g loss is: 3256.1094, the ae loss is: 0.0069628633, the jacobian loss is:0.10388274\n",
            "This is the iter 5571, the d1 loss is: 2987.2578, the d2 loss is: -3333.8828, the g loss is: 3283.5156, the ae loss is: 0.007273879, the jacobian loss is:0.14287277\n",
            "This is the iter 5572, the d1 loss is: 2918.5078, the d2 loss is: -3315.3125, the g loss is: 3313.5625, the ae loss is: 0.008339813, the jacobian loss is:0.12282352\n",
            "This is the iter 5573, the d1 loss is: 2908.7031, the d2 loss is: -3218.2812, the g loss is: 3269.2578, the ae loss is: 0.005838233, the jacobian loss is:0.12131384\n",
            "This is the iter 5574, the d1 loss is: 3268.1328, the d2 loss is: -3594.2344, the g loss is: 3608.211, the ae loss is: 0.0046983226, the jacobian loss is:0.09621867\n",
            "This is the iter 5575, the d1 loss is: 2988.9688, the d2 loss is: -3363.789, the g loss is: 3399.9453, the ae loss is: 0.0073230136, the jacobian loss is:0.120562166\n",
            "This is the iter 5576, the d1 loss is: 3254.0938, the d2 loss is: -3611.1094, the g loss is: 3619.5, the ae loss is: 0.006089548, the jacobian loss is:0.10329329\n",
            "This is the iter 5577, the d1 loss is: 3048.6484, the d2 loss is: -3385.4766, the g loss is: 3387.7578, the ae loss is: 0.0062688394, the jacobian loss is:0.101922244\n",
            "This is the iter 5578, the d1 loss is: 2918.6328, the d2 loss is: -3256.875, the g loss is: 3285.6953, the ae loss is: 0.006355359, the jacobian loss is:0.10885236\n",
            "This is the iter 5579, the d1 loss is: 2903.4688, the d2 loss is: -3242.8672, the g loss is: 3317.0781, the ae loss is: 0.007072323, the jacobian loss is:0.12727071\n",
            "This is the iter 5580, the d1 loss is: 2861.0938, the d2 loss is: -3221.6406, the g loss is: 3249.289, the ae loss is: 0.005021505, the jacobian loss is:0.11399088\n",
            "This is the iter 5581, the d1 loss is: 3001.914, the d2 loss is: -3351.1797, the g loss is: 3331.5234, the ae loss is: 0.010453542, the jacobian loss is:0.103767425\n",
            "This is the iter 5582, the d1 loss is: 3061.6328, the d2 loss is: -3394.0234, the g loss is: 3373.3828, the ae loss is: 0.006918167, the jacobian loss is:0.087382495\n",
            "This is the iter 5583, the d1 loss is: 2963.336, the d2 loss is: -3332.5234, the g loss is: 3312.4375, the ae loss is: 0.0063613206, the jacobian loss is:0.097563595\n",
            "This is the iter 5584, the d1 loss is: 3042.375, the d2 loss is: -3373.7969, the g loss is: 3309.8906, the ae loss is: 0.007450283, the jacobian loss is:0.1587452\n",
            "This is the iter 5585, the d1 loss is: 2874.961, the d2 loss is: -3191.125, the g loss is: 3308.7578, the ae loss is: 0.0062696906, the jacobian loss is:0.10197569\n",
            "This is the iter 5586, the d1 loss is: 2944.0625, the d2 loss is: -3307.0078, the g loss is: 3307.0781, the ae loss is: 0.0066725, the jacobian loss is:0.1114445\n",
            "This is the iter 5587, the d1 loss is: 2884.5234, the d2 loss is: -3189.7344, the g loss is: 3226.164, the ae loss is: 0.006732691, the jacobian loss is:0.1083525\n",
            "This is the iter 5588, the d1 loss is: 2951.8516, the d2 loss is: -3307.3438, the g loss is: 3328.625, the ae loss is: 0.00898679, the jacobian loss is:0.09691615\n",
            "This is the iter 5589, the d1 loss is: 2739.4375, the d2 loss is: -3065.3594, the g loss is: 3063.9531, the ae loss is: 0.0061899773, the jacobian loss is:0.13715485\n",
            "This is the iter 5590, the d1 loss is: 2738.6719, the d2 loss is: -3072.0, the g loss is: 3077.2344, the ae loss is: 0.0074169654, the jacobian loss is:0.117370024\n",
            "This is the iter 5591, the d1 loss is: 2776.25, the d2 loss is: -3115.3828, the g loss is: 3135.75, the ae loss is: 0.0054858206, the jacobian loss is:0.13178445\n",
            "This is the iter 5592, the d1 loss is: 2908.7344, the d2 loss is: -3271.8438, the g loss is: 3269.375, the ae loss is: 0.006099209, the jacobian loss is:0.09589967\n",
            "This is the iter 5593, the d1 loss is: 2953.8203, the d2 loss is: -3272.3047, the g loss is: 3211.414, the ae loss is: 0.005738621, the jacobian loss is:0.10481832\n",
            "This is the iter 5594, the d1 loss is: 2909.5469, the d2 loss is: -3256.4844, the g loss is: 3307.9453, the ae loss is: 0.006285294, the jacobian loss is:0.11314087\n",
            "This is the iter 5595, the d1 loss is: 2925.3984, the d2 loss is: -3247.6719, the g loss is: 3286.0625, the ae loss is: 0.005153527, the jacobian loss is:0.11960842\n",
            "This is the iter 5596, the d1 loss is: 2947.2266, the d2 loss is: -3329.414, the g loss is: 3289.461, the ae loss is: 0.006303517, the jacobian loss is:0.13549046\n",
            "This is the iter 5597, the d1 loss is: 3017.6562, the d2 loss is: -3320.1328, the g loss is: 3265.5078, the ae loss is: 0.007085534, the jacobian loss is:0.10728256\n",
            "This is the iter 5598, the d1 loss is: 2735.8828, the d2 loss is: -3033.1484, the g loss is: 3096.586, the ae loss is: 0.007670624, the jacobian loss is:0.12122472\n",
            "This is the iter 5599, the d1 loss is: 2968.375, the d2 loss is: -3343.336, the g loss is: 3283.3516, the ae loss is: 0.00844316, the jacobian loss is:0.13169253\n",
            "This is the iter 5600, the d1 loss is: 2901.2734, the d2 loss is: -3212.6406, the g loss is: 3209.4375, the ae loss is: 0.006585291, the jacobian loss is:0.28606388\n",
            "0.2175825\n",
            "0.80449355\n",
            "This is the iter 5601, the d1 loss is: 2981.5312, the d2 loss is: -3295.8594, the g loss is: 3251.3203, the ae loss is: 0.0072000893, the jacobian loss is:0.13239275\n",
            "This is the iter 5602, the d1 loss is: 2883.4844, the d2 loss is: -3203.4688, the g loss is: 3181.6797, the ae loss is: 0.00821587, the jacobian loss is:0.15758646\n",
            "This is the iter 5603, the d1 loss is: 3032.3047, the d2 loss is: -3329.914, the g loss is: 3317.9219, the ae loss is: 0.0070632687, the jacobian loss is:0.12157964\n",
            "This is the iter 5604, the d1 loss is: 2999.9375, the d2 loss is: -3336.6953, the g loss is: 3332.5781, the ae loss is: 0.008566733, the jacobian loss is:0.42068478\n",
            "This is the iter 5605, the d1 loss is: 2941.4062, the d2 loss is: -3320.711, the g loss is: 3297.0703, the ae loss is: 0.0072749024, the jacobian loss is:0.09953123\n",
            "This is the iter 5606, the d1 loss is: 2853.125, the d2 loss is: -3202.289, the g loss is: 3229.7188, the ae loss is: 0.008199309, the jacobian loss is:0.15009998\n",
            "This is the iter 5607, the d1 loss is: 2895.8125, the d2 loss is: -3268.4688, the g loss is: 3271.914, the ae loss is: 0.0070748846, the jacobian loss is:0.10252246\n",
            "This is the iter 5608, the d1 loss is: 2845.3516, the d2 loss is: -3132.2734, the g loss is: 3241.0703, the ae loss is: 0.00895091, the jacobian loss is:0.24518064\n",
            "This is the iter 5609, the d1 loss is: 2881.8203, the d2 loss is: -3199.8047, the g loss is: 3215.8281, the ae loss is: 0.008182387, the jacobian loss is:0.15469988\n",
            "This is the iter 5610, the d1 loss is: 2866.0156, the d2 loss is: -3219.9219, the g loss is: 3250.75, the ae loss is: 0.010711813, the jacobian loss is:0.10450268\n",
            "This is the iter 5611, the d1 loss is: 3161.2031, the d2 loss is: -3483.2578, the g loss is: 3462.1172, the ae loss is: 0.0050285324, the jacobian loss is:0.09875469\n",
            "This is the iter 5612, the d1 loss is: 3004.5312, the d2 loss is: -3354.2188, the g loss is: 3332.9922, the ae loss is: 0.0111020375, the jacobian loss is:0.13796726\n",
            "This is the iter 5613, the d1 loss is: 2913.711, the d2 loss is: -3256.0156, the g loss is: 3202.9297, the ae loss is: 0.007846855, the jacobian loss is:0.22373775\n",
            "This is the iter 5614, the d1 loss is: 2897.1953, the d2 loss is: -3260.2578, the g loss is: 3207.2344, the ae loss is: 0.011364813, the jacobian loss is:0.11610799\n",
            "This is the iter 5615, the d1 loss is: 2873.7188, the d2 loss is: -3221.4922, the g loss is: 3252.1406, the ae loss is: 0.005886948, the jacobian loss is:0.11790507\n",
            "This is the iter 5616, the d1 loss is: 2782.3594, the d2 loss is: -3129.0, the g loss is: 3085.664, the ae loss is: 0.0065437322, the jacobian loss is:0.08876003\n",
            "This is the iter 5617, the d1 loss is: 2947.1484, the d2 loss is: -3278.6875, the g loss is: 3275.1406, the ae loss is: 0.0055215345, the jacobian loss is:0.10023537\n",
            "This is the iter 5618, the d1 loss is: 3041.5156, the d2 loss is: -3360.25, the g loss is: 3332.875, the ae loss is: 0.008197032, the jacobian loss is:0.08622436\n",
            "This is the iter 5619, the d1 loss is: 2841.8594, the d2 loss is: -3139.9766, the g loss is: 3140.625, the ae loss is: 0.0056475396, the jacobian loss is:0.08761582\n",
            "This is the iter 5620, the d1 loss is: 2950.1719, the d2 loss is: -3296.211, the g loss is: 3307.0312, the ae loss is: 0.0077883424, the jacobian loss is:0.10024254\n",
            "This is the iter 5621, the d1 loss is: 2656.8438, the d2 loss is: -2941.7734, the g loss is: 3049.2266, the ae loss is: 0.00833877, the jacobian loss is:0.11272151\n",
            "This is the iter 5622, the d1 loss is: 2959.3047, the d2 loss is: -3304.1797, the g loss is: 3290.5938, the ae loss is: 0.011093625, the jacobian loss is:0.129896\n",
            "This is the iter 5623, the d1 loss is: 2966.6562, the d2 loss is: -3291.9531, the g loss is: 3205.9922, the ae loss is: 0.0055754, the jacobian loss is:0.09896172\n",
            "This is the iter 5624, the d1 loss is: 2934.1094, the d2 loss is: -3254.7422, the g loss is: 3186.711, the ae loss is: 0.006908108, the jacobian loss is:0.12476246\n",
            "This is the iter 5625, the d1 loss is: 2898.2812, the d2 loss is: -3215.1328, the g loss is: 3227.6953, the ae loss is: 0.0067488006, the jacobian loss is:0.12047632\n",
            "This is the iter 5626, the d1 loss is: 2946.3594, the d2 loss is: -3273.9453, the g loss is: 3261.3281, the ae loss is: 0.00777622, the jacobian loss is:0.4663191\n",
            "This is the iter 5627, the d1 loss is: 2917.6016, the d2 loss is: -3264.8047, the g loss is: 3247.1719, the ae loss is: 0.0076200943, the jacobian loss is:0.12100023\n",
            "This is the iter 5628, the d1 loss is: 2988.625, the d2 loss is: -3312.9688, the g loss is: 3323.164, the ae loss is: 0.006822276, the jacobian loss is:0.110293865\n",
            "This is the iter 5629, the d1 loss is: 2877.1094, the d2 loss is: -3204.8906, the g loss is: 3293.125, the ae loss is: 0.008438508, the jacobian loss is:0.10364377\n",
            "This is the iter 5630, the d1 loss is: 2907.8906, the d2 loss is: -3248.6328, the g loss is: 3300.1953, the ae loss is: 0.0046995087, the jacobian loss is:0.14075197\n",
            "This is the iter 5631, the d1 loss is: 2919.6484, the d2 loss is: -3250.8281, the g loss is: 3246.0938, the ae loss is: 0.003852787, the jacobian loss is:0.10816635\n",
            "This is the iter 5632, the d1 loss is: 2997.2812, the d2 loss is: -3315.75, the g loss is: 3279.4219, the ae loss is: 0.008563997, the jacobian loss is:0.10455403\n",
            "This is the iter 5633, the d1 loss is: 3041.7031, the d2 loss is: -3397.3906, the g loss is: 3403.4844, the ae loss is: 0.0063211424, the jacobian loss is:0.1456745\n",
            "This is the iter 5634, the d1 loss is: 2904.1328, the d2 loss is: -3271.5, the g loss is: 3263.6406, the ae loss is: 0.008841966, the jacobian loss is:0.093994595\n",
            "This is the iter 5635, the d1 loss is: 2943.9688, the d2 loss is: -3249.3828, the g loss is: 3272.7734, the ae loss is: 0.006706473, the jacobian loss is:0.09523103\n",
            "This is the iter 5636, the d1 loss is: 2981.8281, the d2 loss is: -3286.7266, the g loss is: 3247.6953, the ae loss is: 0.009182721, the jacobian loss is:0.081548624\n",
            "This is the iter 5637, the d1 loss is: 2932.664, the d2 loss is: -3244.0703, the g loss is: 3214.4297, the ae loss is: 0.0061466396, the jacobian loss is:0.108322345\n",
            "This is the iter 5638, the d1 loss is: 2846.0312, the d2 loss is: -3167.5156, the g loss is: 3248.8594, the ae loss is: 0.006348571, the jacobian loss is:0.109524146\n",
            "This is the iter 5639, the d1 loss is: 2750.1406, the d2 loss is: -3040.9375, the g loss is: 3148.7734, the ae loss is: 0.0052689533, the jacobian loss is:0.14560857\n",
            "This is the iter 5640, the d1 loss is: 2841.4766, the d2 loss is: -3201.4297, the g loss is: 3186.8047, the ae loss is: 0.0070600556, the jacobian loss is:0.068568066\n",
            "This is the iter 5641, the d1 loss is: 2813.4219, the d2 loss is: -3134.7266, the g loss is: 3122.2266, the ae loss is: 0.0067012496, the jacobian loss is:0.1266781\n",
            "This is the iter 5642, the d1 loss is: 2916.1953, the d2 loss is: -3243.2422, the g loss is: 3214.836, the ae loss is: 0.009957822, the jacobian loss is:0.09949882\n",
            "This is the iter 5643, the d1 loss is: 2919.8203, the d2 loss is: -3248.0938, the g loss is: 3258.7578, the ae loss is: 0.0074193086, the jacobian loss is:0.16925873\n",
            "This is the iter 5644, the d1 loss is: 2803.1016, the d2 loss is: -3160.25, the g loss is: 3187.1172, the ae loss is: 0.00720653, the jacobian loss is:0.11087377\n",
            "This is the iter 5645, the d1 loss is: 2863.586, the d2 loss is: -3193.3828, the g loss is: 3237.8906, the ae loss is: 0.01088536, the jacobian loss is:0.13917908\n",
            "This is the iter 5646, the d1 loss is: 2874.625, the d2 loss is: -3188.9688, the g loss is: 3223.4844, the ae loss is: 0.008657436, the jacobian loss is:0.13763097\n",
            "This is the iter 5647, the d1 loss is: 2915.3438, the d2 loss is: -3228.6406, the g loss is: 3243.0078, the ae loss is: 0.006151491, the jacobian loss is:0.12608156\n",
            "This is the iter 5648, the d1 loss is: 2920.4766, the d2 loss is: -3266.2422, the g loss is: 3300.289, the ae loss is: 0.00823075, the jacobian loss is:0.102260455\n",
            "This is the iter 5649, the d1 loss is: 2839.2734, the d2 loss is: -3202.9062, the g loss is: 3204.3516, the ae loss is: 0.005978244, the jacobian loss is:0.1104385\n",
            "This is the iter 5650, the d1 loss is: 2824.2969, the d2 loss is: -3180.3594, the g loss is: 3168.0469, the ae loss is: 0.007632884, the jacobian loss is:0.093387336\n",
            "This is the iter 5651, the d1 loss is: 2847.414, the d2 loss is: -3124.3672, the g loss is: 3036.0781, the ae loss is: 0.0071080164, the jacobian loss is:0.12180655\n",
            "This is the iter 5652, the d1 loss is: 2931.875, the d2 loss is: -3253.039, the g loss is: 3215.3516, the ae loss is: 0.0071121426, the jacobian loss is:0.1143086\n",
            "This is the iter 5653, the d1 loss is: 2856.7188, the d2 loss is: -3198.086, the g loss is: 3177.3125, the ae loss is: 0.0073853657, the jacobian loss is:0.11372718\n",
            "This is the iter 5654, the d1 loss is: 2766.1094, the d2 loss is: -3087.3438, the g loss is: 3093.2344, the ae loss is: 0.006962301, the jacobian loss is:0.12562214\n",
            "This is the iter 5655, the d1 loss is: 2894.3984, the d2 loss is: -3215.8906, the g loss is: 3213.3906, the ae loss is: 0.006847392, the jacobian loss is:0.09848706\n",
            "This is the iter 5656, the d1 loss is: 2871.7422, the d2 loss is: -3204.3438, the g loss is: 3232.1484, the ae loss is: 0.00887452, the jacobian loss is:0.2475799\n",
            "This is the iter 5657, the d1 loss is: 2885.3125, the d2 loss is: -3216.4844, the g loss is: 3187.75, the ae loss is: 0.008828316, the jacobian loss is:0.14188048\n",
            "This is the iter 5658, the d1 loss is: 2889.9688, the d2 loss is: -3199.4062, the g loss is: 3164.8203, the ae loss is: 0.008632162, the jacobian loss is:0.23837788\n",
            "This is the iter 5659, the d1 loss is: 2814.875, the d2 loss is: -3186.3984, the g loss is: 3136.1875, the ae loss is: 0.01263531, the jacobian loss is:0.15880185\n",
            "This is the iter 5660, the d1 loss is: 2796.2188, the d2 loss is: -3127.0938, the g loss is: 3168.0469, the ae loss is: 0.010015177, the jacobian loss is:0.26964557\n",
            "This is the iter 5661, the d1 loss is: 2844.8906, the d2 loss is: -3128.7422, the g loss is: 3222.4922, the ae loss is: 0.007358305, the jacobian loss is:0.28829092\n",
            "This is the iter 5662, the d1 loss is: 2890.0, the d2 loss is: -3264.0703, the g loss is: 3268.7969, the ae loss is: 0.0078116627, the jacobian loss is:0.124274366\n",
            "This is the iter 5663, the d1 loss is: 2858.0156, the d2 loss is: -3210.5234, the g loss is: 3175.9297, the ae loss is: 0.006759297, the jacobian loss is:0.14905658\n",
            "This is the iter 5664, the d1 loss is: 2908.2344, the d2 loss is: -3238.5547, the g loss is: 3200.2188, the ae loss is: 0.0082529895, the jacobian loss is:0.12233597\n",
            "This is the iter 5665, the d1 loss is: 2915.8281, the d2 loss is: -3203.0469, the g loss is: 3231.8281, the ae loss is: 0.0049091512, the jacobian loss is:0.10969173\n",
            "This is the iter 5666, the d1 loss is: 2840.7734, the d2 loss is: -3199.039, the g loss is: 3132.836, the ae loss is: 0.009869742, the jacobian loss is:0.13209787\n",
            "This is the iter 5667, the d1 loss is: 2783.75, the d2 loss is: -3131.7578, the g loss is: 3114.0938, the ae loss is: 0.0077615846, the jacobian loss is:0.10131088\n",
            "This is the iter 5668, the d1 loss is: 2833.3906, the d2 loss is: -3148.6328, the g loss is: 3116.3281, the ae loss is: 0.008103138, the jacobian loss is:0.13616388\n",
            "This is the iter 5669, the d1 loss is: 2760.7578, the d2 loss is: -3118.4219, the g loss is: 3078.4219, the ae loss is: 0.0056085773, the jacobian loss is:0.12028783\n",
            "This is the iter 5670, the d1 loss is: 3009.1484, the d2 loss is: -3338.4844, the g loss is: 3294.3203, the ae loss is: 0.008968506, the jacobian loss is:0.30368957\n",
            "This is the iter 5671, the d1 loss is: 2891.961, the d2 loss is: -3210.586, the g loss is: 3208.0938, the ae loss is: 0.008570831, the jacobian loss is:0.10741953\n",
            "This is the iter 5672, the d1 loss is: 2860.3281, the d2 loss is: -3208.586, the g loss is: 3254.5, the ae loss is: 0.0101143485, the jacobian loss is:0.11336242\n",
            "This is the iter 5673, the d1 loss is: 2923.5156, the d2 loss is: -3218.2344, the g loss is: 3203.6328, the ae loss is: 0.0078447, the jacobian loss is:0.11770128\n",
            "This is the iter 5674, the d1 loss is: 2749.625, the d2 loss is: -3086.6406, the g loss is: 3165.2422, the ae loss is: 0.0059762364, the jacobian loss is:0.14231507\n",
            "This is the iter 5675, the d1 loss is: 2624.461, the d2 loss is: -2949.8594, the g loss is: 2928.8672, the ae loss is: 0.0084344465, the jacobian loss is:0.13510677\n",
            "This is the iter 5676, the d1 loss is: 3071.8281, the d2 loss is: -3383.5781, the g loss is: 3349.6719, the ae loss is: 0.00802275, the jacobian loss is:0.10292253\n",
            "This is the iter 5677, the d1 loss is: 2827.1797, the d2 loss is: -3153.5703, the g loss is: 3129.5938, the ae loss is: 0.007927276, the jacobian loss is:0.12500948\n",
            "This is the iter 5678, the d1 loss is: 2967.375, the d2 loss is: -3260.9531, the g loss is: 3208.875, the ae loss is: 0.008434273, the jacobian loss is:0.12295039\n",
            "This is the iter 5679, the d1 loss is: 2859.3281, the d2 loss is: -3203.2422, the g loss is: 3162.039, the ae loss is: 0.0076126363, the jacobian loss is:0.12595826\n",
            "This is the iter 5680, the d1 loss is: 2953.1953, the d2 loss is: -3267.1406, the g loss is: 3285.2344, the ae loss is: 0.008191089, the jacobian loss is:0.14237893\n",
            "This is the iter 5681, the d1 loss is: 2739.0, the d2 loss is: -3092.0, the g loss is: 3097.0938, the ae loss is: 0.006339861, the jacobian loss is:0.106947586\n",
            "This is the iter 5682, the d1 loss is: 2864.9062, the d2 loss is: -3171.9453, the g loss is: 3213.8594, the ae loss is: 0.0082955025, the jacobian loss is:0.099313036\n",
            "This is the iter 5683, the d1 loss is: 2821.3984, the d2 loss is: -3127.0078, the g loss is: 3099.0234, the ae loss is: 0.0074360077, the jacobian loss is:0.14028756\n",
            "This is the iter 5684, the d1 loss is: 2869.086, the d2 loss is: -3212.2344, the g loss is: 3248.789, the ae loss is: 0.008058811, the jacobian loss is:0.115165465\n",
            "This is the iter 5685, the d1 loss is: 2991.9766, the d2 loss is: -3295.5703, the g loss is: 3262.9688, the ae loss is: 0.0053072465, the jacobian loss is:0.5974256\n",
            "This is the iter 5686, the d1 loss is: 2904.2188, the d2 loss is: -3214.4219, the g loss is: 3243.8438, the ae loss is: 0.007302892, the jacobian loss is:0.19278236\n",
            "This is the iter 5687, the d1 loss is: 3082.539, the d2 loss is: -3375.914, the g loss is: 3397.3438, the ae loss is: 0.0077703353, the jacobian loss is:0.116134815\n",
            "This is the iter 5688, the d1 loss is: 2990.2656, the d2 loss is: -3307.3438, the g loss is: 3285.9844, the ae loss is: 0.005719603, the jacobian loss is:0.12750043\n",
            "This is the iter 5689, the d1 loss is: 2870.0625, the d2 loss is: -3199.2969, the g loss is: 3153.0312, the ae loss is: 0.005698166, the jacobian loss is:0.3388474\n",
            "This is the iter 5690, the d1 loss is: 2924.2656, the d2 loss is: -3257.3047, the g loss is: 3256.5234, the ae loss is: 0.008383485, the jacobian loss is:0.13423282\n",
            "This is the iter 5691, the d1 loss is: 3177.6406, the d2 loss is: -3544.5156, the g loss is: 3566.0156, the ae loss is: 0.005217551, the jacobian loss is:0.09499448\n",
            "This is the iter 5692, the d1 loss is: 2975.625, the d2 loss is: -3307.75, the g loss is: 3341.25, the ae loss is: 0.0059725977, the jacobian loss is:0.12280884\n",
            "This is the iter 5693, the d1 loss is: 2814.7656, the d2 loss is: -3155.3594, the g loss is: 3196.7656, the ae loss is: 0.0074079204, the jacobian loss is:0.13812692\n",
            "This is the iter 5694, the d1 loss is: 2829.6875, the d2 loss is: -3180.1094, the g loss is: 3118.8281, the ae loss is: 0.006921036, the jacobian loss is:0.19295518\n",
            "This is the iter 5695, the d1 loss is: 2752.9219, the d2 loss is: -3069.039, the g loss is: 3072.5469, the ae loss is: 0.009985555, the jacobian loss is:0.32562172\n",
            "This is the iter 5696, the d1 loss is: 2857.3125, the d2 loss is: -3175.625, the g loss is: 3168.2188, the ae loss is: 0.007254258, the jacobian loss is:0.23036972\n",
            "This is the iter 5697, the d1 loss is: 2972.2344, the d2 loss is: -3322.7422, the g loss is: 3323.375, the ae loss is: 0.006257859, the jacobian loss is:0.26402736\n",
            "This is the iter 5698, the d1 loss is: 2892.0781, the d2 loss is: -3191.875, the g loss is: 3173.3828, the ae loss is: 0.007219307, the jacobian loss is:0.12697762\n",
            "This is the iter 5699, the d1 loss is: 2985.7656, the d2 loss is: -3322.4453, the g loss is: 3297.4062, the ae loss is: 0.006095753, the jacobian loss is:0.107943974\n",
            "This is the iter 5700, the d1 loss is: 2823.3594, the d2 loss is: -3151.586, the g loss is: 3135.0938, the ae loss is: 0.0047368906, the jacobian loss is:0.094950356\n",
            "0.2303709\n",
            "0.8609606\n",
            "This is the iter 5701, the d1 loss is: 2806.6016, the d2 loss is: -3134.8203, the g loss is: 3127.5938, the ae loss is: 0.009556053, the jacobian loss is:0.3021488\n",
            "This is the iter 5702, the d1 loss is: 2930.3672, the d2 loss is: -3254.789, the g loss is: 3259.5, the ae loss is: 0.010456448, the jacobian loss is:0.10899638\n",
            "This is the iter 5703, the d1 loss is: 2896.0, the d2 loss is: -3223.9062, the g loss is: 3194.0156, the ae loss is: 0.0054297354, the jacobian loss is:0.14397822\n",
            "This is the iter 5704, the d1 loss is: 2922.25, the d2 loss is: -3222.1172, the g loss is: 3239.5469, the ae loss is: 0.007948352, the jacobian loss is:0.1157112\n",
            "This is the iter 5705, the d1 loss is: 2966.9219, the d2 loss is: -3287.1484, the g loss is: 3307.0156, the ae loss is: 0.007457175, the jacobian loss is:0.11272663\n",
            "This is the iter 5706, the d1 loss is: 2979.164, the d2 loss is: -3320.8281, the g loss is: 3305.0625, the ae loss is: 0.007501416, the jacobian loss is:0.344905\n",
            "This is the iter 5707, the d1 loss is: 2928.2344, the d2 loss is: -3259.3125, the g loss is: 3281.9453, the ae loss is: 0.008080498, the jacobian loss is:0.27316186\n",
            "This is the iter 5708, the d1 loss is: 2967.086, the d2 loss is: -3256.9531, the g loss is: 3275.5156, the ae loss is: 0.010691113, the jacobian loss is:0.12087844\n",
            "This is the iter 5709, the d1 loss is: 2959.25, the d2 loss is: -3264.5078, the g loss is: 3193.2266, the ae loss is: 0.008014283, the jacobian loss is:0.12892716\n",
            "This is the iter 5710, the d1 loss is: 2993.125, the d2 loss is: -3279.125, the g loss is: 3284.3125, the ae loss is: 0.007699324, the jacobian loss is:0.14829528\n",
            "This is the iter 5711, the d1 loss is: 2910.4219, the d2 loss is: -3238.3203, the g loss is: 3250.8125, the ae loss is: 0.007140142, the jacobian loss is:0.16009769\n",
            "This is the iter 5712, the d1 loss is: 2990.3984, the d2 loss is: -3315.4844, the g loss is: 3362.2734, the ae loss is: 0.006416574, the jacobian loss is:0.25319928\n",
            "This is the iter 5713, the d1 loss is: 2821.3672, the d2 loss is: -3153.8906, the g loss is: 3162.25, the ae loss is: 0.0073141116, the jacobian loss is:0.120968424\n",
            "This is the iter 5714, the d1 loss is: 2903.6953, the d2 loss is: -3267.7578, the g loss is: 3218.9375, the ae loss is: 0.011331109, the jacobian loss is:0.48044312\n",
            "This is the iter 5715, the d1 loss is: 2683.2188, the d2 loss is: -2994.8438, the g loss is: 3020.2344, the ae loss is: 0.0064261984, the jacobian loss is:0.118416764\n",
            "This is the iter 5716, the d1 loss is: 2878.7266, the d2 loss is: -3147.625, the g loss is: 3205.9375, the ae loss is: 0.009754184, the jacobian loss is:0.38214394\n",
            "This is the iter 5717, the d1 loss is: 2955.5156, the d2 loss is: -3274.5469, the g loss is: 3284.125, the ae loss is: 0.006858631, the jacobian loss is:0.110689424\n",
            "This is the iter 5718, the d1 loss is: 2974.461, the d2 loss is: -3271.6562, the g loss is: 3307.3906, the ae loss is: 0.007665961, the jacobian loss is:0.2746952\n",
            "This is the iter 5719, the d1 loss is: 2870.6875, the d2 loss is: -3145.8984, the g loss is: 3235.9297, the ae loss is: 0.0075828806, the jacobian loss is:0.12791069\n",
            "This is the iter 5720, the d1 loss is: 2695.1719, the d2 loss is: -3021.7188, the g loss is: 2975.9062, the ae loss is: 0.012320289, the jacobian loss is:0.105220236\n",
            "This is the iter 5721, the d1 loss is: 2632.164, the d2 loss is: -3007.6875, the g loss is: 2969.6094, the ae loss is: 0.007015951, the jacobian loss is:0.25623235\n",
            "This is the iter 5722, the d1 loss is: 2936.5781, the d2 loss is: -3235.0234, the g loss is: 3187.4922, the ae loss is: 0.008759212, the jacobian loss is:0.1493506\n",
            "This is the iter 5723, the d1 loss is: 2827.0625, the d2 loss is: -3147.4766, the g loss is: 3156.2656, the ae loss is: 0.0074931514, the jacobian loss is:0.11864879\n",
            "This is the iter 5724, the d1 loss is: 3034.0312, the d2 loss is: -3357.1875, the g loss is: 3388.5312, the ae loss is: 0.0059001963, the jacobian loss is:0.09545777\n",
            "This is the iter 5725, the d1 loss is: 2879.9297, the d2 loss is: -3201.3906, the g loss is: 3267.9766, the ae loss is: 0.007239137, the jacobian loss is:0.12178728\n",
            "This is the iter 5726, the d1 loss is: 2946.289, the d2 loss is: -3235.1484, the g loss is: 3267.5469, the ae loss is: 0.009527714, the jacobian loss is:0.28117624\n",
            "This is the iter 5727, the d1 loss is: 2956.0938, the d2 loss is: -3278.2422, the g loss is: 3251.8672, the ae loss is: 0.0074054515, the jacobian loss is:0.19349822\n",
            "This is the iter 5728, the d1 loss is: 2861.0703, the d2 loss is: -3167.0938, the g loss is: 3179.9219, the ae loss is: 0.007799859, the jacobian loss is:0.11848471\n",
            "This is the iter 5729, the d1 loss is: 2785.1406, the d2 loss is: -3120.1094, the g loss is: 3109.5781, the ae loss is: 0.00823235, the jacobian loss is:0.13285536\n",
            "This is the iter 5730, the d1 loss is: 3216.3984, the d2 loss is: -3503.5938, the g loss is: 3477.164, the ae loss is: 0.009433292, the jacobian loss is:0.42434344\n",
            "This is the iter 5731, the d1 loss is: 2956.1484, the d2 loss is: -3291.1094, the g loss is: 3267.664, the ae loss is: 0.008128239, the jacobian loss is:0.10402787\n",
            "This is the iter 5732, the d1 loss is: 2940.6719, the d2 loss is: -3250.8125, the g loss is: 3180.8828, the ae loss is: 0.008653293, the jacobian loss is:0.28713548\n",
            "This is the iter 5733, the d1 loss is: 2932.125, the d2 loss is: -3299.7031, the g loss is: 3273.7656, the ae loss is: 0.007788065, the jacobian loss is:0.15151861\n",
            "This is the iter 5734, the d1 loss is: 2898.8672, the d2 loss is: -3209.2422, the g loss is: 3274.0078, the ae loss is: 0.010575381, the jacobian loss is:0.22670823\n",
            "This is the iter 5735, the d1 loss is: 2909.8125, the d2 loss is: -3218.8672, the g loss is: 3295.5312, the ae loss is: 0.0055548977, the jacobian loss is:0.093794696\n",
            "This is the iter 5736, the d1 loss is: 2910.7812, the d2 loss is: -3227.2812, the g loss is: 3208.3516, the ae loss is: 0.009559655, the jacobian loss is:0.22621581\n",
            "This is the iter 5737, the d1 loss is: 2959.3906, the d2 loss is: -3262.6172, the g loss is: 3227.1797, the ae loss is: 0.0055323606, the jacobian loss is:0.13522105\n",
            "This is the iter 5738, the d1 loss is: 2942.961, the d2 loss is: -3224.3281, the g loss is: 3222.4688, the ae loss is: 0.009351315, the jacobian loss is:0.2889887\n",
            "This is the iter 5739, the d1 loss is: 2903.375, the d2 loss is: -3210.4531, the g loss is: 3209.836, the ae loss is: 0.007476745, the jacobian loss is:0.1296042\n",
            "This is the iter 5740, the d1 loss is: 2958.9922, the d2 loss is: -3225.4453, the g loss is: 3238.8438, the ae loss is: 0.011738205, the jacobian loss is:0.2933896\n",
            "This is the iter 5741, the d1 loss is: 2974.9766, the d2 loss is: -3243.2656, the g loss is: 3270.2578, the ae loss is: 0.0077438937, the jacobian loss is:0.12532714\n",
            "This is the iter 5742, the d1 loss is: 2936.4453, the d2 loss is: -3261.5078, the g loss is: 3244.836, the ae loss is: 0.009713191, the jacobian loss is:0.23557514\n",
            "This is the iter 5743, the d1 loss is: 2883.875, the d2 loss is: -3204.7656, the g loss is: 3199.2734, the ae loss is: 0.009246477, the jacobian loss is:0.24224132\n",
            "This is the iter 5744, the d1 loss is: 2912.0703, the d2 loss is: -3196.7422, the g loss is: 3251.25, the ae loss is: 0.009867985, the jacobian loss is:0.19800915\n",
            "This is the iter 5745, the d1 loss is: 3135.6172, the d2 loss is: -3462.3828, the g loss is: 3460.7344, the ae loss is: 0.0053454167, the jacobian loss is:0.12647574\n",
            "This is the iter 5746, the d1 loss is: 2715.039, the d2 loss is: -3047.3672, the g loss is: 3114.5469, the ae loss is: 0.008556422, the jacobian loss is:0.21360183\n",
            "This is the iter 5747, the d1 loss is: 2811.3516, the d2 loss is: -3163.4531, the g loss is: 3130.7422, the ae loss is: 0.005778916, the jacobian loss is:0.13067509\n",
            "This is the iter 5748, the d1 loss is: 2952.4375, the d2 loss is: -3248.2188, the g loss is: 3271.4766, the ae loss is: 0.00765447, the jacobian loss is:0.20410687\n",
            "This is the iter 5749, the d1 loss is: 2871.6406, the d2 loss is: -3172.5703, the g loss is: 3194.1406, the ae loss is: 0.0058425316, the jacobian loss is:0.23764788\n",
            "This is the iter 5750, the d1 loss is: 2895.8125, the d2 loss is: -3200.75, the g loss is: 3131.8438, the ae loss is: 0.009919279, the jacobian loss is:0.5235997\n",
            "This is the iter 5751, the d1 loss is: 2904.664, the d2 loss is: -3228.2188, the g loss is: 3238.1328, the ae loss is: 0.009957673, the jacobian loss is:0.3718017\n",
            "This is the iter 5752, the d1 loss is: 3009.6484, the d2 loss is: -3267.0703, the g loss is: 3214.8906, the ae loss is: 0.0073129255, the jacobian loss is:0.30677608\n",
            "This is the iter 5753, the d1 loss is: 2908.7344, the d2 loss is: -3189.414, the g loss is: 3238.7969, the ae loss is: 0.007069956, the jacobian loss is:0.15455969\n",
            "This is the iter 5754, the d1 loss is: 2955.1797, the d2 loss is: -3230.9688, the g loss is: 3231.3516, the ae loss is: 0.009518129, the jacobian loss is:0.39660084\n",
            "This is the iter 5755, the d1 loss is: 2863.5156, the d2 loss is: -3185.664, the g loss is: 3171.3594, the ae loss is: 0.008507658, the jacobian loss is:0.31376234\n",
            "This is the iter 5756, the d1 loss is: 2898.4531, the d2 loss is: -3191.164, the g loss is: 3154.5703, the ae loss is: 0.009429078, the jacobian loss is:0.39805144\n",
            "This is the iter 5757, the d1 loss is: 2860.664, the d2 loss is: -3102.8125, the g loss is: 3191.9062, the ae loss is: 0.0067579, the jacobian loss is:0.3569138\n",
            "This is the iter 5758, the d1 loss is: 2985.0938, the d2 loss is: -3256.2344, the g loss is: 3251.2734, the ae loss is: 0.010556795, the jacobian loss is:0.4768998\n",
            "This is the iter 5759, the d1 loss is: 2894.2266, the d2 loss is: -3164.836, the g loss is: 3108.5625, the ae loss is: 0.008622599, the jacobian loss is:0.2350528\n",
            "This is the iter 5760, the d1 loss is: 2935.3125, the d2 loss is: -3246.5703, the g loss is: 3199.836, the ae loss is: 0.012098509, the jacobian loss is:0.49383116\n",
            "This is the iter 5761, the d1 loss is: 2850.1094, the d2 loss is: -3150.1719, the g loss is: 3195.1094, the ae loss is: 0.0074349614, the jacobian loss is:0.33346057\n",
            "This is the iter 5762, the d1 loss is: 2910.3672, the d2 loss is: -3186.8906, the g loss is: 3161.7266, the ae loss is: 0.009244927, the jacobian loss is:0.3283571\n",
            "This is the iter 5763, the d1 loss is: 2982.2422, the d2 loss is: -3247.7578, the g loss is: 3254.7031, the ae loss is: 0.009985007, the jacobian loss is:0.5462869\n",
            "This is the iter 5764, the d1 loss is: 2840.7969, the d2 loss is: -3115.4922, the g loss is: 3085.2422, the ae loss is: 0.010145652, the jacobian loss is:0.44605497\n",
            "This is the iter 5765, the d1 loss is: 2930.8594, the d2 loss is: -3219.2969, the g loss is: 3160.8281, the ae loss is: 0.0068913996, the jacobian loss is:0.51600087\n",
            "This is the iter 5766, the d1 loss is: 2954.8125, the d2 loss is: -3263.3906, the g loss is: 3215.2969, the ae loss is: 0.012805197, the jacobian loss is:0.36613572\n",
            "This is the iter 5767, the d1 loss is: 2897.8438, the d2 loss is: -3206.5078, the g loss is: 3265.4531, the ae loss is: 0.0080509, the jacobian loss is:0.41150048\n",
            "This is the iter 5768, the d1 loss is: 2847.5469, the d2 loss is: -3193.0547, the g loss is: 3184.0625, the ae loss is: 0.010549037, the jacobian loss is:0.28253874\n",
            "This is the iter 5769, the d1 loss is: 2929.6562, the d2 loss is: -3204.9766, the g loss is: 3197.375, the ae loss is: 0.008023787, the jacobian loss is:0.3617376\n",
            "This is the iter 5770, the d1 loss is: 2834.9688, the d2 loss is: -3156.5938, the g loss is: 3197.0703, the ae loss is: 0.009587212, the jacobian loss is:0.40787837\n",
            "This is the iter 5771, the d1 loss is: 2946.4453, the d2 loss is: -3201.836, the g loss is: 3213.3125, the ae loss is: 0.0053322744, the jacobian loss is:0.20191409\n",
            "This is the iter 5772, the d1 loss is: 2920.7656, the d2 loss is: -3184.1875, the g loss is: 3215.5, the ae loss is: 0.01328253, the jacobian loss is:0.20313077\n",
            "This is the iter 5773, the d1 loss is: 2926.9844, the d2 loss is: -3237.039, the g loss is: 3186.086, the ae loss is: 0.010377189, the jacobian loss is:0.5505367\n",
            "This is the iter 5774, the d1 loss is: 2799.0234, the d2 loss is: -3062.836, the g loss is: 3089.9922, the ae loss is: 0.009752077, the jacobian loss is:0.25401232\n",
            "This is the iter 5775, the d1 loss is: 2916.5703, the d2 loss is: -3205.2812, the g loss is: 3173.3906, the ae loss is: 0.0070763594, the jacobian loss is:0.19215523\n",
            "This is the iter 5776, the d1 loss is: 2764.836, the d2 loss is: -3035.086, the g loss is: 3066.7266, the ae loss is: 0.013091441, the jacobian loss is:0.39566848\n",
            "This is the iter 5777, the d1 loss is: 3097.4766, the d2 loss is: -3410.4375, the g loss is: 3417.1719, the ae loss is: 0.008005876, the jacobian loss is:0.22472931\n",
            "This is the iter 5778, the d1 loss is: 3259.0156, the d2 loss is: -3525.3672, the g loss is: 3576.3281, the ae loss is: 0.005463479, the jacobian loss is:0.3095062\n",
            "This is the iter 5779, the d1 loss is: 2804.2734, the d2 loss is: -3105.6172, the g loss is: 3103.789, the ae loss is: 0.006369793, the jacobian loss is:0.26125336\n",
            "This is the iter 5780, the d1 loss is: 2926.6328, the d2 loss is: -3249.3516, the g loss is: 3203.0469, the ae loss is: 0.008766707, the jacobian loss is:0.28227937\n",
            "This is the iter 5781, the d1 loss is: 2933.5469, the d2 loss is: -3250.625, the g loss is: 3235.6562, the ae loss is: 0.005500894, the jacobian loss is:0.26068145\n",
            "This is the iter 5782, the d1 loss is: 2943.6094, the d2 loss is: -3213.7266, the g loss is: 3241.9531, the ae loss is: 0.010411232, the jacobian loss is:0.40018767\n",
            "This is the iter 5783, the d1 loss is: 2917.4219, the d2 loss is: -3216.7656, the g loss is: 3231.6172, the ae loss is: 0.010761027, the jacobian loss is:0.44133192\n",
            "This is the iter 5784, the d1 loss is: 3003.1406, the d2 loss is: -3316.3828, the g loss is: 3321.2188, the ae loss is: 0.0072528585, the jacobian loss is:0.24967976\n",
            "This is the iter 5785, the d1 loss is: 2872.7812, the d2 loss is: -3201.1875, the g loss is: 3226.7812, the ae loss is: 0.0050800703, the jacobian loss is:0.29424778\n",
            "This is the iter 5786, the d1 loss is: 2919.7031, the d2 loss is: -3199.961, the g loss is: 3197.2812, the ae loss is: 0.0071187774, the jacobian loss is:0.2674739\n",
            "This is the iter 5787, the d1 loss is: 2936.6172, the d2 loss is: -3230.8906, the g loss is: 3162.3516, the ae loss is: 0.009308937, the jacobian loss is:0.34263146\n",
            "This is the iter 5788, the d1 loss is: 3009.0234, the d2 loss is: -3296.125, the g loss is: 3321.5781, the ae loss is: 0.0127321035, the jacobian loss is:0.31588945\n",
            "This is the iter 5789, the d1 loss is: 2931.1172, the d2 loss is: -3229.3281, the g loss is: 3234.4531, the ae loss is: 0.005040548, the jacobian loss is:0.3130528\n",
            "This is the iter 5790, the d1 loss is: 2803.8906, the d2 loss is: -3102.1484, the g loss is: 3107.1484, the ae loss is: 0.0083505465, the jacobian loss is:0.2895698\n",
            "This is the iter 5791, the d1 loss is: 2852.1797, the d2 loss is: -3165.9688, the g loss is: 3184.3594, the ae loss is: 0.008614798, the jacobian loss is:0.2831082\n",
            "This is the iter 5792, the d1 loss is: 2855.25, the d2 loss is: -3187.2344, the g loss is: 3223.3438, the ae loss is: 0.008229853, the jacobian loss is:0.24762748\n",
            "This is the iter 5793, the d1 loss is: 2989.7188, the d2 loss is: -3272.2266, the g loss is: 3252.125, the ae loss is: 0.0051625283, the jacobian loss is:0.26628816\n",
            "This is the iter 5794, the d1 loss is: 2900.0469, the d2 loss is: -3214.4688, the g loss is: 3176.7734, the ae loss is: 0.008060606, the jacobian loss is:0.33627558\n",
            "This is the iter 5795, the d1 loss is: 2924.9062, the d2 loss is: -3213.6875, the g loss is: 3147.4844, the ae loss is: 0.0098580485, the jacobian loss is:0.32713324\n",
            "This is the iter 5796, the d1 loss is: 2863.1328, the d2 loss is: -3139.414, the g loss is: 3140.6094, the ae loss is: 0.0073525785, the jacobian loss is:0.24582233\n",
            "This is the iter 5797, the d1 loss is: 2912.8281, the d2 loss is: -3174.5, the g loss is: 3147.9062, the ae loss is: 0.006546488, the jacobian loss is:0.34507397\n",
            "This is the iter 5798, the d1 loss is: 2949.6328, the d2 loss is: -3245.086, the g loss is: 3118.9844, the ae loss is: 0.0151009215, the jacobian loss is:0.30589753\n",
            "This is the iter 5799, the d1 loss is: 2752.461, the d2 loss is: -3040.5234, the g loss is: 3044.0547, the ae loss is: 0.0067547252, the jacobian loss is:0.2326235\n",
            "This is the iter 5800, the d1 loss is: 2866.9062, the d2 loss is: -3150.6406, the g loss is: 3142.1719, the ae loss is: 0.0070286086, the jacobian loss is:0.25790524\n",
            "0.2659593\n",
            "1.0231603\n",
            "This is the iter 5801, the d1 loss is: 2748.211, the d2 loss is: -3050.7422, the g loss is: 3028.6172, the ae loss is: 0.007954496, the jacobian loss is:0.28025886\n",
            "This is the iter 5802, the d1 loss is: 2920.0312, the d2 loss is: -3222.5938, the g loss is: 3224.039, the ae loss is: 0.0072305426, the jacobian loss is:0.29033768\n",
            "This is the iter 5803, the d1 loss is: 2807.5078, the d2 loss is: -3096.2344, the g loss is: 3038.164, the ae loss is: 0.0065735145, the jacobian loss is:0.23457658\n",
            "This is the iter 5804, the d1 loss is: 2832.039, the d2 loss is: -3141.5625, the g loss is: 3129.3047, the ae loss is: 0.010235708, the jacobian loss is:0.20956308\n",
            "This is the iter 5805, the d1 loss is: 2862.7422, the d2 loss is: -3124.039, the g loss is: 3105.0469, the ae loss is: 0.006710369, the jacobian loss is:0.22994316\n",
            "This is the iter 5806, the d1 loss is: 2794.0, the d2 loss is: -3094.4844, the g loss is: 3133.8047, the ae loss is: 0.0067401077, the jacobian loss is:0.17544636\n",
            "This is the iter 5807, the d1 loss is: 2860.4453, the d2 loss is: -3167.6094, the g loss is: 3172.7969, the ae loss is: 0.006637088, the jacobian loss is:0.14294773\n",
            "This is the iter 5808, the d1 loss is: 2940.7812, the d2 loss is: -3227.1016, the g loss is: 3255.0469, the ae loss is: 0.014708517, the jacobian loss is:0.22925304\n",
            "This is the iter 5809, the d1 loss is: 2948.2578, the d2 loss is: -3232.2969, the g loss is: 3227.5703, the ae loss is: 0.006933994, the jacobian loss is:0.24394618\n",
            "This is the iter 5810, the d1 loss is: 2948.1562, the d2 loss is: -3211.914, the g loss is: 3230.0312, the ae loss is: 0.00861899, the jacobian loss is:0.23431513\n",
            "This is the iter 5811, the d1 loss is: 2954.0469, the d2 loss is: -3220.3984, the g loss is: 3265.1797, the ae loss is: 0.0085045155, the jacobian loss is:0.245058\n",
            "This is the iter 5812, the d1 loss is: 2949.6562, the d2 loss is: -3195.625, the g loss is: 3206.3281, the ae loss is: 0.009684753, the jacobian loss is:0.21784508\n",
            "This is the iter 5813, the d1 loss is: 2850.6562, the d2 loss is: -3133.5625, the g loss is: 3168.6484, the ae loss is: 0.0054641124, the jacobian loss is:0.2730415\n",
            "This is the iter 5814, the d1 loss is: 2724.2344, the d2 loss is: -3018.3047, the g loss is: 2983.3828, the ae loss is: 0.008974165, the jacobian loss is:0.21954423\n",
            "This is the iter 5815, the d1 loss is: 2821.5938, the d2 loss is: -3115.9688, the g loss is: 3122.6328, the ae loss is: 0.006687381, the jacobian loss is:0.27261606\n",
            "This is the iter 5816, the d1 loss is: 2817.0938, the d2 loss is: -3147.6406, the g loss is: 3174.6953, the ae loss is: 0.010003158, the jacobian loss is:0.18689783\n",
            "This is the iter 5817, the d1 loss is: 2849.8828, the d2 loss is: -3182.8438, the g loss is: 3136.7266, the ae loss is: 0.0072293803, the jacobian loss is:0.3056548\n",
            "This is the iter 5818, the d1 loss is: 2924.289, the d2 loss is: -3217.1875, the g loss is: 3204.4688, the ae loss is: 0.009430735, the jacobian loss is:0.18612954\n",
            "This is the iter 5819, the d1 loss is: 2843.4453, the d2 loss is: -3135.1406, the g loss is: 3109.1094, the ae loss is: 0.008636452, the jacobian loss is:0.22226512\n",
            "This is the iter 5820, the d1 loss is: 2844.2188, the d2 loss is: -3132.414, the g loss is: 3119.914, the ae loss is: 0.008304986, the jacobian loss is:0.2442228\n",
            "This is the iter 5821, the d1 loss is: 2864.6562, the d2 loss is: -3180.2188, the g loss is: 3136.3125, the ae loss is: 0.00556303, the jacobian loss is:0.22583558\n",
            "This is the iter 5822, the d1 loss is: 3066.1797, the d2 loss is: -3359.5312, the g loss is: 3404.1406, the ae loss is: 0.010520374, the jacobian loss is:0.27224168\n",
            "This is the iter 5823, the d1 loss is: 2878.8203, the d2 loss is: -3180.5938, the g loss is: 3203.8828, the ae loss is: 0.006709475, the jacobian loss is:0.187762\n",
            "This is the iter 5824, the d1 loss is: 2842.7422, the d2 loss is: -3139.7656, the g loss is: 3126.2031, the ae loss is: 0.008787793, the jacobian loss is:0.22492446\n",
            "This is the iter 5825, the d1 loss is: 2818.4453, the d2 loss is: -3089.289, the g loss is: 3117.8047, the ae loss is: 0.007592095, the jacobian loss is:0.1708942\n",
            "This is the iter 5826, the d1 loss is: 2823.0078, the d2 loss is: -3143.7578, the g loss is: 3110.2266, the ae loss is: 0.006015311, the jacobian loss is:0.182186\n",
            "This is the iter 5827, the d1 loss is: 2898.8203, the d2 loss is: -3166.8516, the g loss is: 3135.9531, the ae loss is: 0.005282189, the jacobian loss is:0.2372697\n",
            "This is the iter 5828, the d1 loss is: 2921.0078, the d2 loss is: -3196.5469, the g loss is: 3233.3281, the ae loss is: 0.008237252, the jacobian loss is:0.18603662\n",
            "This is the iter 5829, the d1 loss is: 2884.1016, the d2 loss is: -3160.1875, the g loss is: 3167.0938, the ae loss is: 0.005668071, the jacobian loss is:0.23926337\n",
            "This is the iter 5830, the d1 loss is: 2933.7578, the d2 loss is: -3200.3047, the g loss is: 3170.7188, the ae loss is: 0.008532228, the jacobian loss is:0.16030747\n",
            "This is the iter 5831, the d1 loss is: 2859.2266, the d2 loss is: -3141.7422, the g loss is: 3113.9531, the ae loss is: 0.006964191, the jacobian loss is:0.19242729\n",
            "This is the iter 5832, the d1 loss is: 2848.9297, the d2 loss is: -3152.6562, the g loss is: 3201.4297, the ae loss is: 0.008175823, the jacobian loss is:0.20273767\n",
            "This is the iter 5833, the d1 loss is: 2899.9375, the d2 loss is: -3200.125, the g loss is: 3168.7656, the ae loss is: 0.0067255553, the jacobian loss is:0.18103893\n",
            "This is the iter 5834, the d1 loss is: 2833.289, the d2 loss is: -3153.6328, the g loss is: 3116.6719, the ae loss is: 0.0057195956, the jacobian loss is:0.1724339\n",
            "This is the iter 5835, the d1 loss is: 2858.7656, the d2 loss is: -3154.7734, the g loss is: 3112.4844, the ae loss is: 0.006505293, the jacobian loss is:0.22775362\n",
            "This is the iter 5836, the d1 loss is: 2834.6016, the d2 loss is: -3133.1172, the g loss is: 3103.2578, the ae loss is: 0.007231834, the jacobian loss is:0.13449346\n",
            "This is the iter 5837, the d1 loss is: 2783.0547, the d2 loss is: -3063.6719, the g loss is: 3101.9922, the ae loss is: 0.0066419295, the jacobian loss is:0.1871839\n",
            "This is the iter 5838, the d1 loss is: 2865.3594, the d2 loss is: -3172.6016, the g loss is: 3190.0938, the ae loss is: 0.0067633037, the jacobian loss is:0.22445007\n",
            "This is the iter 5839, the d1 loss is: 2632.6094, the d2 loss is: -2891.4062, the g loss is: 2869.7578, the ae loss is: 0.006887842, the jacobian loss is:0.20200862\n",
            "This is the iter 5840, the d1 loss is: 2794.6484, the d2 loss is: -3078.6328, the g loss is: 3074.586, the ae loss is: 0.0101408325, the jacobian loss is:0.21970655\n",
            "This is the iter 5841, the d1 loss is: 2906.789, the d2 loss is: -3217.6172, the g loss is: 3225.8516, the ae loss is: 0.006010933, the jacobian loss is:0.19158344\n",
            "This is the iter 5842, the d1 loss is: 2733.4375, the d2 loss is: -3043.289, the g loss is: 3015.9531, the ae loss is: 0.013196588, the jacobian loss is:0.1488878\n",
            "This is the iter 5843, the d1 loss is: 2743.7812, the d2 loss is: -3031.289, the g loss is: 2962.9688, the ae loss is: 0.011255429, the jacobian loss is:0.2689959\n",
            "This is the iter 5844, the d1 loss is: 2548.2422, the d2 loss is: -2846.3203, the g loss is: 2842.6719, the ae loss is: 0.0057122805, the jacobian loss is:0.23925462\n",
            "This is the iter 5845, the d1 loss is: 2770.6406, the d2 loss is: -3075.4844, the g loss is: 3082.2031, the ae loss is: 0.004849456, the jacobian loss is:0.19126223\n",
            "This is the iter 5846, the d1 loss is: 2834.9062, the d2 loss is: -3122.3438, the g loss is: 3130.914, the ae loss is: 0.008818804, the jacobian loss is:0.18596973\n",
            "This is the iter 5847, the d1 loss is: 2875.7969, the d2 loss is: -3123.336, the g loss is: 3085.7812, the ae loss is: 0.0070894724, the jacobian loss is:0.19356638\n",
            "This is the iter 5848, the d1 loss is: 2931.3438, the d2 loss is: -3209.7969, the g loss is: 3183.836, the ae loss is: 0.007664998, the jacobian loss is:0.2865459\n",
            "This is the iter 5849, the d1 loss is: 2802.2969, the d2 loss is: -3090.6484, the g loss is: 3143.4375, the ae loss is: 0.0071721436, the jacobian loss is:0.24480808\n",
            "This is the iter 5850, the d1 loss is: 2795.1172, the d2 loss is: -3088.1562, the g loss is: 3121.086, the ae loss is: 0.007805478, the jacobian loss is:0.24855027\n",
            "This is the iter 5851, the d1 loss is: 2789.3516, the d2 loss is: -3073.6172, the g loss is: 3067.9531, the ae loss is: 0.006439441, the jacobian loss is:0.1616682\n",
            "This is the iter 5852, the d1 loss is: 2826.2344, the d2 loss is: -3148.4062, the g loss is: 3109.586, the ae loss is: 0.006287678, the jacobian loss is:0.15390722\n",
            "This is the iter 5853, the d1 loss is: 2798.7969, the d2 loss is: -3096.4375, the g loss is: 3090.336, the ae loss is: 0.007139788, the jacobian loss is:0.19600989\n",
            "This is the iter 5854, the d1 loss is: 2859.2812, the d2 loss is: -3111.1328, the g loss is: 3088.3125, the ae loss is: 0.010707395, the jacobian loss is:0.17609586\n",
            "This is the iter 5855, the d1 loss is: 2786.6406, the d2 loss is: -3071.4688, the g loss is: 3114.9688, the ae loss is: 0.007522503, the jacobian loss is:0.16613579\n",
            "This is the iter 5856, the d1 loss is: 2840.3047, the d2 loss is: -3152.164, the g loss is: 3146.3594, the ae loss is: 0.006955664, the jacobian loss is:0.15561117\n",
            "This is the iter 5857, the d1 loss is: 2704.1719, the d2 loss is: -3012.0547, the g loss is: 2983.8203, the ae loss is: 0.0054404177, the jacobian loss is:0.1625395\n",
            "This is the iter 5858, the d1 loss is: 2768.4219, the d2 loss is: -3077.5938, the g loss is: 3134.664, the ae loss is: 0.008204282, the jacobian loss is:0.17798084\n",
            "This is the iter 5859, the d1 loss is: 2813.9531, the d2 loss is: -3144.7188, the g loss is: 3097.3984, the ae loss is: 0.007350391, the jacobian loss is:0.23167711\n",
            "This is the iter 5860, the d1 loss is: 2801.6562, the d2 loss is: -3112.211, the g loss is: 3092.4844, the ae loss is: 0.009042522, the jacobian loss is:0.16518895\n",
            "This is the iter 5861, the d1 loss is: 2817.3828, the d2 loss is: -3105.1797, the g loss is: 3115.2969, the ae loss is: 0.0067754593, the jacobian loss is:0.22189312\n",
            "This is the iter 5862, the d1 loss is: 2852.9297, the d2 loss is: -3153.3438, the g loss is: 3092.086, the ae loss is: 0.0062954263, the jacobian loss is:0.20270544\n",
            "This is the iter 5863, the d1 loss is: 2807.1953, the d2 loss is: -3116.7188, the g loss is: 3078.4688, the ae loss is: 0.0070210006, the jacobian loss is:0.26614827\n",
            "This is the iter 5864, the d1 loss is: 2669.664, the d2 loss is: -3021.414, the g loss is: 2968.0938, the ae loss is: 0.0074068215, the jacobian loss is:0.17626566\n",
            "This is the iter 5865, the d1 loss is: 2720.8281, the d2 loss is: -2983.4766, the g loss is: 3137.8125, the ae loss is: 0.0084265545, the jacobian loss is:0.16215703\n",
            "This is the iter 5866, the d1 loss is: 2755.5078, the d2 loss is: -3070.5078, the g loss is: 3103.9922, the ae loss is: 0.0060055777, the jacobian loss is:0.15914077\n",
            "This is the iter 5867, the d1 loss is: 2715.8125, the d2 loss is: -3020.9766, the g loss is: 3078.9688, the ae loss is: 0.0050306795, the jacobian loss is:0.17657597\n",
            "This is the iter 5868, the d1 loss is: 2780.0938, the d2 loss is: -3085.8438, the g loss is: 3015.5156, the ae loss is: 0.0077882353, the jacobian loss is:0.15223107\n",
            "This is the iter 5869, the d1 loss is: 2834.3984, the d2 loss is: -3161.0, the g loss is: 3090.3125, the ae loss is: 0.007732939, the jacobian loss is:0.22427659\n",
            "This is the iter 5870, the d1 loss is: 2872.6719, the d2 loss is: -3167.9062, the g loss is: 3217.7188, the ae loss is: 0.0105482945, the jacobian loss is:0.18579482\n",
            "This is the iter 5871, the d1 loss is: 2831.4375, the d2 loss is: -3132.3438, the g loss is: 3128.289, the ae loss is: 0.008607265, the jacobian loss is:0.16209108\n",
            "This is the iter 5872, the d1 loss is: 2898.4453, the d2 loss is: -3163.7188, the g loss is: 3118.875, the ae loss is: 0.009755475, the jacobian loss is:0.13775383\n",
            "This is the iter 5873, the d1 loss is: 2807.1328, the d2 loss is: -3086.2656, the g loss is: 3086.1562, the ae loss is: 0.007830217, the jacobian loss is:0.18282318\n",
            "This is the iter 5874, the d1 loss is: 2645.7031, the d2 loss is: -2979.4844, the g loss is: 2920.4688, the ae loss is: 0.0072906865, the jacobian loss is:0.2033362\n",
            "This is the iter 5875, the d1 loss is: 2762.1094, the d2 loss is: -3052.0781, the g loss is: 3014.1328, the ae loss is: 0.006065843, the jacobian loss is:0.13791557\n",
            "This is the iter 5876, the d1 loss is: 2830.2734, the d2 loss is: -3105.1875, the g loss is: 3106.7969, the ae loss is: 0.0062696263, the jacobian loss is:0.13268378\n",
            "This is the iter 5877, the d1 loss is: 2853.4062, the d2 loss is: -3133.4688, the g loss is: 3126.7734, the ae loss is: 0.008184317, the jacobian loss is:0.17606613\n",
            "This is the iter 5878, the d1 loss is: 2917.2188, the d2 loss is: -3211.086, the g loss is: 3177.4297, the ae loss is: 0.0060903197, the jacobian loss is:0.14326355\n",
            "This is the iter 5879, the d1 loss is: 2764.9531, the d2 loss is: -3031.7812, the g loss is: 3028.1484, the ae loss is: 0.0055219303, the jacobian loss is:0.20632407\n",
            "This is the iter 5880, the d1 loss is: 2917.586, the d2 loss is: -3222.9922, the g loss is: 3216.125, the ae loss is: 0.008340374, the jacobian loss is:0.15984873\n",
            "This is the iter 5881, the d1 loss is: 2855.0938, the d2 loss is: -3143.8828, the g loss is: 3030.6328, the ae loss is: 0.0102805365, the jacobian loss is:0.20045008\n",
            "This is the iter 5882, the d1 loss is: 2743.0469, the d2 loss is: -3047.7344, the g loss is: 3080.5781, the ae loss is: 0.008605755, the jacobian loss is:0.2143224\n",
            "This is the iter 5883, the d1 loss is: 2710.4688, the d2 loss is: -2984.9766, the g loss is: 3059.4219, the ae loss is: 0.0056043295, the jacobian loss is:0.16744512\n",
            "This is the iter 5884, the d1 loss is: 2876.0938, the d2 loss is: -3176.7266, the g loss is: 3229.6328, the ae loss is: 0.0099771265, the jacobian loss is:0.18768637\n",
            "This is the iter 5885, the d1 loss is: 2788.4219, the d2 loss is: -3096.6797, the g loss is: 3075.1797, the ae loss is: 0.0075911256, the jacobian loss is:0.16013184\n",
            "This is the iter 5886, the d1 loss is: 2852.2812, the d2 loss is: -3156.6875, the g loss is: 3167.8906, the ae loss is: 0.007123372, the jacobian loss is:0.18408927\n",
            "This is the iter 5887, the d1 loss is: 2855.1875, the d2 loss is: -3112.211, the g loss is: 3115.4844, the ae loss is: 0.008210949, the jacobian loss is:0.21339169\n",
            "This is the iter 5888, the d1 loss is: 2824.1875, the d2 loss is: -3094.6484, the g loss is: 3051.6094, the ae loss is: 0.00788541, the jacobian loss is:0.17425402\n",
            "This is the iter 5889, the d1 loss is: 2816.4375, the d2 loss is: -3107.7031, the g loss is: 3110.8438, the ae loss is: 0.006995924, the jacobian loss is:0.17418654\n",
            "This is the iter 5890, the d1 loss is: 2890.8281, the d2 loss is: -3157.8906, the g loss is: 3170.4766, the ae loss is: 0.0054510776, the jacobian loss is:0.17123532\n",
            "This is the iter 5891, the d1 loss is: 2876.4922, the d2 loss is: -3132.7188, the g loss is: 3190.1875, the ae loss is: 0.006510284, the jacobian loss is:0.15148938\n",
            "This is the iter 5892, the d1 loss is: 2823.1172, the d2 loss is: -3109.1484, the g loss is: 3129.0625, the ae loss is: 0.008323936, the jacobian loss is:0.15508033\n",
            "This is the iter 5893, the d1 loss is: 2870.1484, the d2 loss is: -3156.164, the g loss is: 3176.0312, the ae loss is: 0.007068194, the jacobian loss is:0.29073977\n",
            "This is the iter 5894, the d1 loss is: 2878.0547, the d2 loss is: -3162.6953, the g loss is: 3164.0312, the ae loss is: 0.0060643675, the jacobian loss is:0.15134992\n",
            "This is the iter 5895, the d1 loss is: 2813.6484, the d2 loss is: -3124.9375, the g loss is: 3083.7578, the ae loss is: 0.010217505, the jacobian loss is:0.23772533\n",
            "This is the iter 5896, the d1 loss is: 2873.5625, the d2 loss is: -3155.1719, the g loss is: 3147.1016, the ae loss is: 0.0064106854, the jacobian loss is:0.23444256\n",
            "This is the iter 5897, the d1 loss is: 2775.3125, the d2 loss is: -3056.5156, the g loss is: 3139.1797, the ae loss is: 0.00650202, the jacobian loss is:0.20643374\n",
            "This is the iter 5898, the d1 loss is: 2879.4062, the d2 loss is: -3158.2812, the g loss is: 3193.2656, the ae loss is: 0.005876331, the jacobian loss is:0.19328916\n",
            "This is the iter 5899, the d1 loss is: 2654.8203, the d2 loss is: -2930.5312, the g loss is: 2937.25, the ae loss is: 0.0054673003, the jacobian loss is:0.17480806\n",
            "This is the iter 5900, the d1 loss is: 2752.5625, the d2 loss is: -3051.7578, the g loss is: 3002.961, the ae loss is: 0.0069939764, the jacobian loss is:0.11956232\n",
            "0.26126665\n",
            "1.0126408\n",
            "This is the iter 5901, the d1 loss is: 2757.8125, the d2 loss is: -3068.8828, the g loss is: 3046.3906, the ae loss is: 0.007410546, the jacobian loss is:0.1861795\n",
            "This is the iter 5902, the d1 loss is: 2747.914, the d2 loss is: -3055.0703, the g loss is: 2999.0078, the ae loss is: 0.007451948, the jacobian loss is:0.11872045\n",
            "This is the iter 5903, the d1 loss is: 2710.7188, the d2 loss is: -2972.3281, the g loss is: 3046.75, the ae loss is: 0.005031001, the jacobian loss is:0.12359838\n",
            "This is the iter 5904, the d1 loss is: 2793.5625, the d2 loss is: -3103.3438, the g loss is: 3089.5078, the ae loss is: 0.0072201304, the jacobian loss is:0.2656401\n",
            "This is the iter 5905, the d1 loss is: 2832.375, the d2 loss is: -3098.0469, the g loss is: 3048.3828, the ae loss is: 0.0064658676, the jacobian loss is:0.13994937\n",
            "This is the iter 5906, the d1 loss is: 2936.6953, the d2 loss is: -3234.9688, the g loss is: 3219.9688, the ae loss is: 0.006571682, the jacobian loss is:0.12186796\n",
            "This is the iter 5907, the d1 loss is: 2827.9062, the d2 loss is: -3123.3828, the g loss is: 3129.25, the ae loss is: 0.0068196664, the jacobian loss is:0.16759282\n",
            "This is the iter 5908, the d1 loss is: 2801.9297, the d2 loss is: -3087.7578, the g loss is: 3068.8984, the ae loss is: 0.009975429, the jacobian loss is:0.19623865\n",
            "This is the iter 5909, the d1 loss is: 2766.3047, the d2 loss is: -3074.5703, the g loss is: 3106.039, the ae loss is: 0.006934793, the jacobian loss is:0.28386042\n",
            "This is the iter 5910, the d1 loss is: 2915.5312, the d2 loss is: -3205.3672, the g loss is: 3216.1562, the ae loss is: 0.0063596666, the jacobian loss is:0.15223347\n",
            "This is the iter 5911, the d1 loss is: 2711.0, the d2 loss is: -2988.9531, the g loss is: 3008.7031, the ae loss is: 0.007174678, the jacobian loss is:0.16354574\n",
            "This is the iter 5912, the d1 loss is: 2768.2734, the d2 loss is: -3089.8125, the g loss is: 3062.789, the ae loss is: 0.009712827, the jacobian loss is:0.21423112\n",
            "This is the iter 5913, the d1 loss is: 2799.8594, the d2 loss is: -3109.75, the g loss is: 3105.3594, the ae loss is: 0.006135105, the jacobian loss is:0.1361943\n",
            "This is the iter 5914, the d1 loss is: 2909.0781, the d2 loss is: -3172.414, the g loss is: 3145.125, the ae loss is: 0.008454785, the jacobian loss is:0.2711126\n",
            "This is the iter 5915, the d1 loss is: 2856.625, the d2 loss is: -3113.9375, the g loss is: 3083.7578, the ae loss is: 0.0074377633, the jacobian loss is:0.20429459\n",
            "This is the iter 5916, the d1 loss is: 2865.6172, the d2 loss is: -3186.7812, the g loss is: 3180.1328, the ae loss is: 0.005583545, the jacobian loss is:0.13218687\n",
            "This is the iter 5917, the d1 loss is: 2786.1094, the d2 loss is: -3081.6875, the g loss is: 3044.9062, the ae loss is: 0.009297231, the jacobian loss is:0.20260313\n",
            "This is the iter 5918, the d1 loss is: 2826.1953, the d2 loss is: -3089.7578, the g loss is: 3106.2031, the ae loss is: 0.006392473, the jacobian loss is:0.15948108\n",
            "This is the iter 5919, the d1 loss is: 2899.586, the d2 loss is: -3155.3594, the g loss is: 3134.4219, the ae loss is: 0.0069439774, the jacobian loss is:0.11885929\n",
            "This is the iter 5920, the d1 loss is: 2781.4922, the d2 loss is: -3066.75, the g loss is: 3131.2031, the ae loss is: 0.008484102, the jacobian loss is:0.15073279\n",
            "This is the iter 5921, the d1 loss is: 2792.8125, the d2 loss is: -3088.875, the g loss is: 3095.8984, the ae loss is: 0.006898366, the jacobian loss is:0.119546495\n",
            "This is the iter 5922, the d1 loss is: 2740.7266, the d2 loss is: -3055.25, the g loss is: 3130.5547, the ae loss is: 0.0050307345, the jacobian loss is:0.119198516\n",
            "This is the iter 5923, the d1 loss is: 2915.2344, the d2 loss is: -3185.1797, the g loss is: 3157.414, the ae loss is: 0.00678334, the jacobian loss is:0.21747147\n",
            "This is the iter 5924, the d1 loss is: 2688.5156, the d2 loss is: -3011.1719, the g loss is: 2994.6328, the ae loss is: 0.0061159446, the jacobian loss is:0.15154389\n",
            "This is the iter 5925, the d1 loss is: 2797.0, the d2 loss is: -3103.8516, the g loss is: 3091.711, the ae loss is: 0.0086559085, the jacobian loss is:0.19497114\n",
            "This is the iter 5926, the d1 loss is: 2843.2656, the d2 loss is: -3177.1719, the g loss is: 3149.9062, the ae loss is: 0.005376692, the jacobian loss is:0.13612866\n",
            "This is the iter 5927, the d1 loss is: 2813.461, the d2 loss is: -3116.6562, the g loss is: 3105.1875, the ae loss is: 0.006747773, the jacobian loss is:0.18474247\n",
            "This is the iter 5928, the d1 loss is: 2843.0625, the d2 loss is: -3157.9688, the g loss is: 3168.8906, the ae loss is: 0.006798696, the jacobian loss is:0.16476853\n",
            "This is the iter 5929, the d1 loss is: 2882.6094, the d2 loss is: -3188.0938, the g loss is: 3124.539, the ae loss is: 0.008670872, the jacobian loss is:0.17500605\n",
            "This is the iter 5930, the d1 loss is: 2891.9531, the d2 loss is: -3161.1172, the g loss is: 3177.8672, the ae loss is: 0.009112476, the jacobian loss is:0.14862303\n",
            "This is the iter 5931, the d1 loss is: 2786.789, the d2 loss is: -3078.125, the g loss is: 3161.8906, the ae loss is: 0.0072557013, the jacobian loss is:0.17725728\n",
            "This is the iter 5932, the d1 loss is: 2850.9531, the d2 loss is: -3169.9453, the g loss is: 3132.4844, the ae loss is: 0.00622268, the jacobian loss is:0.16018212\n",
            "This is the iter 5933, the d1 loss is: 2847.1875, the d2 loss is: -3133.8906, the g loss is: 3135.8672, the ae loss is: 0.004441878, the jacobian loss is:0.13567111\n",
            "This is the iter 5934, the d1 loss is: 2830.0469, the d2 loss is: -3104.8828, the g loss is: 3071.1016, the ae loss is: 0.009060826, the jacobian loss is:0.20787035\n",
            "This is the iter 5935, the d1 loss is: 2702.961, the d2 loss is: -2995.2266, the g loss is: 3017.0156, the ae loss is: 0.008181447, the jacobian loss is:0.21045914\n",
            "This is the iter 5936, the d1 loss is: 2830.914, the d2 loss is: -3145.4062, the g loss is: 3123.4219, the ae loss is: 0.0065714344, the jacobian loss is:0.14466064\n",
            "This is the iter 5937, the d1 loss is: 2756.6172, the d2 loss is: -3023.3281, the g loss is: 3067.5, the ae loss is: 0.0064277537, the jacobian loss is:0.14867252\n",
            "This is the iter 5938, the d1 loss is: 2889.9375, the d2 loss is: -3184.1094, the g loss is: 3189.0, the ae loss is: 0.006050133, the jacobian loss is:0.17650816\n",
            "This is the iter 5939, the d1 loss is: 2616.4766, the d2 loss is: -2927.3438, the g loss is: 2885.3594, the ae loss is: 0.0056763114, the jacobian loss is:0.1484075\n",
            "This is the iter 5940, the d1 loss is: 2743.0469, the d2 loss is: -3052.1406, the g loss is: 3051.125, the ae loss is: 0.008431949, the jacobian loss is:0.17167605\n",
            "This is the iter 5941, the d1 loss is: 2745.5625, the d2 loss is: -3053.039, the g loss is: 3037.789, the ae loss is: 0.006906495, the jacobian loss is:0.14207594\n",
            "This is the iter 5942, the d1 loss is: 2895.8125, the d2 loss is: -3211.0312, the g loss is: 3192.5781, the ae loss is: 0.009296901, the jacobian loss is:0.16279326\n",
            "This is the iter 5943, the d1 loss is: 2900.3125, the d2 loss is: -3148.9375, the g loss is: 3090.789, the ae loss is: 0.008366937, the jacobian loss is:0.19465427\n",
            "This is the iter 5944, the d1 loss is: 2809.7656, the d2 loss is: -3097.8125, the g loss is: 3151.8594, the ae loss is: 0.006591008, the jacobian loss is:0.13596424\n",
            "This is the iter 5945, the d1 loss is: 2780.5312, the d2 loss is: -3074.7656, the g loss is: 3108.8516, the ae loss is: 0.0050262944, the jacobian loss is:0.23278804\n",
            "This is the iter 5946, the d1 loss is: 2809.9219, the d2 loss is: -3119.0781, the g loss is: 3099.336, the ae loss is: 0.0100423815, the jacobian loss is:0.16431579\n",
            "This is the iter 5947, the d1 loss is: 3092.1719, the d2 loss is: -3347.0781, the g loss is: 3318.2422, the ae loss is: 0.0071402723, the jacobian loss is:0.15163875\n",
            "This is the iter 5948, the d1 loss is: 2865.0781, the d2 loss is: -3128.4062, the g loss is: 3113.4766, the ae loss is: 0.010947347, the jacobian loss is:0.16907631\n",
            "This is the iter 5949, the d1 loss is: 2867.086, the d2 loss is: -3142.6094, the g loss is: 3194.375, the ae loss is: 0.0069816066, the jacobian loss is:0.17656171\n",
            "This is the iter 5950, the d1 loss is: 3157.2344, the d2 loss is: -3470.7344, the g loss is: 3509.6953, the ae loss is: 0.006897725, the jacobian loss is:0.16840065\n",
            "This is the iter 5951, the d1 loss is: 2962.6484, the d2 loss is: -3251.8047, the g loss is: 3259.8828, the ae loss is: 0.009017905, the jacobian loss is:0.18366693\n",
            "This is the iter 5952, the d1 loss is: 2822.3438, the d2 loss is: -3089.5, the g loss is: 3066.9219, the ae loss is: 0.0102439355, the jacobian loss is:0.21451151\n",
            "This is the iter 5953, the d1 loss is: 2896.2734, the d2 loss is: -3184.5625, the g loss is: 3163.25, the ae loss is: 0.00992766, the jacobian loss is:0.17515874\n",
            "This is the iter 5954, the d1 loss is: 2816.5625, the d2 loss is: -3107.4375, the g loss is: 3077.1875, the ae loss is: 0.0065437187, the jacobian loss is:0.18229045\n",
            "This is the iter 5955, the d1 loss is: 2955.0156, the d2 loss is: -3250.4688, the g loss is: 3233.75, the ae loss is: 0.00597199, the jacobian loss is:0.18927544\n",
            "This is the iter 5956, the d1 loss is: 2954.8906, the d2 loss is: -3280.7188, the g loss is: 3245.7422, the ae loss is: 0.009023124, the jacobian loss is:0.17065305\n",
            "This is the iter 5957, the d1 loss is: 2953.1484, the d2 loss is: -3231.0703, the g loss is: 3207.4531, the ae loss is: 0.0072690756, the jacobian loss is:0.15197626\n",
            "This is the iter 5958, the d1 loss is: 2827.789, the d2 loss is: -3121.4297, the g loss is: 3213.0312, the ae loss is: 0.0063201855, the jacobian loss is:0.20796789\n",
            "This is the iter 5959, the d1 loss is: 2718.9922, the d2 loss is: -2998.5547, the g loss is: 3091.9375, the ae loss is: 0.007233288, the jacobian loss is:0.18375641\n",
            "This is the iter 5960, the d1 loss is: 2820.25, the d2 loss is: -3106.2266, the g loss is: 3121.6953, the ae loss is: 0.005656011, the jacobian loss is:0.08707409\n",
            "This is the iter 5961, the d1 loss is: 2885.1875, the d2 loss is: -3194.625, the g loss is: 3165.5312, the ae loss is: 0.0063993633, the jacobian loss is:0.21772243\n",
            "This is the iter 5962, the d1 loss is: 2854.1406, the d2 loss is: -3112.75, the g loss is: 3076.0469, the ae loss is: 0.00798057, the jacobian loss is:0.17323242\n",
            "This is the iter 5963, the d1 loss is: 2847.6484, the d2 loss is: -3134.836, the g loss is: 3170.9375, the ae loss is: 0.0062070624, the jacobian loss is:0.14291711\n",
            "This is the iter 5964, the d1 loss is: 2918.8984, the d2 loss is: -3234.8125, the g loss is: 3274.6172, the ae loss is: 0.005290909, the jacobian loss is:0.1754541\n",
            "This is the iter 5965, the d1 loss is: 2910.25, the d2 loss is: -3192.6562, the g loss is: 3170.6562, the ae loss is: 0.008035397, the jacobian loss is:0.18240248\n",
            "This is the iter 5966, the d1 loss is: 2989.2344, the d2 loss is: -3328.125, the g loss is: 3259.836, the ae loss is: 0.009617005, the jacobian loss is:0.17471465\n",
            "This is the iter 5967, the d1 loss is: 2944.8516, the d2 loss is: -3222.7969, the g loss is: 3186.0156, the ae loss is: 0.007782016, the jacobian loss is:0.1688043\n",
            "This is the iter 5968, the d1 loss is: 2940.4688, the d2 loss is: -3249.8125, the g loss is: 3244.7812, the ae loss is: 0.006975266, the jacobian loss is:0.13962257\n",
            "This is the iter 5969, the d1 loss is: 3011.7266, the d2 loss is: -3265.7266, the g loss is: 3269.1094, the ae loss is: 0.0070495643, the jacobian loss is:0.15523694\n",
            "This is the iter 5970, the d1 loss is: 2798.4531, the d2 loss is: -3106.9453, the g loss is: 3099.9062, the ae loss is: 0.006835907, the jacobian loss is:0.18791218\n",
            "This is the iter 5971, the d1 loss is: 2878.6094, the d2 loss is: -3174.75, the g loss is: 3121.6172, the ae loss is: 0.0071886536, the jacobian loss is:0.21051946\n",
            "This is the iter 5972, the d1 loss is: 2888.9844, the d2 loss is: -3152.4297, the g loss is: 3205.914, the ae loss is: 0.005689392, the jacobian loss is:0.17772315\n",
            "This is the iter 5973, the d1 loss is: 2641.8672, the d2 loss is: -2943.1094, the g loss is: 2900.4219, the ae loss is: 0.008291678, the jacobian loss is:0.19724151\n",
            "This is the iter 5974, the d1 loss is: 2873.2031, the d2 loss is: -3178.9688, the g loss is: 3152.0625, the ae loss is: 0.0068320944, the jacobian loss is:0.1552815\n",
            "This is the iter 5975, the d1 loss is: 2789.3047, the d2 loss is: -3081.4219, the g loss is: 3093.3047, the ae loss is: 0.010077784, the jacobian loss is:0.15117942\n",
            "This is the iter 5976, the d1 loss is: 2772.4062, the d2 loss is: -3069.5625, the g loss is: 3030.2422, the ae loss is: 0.007291221, the jacobian loss is:0.1639302\n",
            "This is the iter 5977, the d1 loss is: 2833.9844, the d2 loss is: -3128.7344, the g loss is: 3157.3828, the ae loss is: 0.0052945456, the jacobian loss is:0.17224146\n",
            "This is the iter 5978, the d1 loss is: 2904.4453, the d2 loss is: -3166.9844, the g loss is: 3202.7812, the ae loss is: 0.007808605, the jacobian loss is:0.19347972\n",
            "This is the iter 5979, the d1 loss is: 2919.6406, the d2 loss is: -3214.5938, the g loss is: 3176.9922, the ae loss is: 0.007010758, the jacobian loss is:0.19756913\n",
            "This is the iter 5980, the d1 loss is: 2906.2031, the d2 loss is: -3168.1953, the g loss is: 3186.9062, the ae loss is: 0.008223685, the jacobian loss is:0.15824027\n",
            "This is the iter 5981, the d1 loss is: 2890.7656, the d2 loss is: -3165.9531, the g loss is: 3217.6328, the ae loss is: 0.006029281, the jacobian loss is:0.12181534\n",
            "This is the iter 5982, the d1 loss is: 2833.6172, the d2 loss is: -3127.6875, the g loss is: 3107.625, the ae loss is: 0.008692229, the jacobian loss is:0.13195178\n",
            "This is the iter 5983, the d1 loss is: 2852.5234, the d2 loss is: -3120.0781, the g loss is: 3163.8125, the ae loss is: 0.005928468, the jacobian loss is:0.19094254\n",
            "This is the iter 5984, the d1 loss is: 2908.3125, the d2 loss is: -3164.6797, the g loss is: 3187.6562, the ae loss is: 0.008933879, the jacobian loss is:0.20877291\n",
            "This is the iter 5985, the d1 loss is: 2937.3438, the d2 loss is: -3208.1953, the g loss is: 3154.0938, the ae loss is: 0.00771691, the jacobian loss is:0.12777771\n",
            "This is the iter 5986, the d1 loss is: 2887.5, the d2 loss is: -3216.2812, the g loss is: 3186.9453, the ae loss is: 0.0098836385, the jacobian loss is:0.18591225\n",
            "This is the iter 5987, the d1 loss is: 2791.1094, the d2 loss is: -3071.5625, the g loss is: 3110.0625, the ae loss is: 0.005284038, the jacobian loss is:0.12939\n",
            "This is the iter 5988, the d1 loss is: 3242.4297, the d2 loss is: -3507.9297, the g loss is: 3526.5469, the ae loss is: 0.00781378, the jacobian loss is:0.13594379\n",
            "This is the iter 5989, the d1 loss is: 2860.0625, the d2 loss is: -3163.4688, the g loss is: 3196.7969, the ae loss is: 0.0062242094, the jacobian loss is:0.17509538\n",
            "This is the iter 5990, the d1 loss is: 2894.4531, the d2 loss is: -3153.4297, the g loss is: 3148.6875, the ae loss is: 0.00911509, the jacobian loss is:0.16915706\n",
            "This is the iter 5991, the d1 loss is: 2699.9922, the d2 loss is: -2996.6562, the g loss is: 2964.875, the ae loss is: 0.0069474913, the jacobian loss is:0.13578485\n",
            "This is the iter 5992, the d1 loss is: 2749.1875, the d2 loss is: -3041.4062, the g loss is: 3109.9844, the ae loss is: 0.008345646, the jacobian loss is:0.17195417\n",
            "This is the iter 5993, the d1 loss is: 2803.9297, the d2 loss is: -3053.4844, the g loss is: 3038.4844, the ae loss is: 0.0061095404, the jacobian loss is:0.1408159\n",
            "This is the iter 5994, the d1 loss is: 2769.0312, the d2 loss is: -3085.2031, the g loss is: 3097.3594, the ae loss is: 0.0057170563, the jacobian loss is:0.12993492\n",
            "This is the iter 5995, the d1 loss is: 2826.5312, the d2 loss is: -3117.7422, the g loss is: 3111.5625, the ae loss is: 0.0064586424, the jacobian loss is:0.10024211\n",
            "This is the iter 5996, the d1 loss is: 2804.9922, the d2 loss is: -3102.25, the g loss is: 3113.336, the ae loss is: 0.0085929725, the jacobian loss is:0.16817042\n",
            "This is the iter 5997, the d1 loss is: 2821.8438, the d2 loss is: -3118.6406, the g loss is: 3129.0469, the ae loss is: 0.004997764, the jacobian loss is:0.21595754\n",
            "This is the iter 5998, the d1 loss is: 2718.4375, the d2 loss is: -3051.5938, the g loss is: 3137.1562, the ae loss is: 0.011192397, the jacobian loss is:0.1798955\n",
            "This is the iter 5999, the d1 loss is: 2589.6719, the d2 loss is: -2896.1875, the g loss is: 2884.2188, the ae loss is: 0.0065276707, the jacobian loss is:0.16525501\n",
            "This is the iter 6000, the d1 loss is: 2765.9688, the d2 loss is: -3105.7266, the g loss is: 3099.125, the ae loss is: 0.0053762095, the jacobian loss is:0.13315067\n",
            "0.2557957\n",
            "0.9951666\n",
            "This is the iter 6001, the d1 loss is: 2669.039, the d2 loss is: -2972.2969, the g loss is: 3003.7656, the ae loss is: 0.0054171868, the jacobian loss is:0.22866789\n",
            "This is the iter 6002, the d1 loss is: 2757.4922, the d2 loss is: -3069.4766, the g loss is: 3006.961, the ae loss is: 0.006700619, the jacobian loss is:0.20487921\n",
            "This is the iter 6003, the d1 loss is: 2773.6406, the d2 loss is: -3099.0781, the g loss is: 3067.836, the ae loss is: 0.0055963225, the jacobian loss is:0.19464318\n",
            "This is the iter 6004, the d1 loss is: 2936.1094, the d2 loss is: -3197.9453, the g loss is: 3138.3672, the ae loss is: 0.008989738, the jacobian loss is:0.15930289\n",
            "This is the iter 6005, the d1 loss is: 2795.0781, the d2 loss is: -3082.4531, the g loss is: 3064.125, the ae loss is: 0.005988046, the jacobian loss is:0.15094224\n",
            "This is the iter 6006, the d1 loss is: 2762.9688, the d2 loss is: -3040.0781, the g loss is: 3044.5156, the ae loss is: 0.007870223, the jacobian loss is:0.17021406\n",
            "This is the iter 6007, the d1 loss is: 2770.789, the d2 loss is: -3052.5312, the g loss is: 3074.914, the ae loss is: 0.007703943, the jacobian loss is:0.16015305\n",
            "This is the iter 6008, the d1 loss is: 2758.3828, the d2 loss is: -3026.4922, the g loss is: 3055.0547, the ae loss is: 0.0073055318, the jacobian loss is:0.12621199\n",
            "This is the iter 6009, the d1 loss is: 2765.7656, the d2 loss is: -3084.414, the g loss is: 3099.586, the ae loss is: 0.0044674524, the jacobian loss is:0.16733296\n",
            "This is the iter 6010, the d1 loss is: 2811.6016, the d2 loss is: -3139.8828, the g loss is: 3158.5781, the ae loss is: 0.0061530257, the jacobian loss is:0.15814988\n",
            "This is the iter 6011, the d1 loss is: 2704.5938, the d2 loss is: -2982.4375, the g loss is: 3001.5156, the ae loss is: 0.00834561, the jacobian loss is:0.19082552\n",
            "This is the iter 6012, the d1 loss is: 2689.0547, the d2 loss is: -3013.0625, the g loss is: 2995.6875, the ae loss is: 0.0066219647, the jacobian loss is:0.16572012\n",
            "This is the iter 6013, the d1 loss is: 2740.1094, the d2 loss is: -3075.4375, the g loss is: 3100.1562, the ae loss is: 0.007926979, the jacobian loss is:0.18929838\n",
            "This is the iter 6014, the d1 loss is: 2799.8047, the d2 loss is: -3095.7812, the g loss is: 3090.4062, the ae loss is: 0.008781623, the jacobian loss is:0.17463188\n",
            "This is the iter 6015, the d1 loss is: 2674.7969, the d2 loss is: -2986.6953, the g loss is: 3009.6172, the ae loss is: 0.005444346, the jacobian loss is:0.13199487\n",
            "This is the iter 6016, the d1 loss is: 2797.8672, the d2 loss is: -3120.875, the g loss is: 3130.961, the ae loss is: 0.008898162, the jacobian loss is:0.2408048\n",
            "This is the iter 6017, the d1 loss is: 2760.1719, the d2 loss is: -3036.7969, the g loss is: 3018.5547, the ae loss is: 0.007980011, the jacobian loss is:0.24287602\n",
            "This is the iter 6018, the d1 loss is: 2820.2031, the d2 loss is: -3104.1016, the g loss is: 3090.9531, the ae loss is: 0.00826453, the jacobian loss is:0.12723887\n",
            "This is the iter 6019, the d1 loss is: 2754.0469, the d2 loss is: -3052.789, the g loss is: 3019.711, the ae loss is: 0.0059164, the jacobian loss is:0.113348976\n",
            "This is the iter 6020, the d1 loss is: 2862.336, the d2 loss is: -3146.875, the g loss is: 3111.4062, the ae loss is: 0.006693924, the jacobian loss is:0.12069638\n",
            "This is the iter 6021, the d1 loss is: 2716.3672, the d2 loss is: -3031.4062, the g loss is: 3011.0938, the ae loss is: 0.006721922, the jacobian loss is:0.14463745\n",
            "This is the iter 6022, the d1 loss is: 2793.6953, the d2 loss is: -3096.5, the g loss is: 3083.6016, the ae loss is: 0.0076404763, the jacobian loss is:0.12448416\n",
            "This is the iter 6023, the d1 loss is: 2854.5156, the d2 loss is: -3152.7812, the g loss is: 3173.461, the ae loss is: 0.005371539, the jacobian loss is:0.123818785\n",
            "This is the iter 6024, the d1 loss is: 2677.789, the d2 loss is: -2962.8906, the g loss is: 3056.9844, the ae loss is: 0.0066022864, the jacobian loss is:0.15952067\n",
            "This is the iter 6025, the d1 loss is: 2724.3984, the d2 loss is: -3047.914, the g loss is: 3078.711, the ae loss is: 0.00496425, the jacobian loss is:0.20619881\n",
            "This is the iter 6026, the d1 loss is: 2772.1562, the d2 loss is: -3081.9844, the g loss is: 3022.2656, the ae loss is: 0.0122380685, the jacobian loss is:0.17212784\n",
            "This is the iter 6027, the d1 loss is: 2821.3203, the d2 loss is: -3121.1406, the g loss is: 3126.4688, the ae loss is: 0.0073888735, the jacobian loss is:0.1324408\n",
            "This is the iter 6028, the d1 loss is: 2789.6875, the d2 loss is: -3088.7188, the g loss is: 3098.4922, the ae loss is: 0.0071111144, the jacobian loss is:0.16723834\n",
            "This is the iter 6029, the d1 loss is: 2787.1406, the d2 loss is: -3112.0547, the g loss is: 3083.3281, the ae loss is: 0.008657141, the jacobian loss is:0.16087209\n",
            "This is the iter 6030, the d1 loss is: 2731.3906, the d2 loss is: -3001.2734, the g loss is: 2996.6797, the ae loss is: 0.006819323, the jacobian loss is:0.13977222\n",
            "This is the iter 6031, the d1 loss is: 2811.1094, the d2 loss is: -3086.1875, the g loss is: 3097.5625, the ae loss is: 0.0067157065, the jacobian loss is:0.23676383\n",
            "This is the iter 6032, the d1 loss is: 2686.1484, the d2 loss is: -2984.086, the g loss is: 3020.2656, the ae loss is: 0.0050001526, the jacobian loss is:0.16114348\n",
            "This is the iter 6033, the d1 loss is: 2797.0, the d2 loss is: -3087.3203, the g loss is: 3088.0781, the ae loss is: 0.0063244402, the jacobian loss is:0.16716899\n",
            "This is the iter 6034, the d1 loss is: 2770.5469, the d2 loss is: -3059.4062, the g loss is: 3097.3906, the ae loss is: 0.0049317665, the jacobian loss is:0.15977503\n",
            "This is the iter 6035, the d1 loss is: 2805.9688, the d2 loss is: -3074.3281, the g loss is: 3096.8594, the ae loss is: 0.0051041115, the jacobian loss is:0.10482314\n",
            "This is the iter 6036, the d1 loss is: 2777.7578, the d2 loss is: -3067.9844, the g loss is: 3130.1562, the ae loss is: 0.008473582, the jacobian loss is:0.1894237\n",
            "This is the iter 6037, the d1 loss is: 2698.6953, the d2 loss is: -2977.7188, the g loss is: 3070.125, the ae loss is: 0.0070300894, the jacobian loss is:0.17767583\n",
            "This is the iter 6038, the d1 loss is: 2861.914, the d2 loss is: -3160.5234, the g loss is: 3150.9766, the ae loss is: 0.0067252377, the jacobian loss is:0.11917331\n",
            "This is the iter 6039, the d1 loss is: 2933.0469, the d2 loss is: -3203.1406, the g loss is: 3247.8594, the ae loss is: 0.0060644913, the jacobian loss is:0.11526706\n",
            "This is the iter 6040, the d1 loss is: 2819.9297, the d2 loss is: -3110.5469, the g loss is: 3128.4062, the ae loss is: 0.006398265, the jacobian loss is:0.13350457\n",
            "This is the iter 6041, the d1 loss is: 2786.1719, the d2 loss is: -3085.8125, the g loss is: 3109.164, the ae loss is: 0.006829013, the jacobian loss is:0.12923688\n",
            "This is the iter 6042, the d1 loss is: 2776.3438, the d2 loss is: -3089.1328, the g loss is: 3051.1094, the ae loss is: 0.011373337, the jacobian loss is:0.16309372\n",
            "This is the iter 6043, the d1 loss is: 2790.8594, the d2 loss is: -3111.4219, the g loss is: 3096.5547, the ae loss is: 0.0052937316, the jacobian loss is:0.20259078\n",
            "This is the iter 6044, the d1 loss is: 2838.4062, the d2 loss is: -3113.6484, the g loss is: 3180.4375, the ae loss is: 0.006849612, the jacobian loss is:0.18738048\n",
            "This is the iter 6045, the d1 loss is: 2834.8438, the d2 loss is: -3130.039, the g loss is: 3091.75, the ae loss is: 0.0047682775, the jacobian loss is:0.14185043\n",
            "This is the iter 6046, the d1 loss is: 2859.375, the d2 loss is: -3135.3594, the g loss is: 3129.8438, the ae loss is: 0.0063897343, the jacobian loss is:0.1791212\n",
            "This is the iter 6047, the d1 loss is: 2800.8984, the d2 loss is: -3091.0469, the g loss is: 3084.25, the ae loss is: 0.0062769325, the jacobian loss is:0.10617677\n",
            "This is the iter 6048, the d1 loss is: 2769.8906, the d2 loss is: -3042.9375, the g loss is: 3015.6094, the ae loss is: 0.0060961386, the jacobian loss is:0.18829823\n",
            "This is the iter 6049, the d1 loss is: 2925.4062, the d2 loss is: -3237.039, the g loss is: 3208.3125, the ae loss is: 0.0069862986, the jacobian loss is:0.1895013\n",
            "This is the iter 6050, the d1 loss is: 2841.6953, the d2 loss is: -3117.289, the g loss is: 3113.8672, the ae loss is: 0.006117165, the jacobian loss is:0.11569377\n",
            "This is the iter 6051, the d1 loss is: 2775.6406, the d2 loss is: -3084.3203, the g loss is: 3083.0469, the ae loss is: 0.006411882, the jacobian loss is:0.15376887\n",
            "This is the iter 6052, the d1 loss is: 2762.211, the d2 loss is: -3047.4219, the g loss is: 3092.3828, the ae loss is: 0.0077396547, the jacobian loss is:0.14457114\n",
            "This is the iter 6053, the d1 loss is: 2603.2578, the d2 loss is: -2885.1484, the g loss is: 2948.2422, the ae loss is: 0.006601527, the jacobian loss is:0.20585907\n",
            "This is the iter 6054, the d1 loss is: 2856.2969, the d2 loss is: -3191.6875, the g loss is: 3164.8438, the ae loss is: 0.007893615, the jacobian loss is:0.13871358\n",
            "This is the iter 6055, the d1 loss is: 2817.6797, the d2 loss is: -3066.875, the g loss is: 3101.3281, the ae loss is: 0.008360214, the jacobian loss is:0.17633662\n",
            "This is the iter 6056, the d1 loss is: 2770.2734, the d2 loss is: -3093.9062, the g loss is: 3049.7188, the ae loss is: 0.005339384, the jacobian loss is:0.18868576\n",
            "This is the iter 6057, the d1 loss is: 2663.211, the d2 loss is: -2979.2969, the g loss is: 3069.1094, the ae loss is: 0.0073143654, the jacobian loss is:0.1253943\n",
            "This is the iter 6058, the d1 loss is: 2607.039, the d2 loss is: -2896.6562, the g loss is: 2894.586, the ae loss is: 0.005331305, the jacobian loss is:0.14922763\n",
            "This is the iter 6059, the d1 loss is: 2768.5938, the d2 loss is: -3083.8438, the g loss is: 3007.4062, the ae loss is: 0.008418772, the jacobian loss is:0.16547921\n",
            "This is the iter 6060, the d1 loss is: 2819.1328, the d2 loss is: -3080.9531, the g loss is: 3092.5156, the ae loss is: 0.0073528606, the jacobian loss is:0.19746974\n",
            "This is the iter 6061, the d1 loss is: 2781.3828, the d2 loss is: -3085.125, the g loss is: 3127.414, the ae loss is: 0.0061146547, the jacobian loss is:0.1739693\n",
            "This is the iter 6062, the d1 loss is: 2612.0078, the d2 loss is: -2831.086, the g loss is: 2883.6562, the ae loss is: 0.0071401577, the jacobian loss is:0.12182297\n",
            "This is the iter 6063, the d1 loss is: 2631.1016, the d2 loss is: -2950.9688, the g loss is: 2941.9531, the ae loss is: 0.0073976545, the jacobian loss is:0.20353895\n",
            "This is the iter 6064, the d1 loss is: 2768.9688, the d2 loss is: -3073.6484, the g loss is: 3075.9453, the ae loss is: 0.0054956274, the jacobian loss is:0.15161686\n",
            "This is the iter 6065, the d1 loss is: 2675.7969, the d2 loss is: -2938.8672, the g loss is: 2959.0625, the ae loss is: 0.008617221, the jacobian loss is:0.1697184\n",
            "This is the iter 6066, the d1 loss is: 2793.625, the d2 loss is: -3032.914, the g loss is: 3000.8438, the ae loss is: 0.0068656304, the jacobian loss is:0.16409443\n",
            "This is the iter 6067, the d1 loss is: 2786.5156, the d2 loss is: -3057.461, the g loss is: 3064.4375, the ae loss is: 0.0050869095, the jacobian loss is:0.19318236\n",
            "This is the iter 6068, the d1 loss is: 2753.4922, the d2 loss is: -3063.6484, the g loss is: 3065.711, the ae loss is: 0.005665357, the jacobian loss is:0.13956216\n",
            "This is the iter 6069, the d1 loss is: 3043.1094, the d2 loss is: -3341.2344, the g loss is: 3344.9297, the ae loss is: 0.008524692, the jacobian loss is:0.19014841\n",
            "This is the iter 6070, the d1 loss is: 2759.4531, the d2 loss is: -3067.6406, the g loss is: 3109.25, the ae loss is: 0.005910415, the jacobian loss is:0.13226938\n",
            "This is the iter 6071, the d1 loss is: 2825.914, the d2 loss is: -3094.539, the g loss is: 3113.8125, the ae loss is: 0.0071125827, the jacobian loss is:0.15299304\n",
            "This is the iter 6072, the d1 loss is: 2792.2656, the d2 loss is: -3107.5156, the g loss is: 3070.914, the ae loss is: 0.0050839772, the jacobian loss is:0.09915826\n",
            "This is the iter 6073, the d1 loss is: 2791.9688, the d2 loss is: -3116.7422, the g loss is: 3080.2344, the ae loss is: 0.006691907, the jacobian loss is:0.1629043\n",
            "This is the iter 6074, the d1 loss is: 2759.5938, the d2 loss is: -3053.4688, the g loss is: 3056.336, the ae loss is: 0.005075268, the jacobian loss is:0.15357335\n",
            "This is the iter 6075, the d1 loss is: 2723.4297, the d2 loss is: -3023.3594, the g loss is: 2999.4062, the ae loss is: 0.005945238, the jacobian loss is:0.1608097\n",
            "This is the iter 6076, the d1 loss is: 2630.711, the d2 loss is: -2932.1406, the g loss is: 3038.2422, the ae loss is: 0.006524356, the jacobian loss is:0.12371952\n",
            "This is the iter 6077, the d1 loss is: 2818.0078, the d2 loss is: -3133.8984, the g loss is: 3119.7656, the ae loss is: 0.0053929314, the jacobian loss is:0.14869069\n",
            "This is the iter 6078, the d1 loss is: 2607.0312, the d2 loss is: -2880.1562, the g loss is: 2891.1797, the ae loss is: 0.0060065985, the jacobian loss is:0.12916411\n",
            "This is the iter 6079, the d1 loss is: 2721.1562, the d2 loss is: -3028.1875, the g loss is: 3005.25, the ae loss is: 0.0069727865, the jacobian loss is:0.16558346\n",
            "This is the iter 6080, the d1 loss is: 2656.75, the d2 loss is: -2949.375, the g loss is: 2898.1562, the ae loss is: 0.0072098607, the jacobian loss is:0.16906437\n",
            "This is the iter 6081, the d1 loss is: 2747.836, the d2 loss is: -3033.9766, the g loss is: 3038.2188, the ae loss is: 0.008323386, the jacobian loss is:0.16474068\n",
            "This is the iter 6082, the d1 loss is: 2738.0312, the d2 loss is: -3032.914, the g loss is: 3050.0156, the ae loss is: 0.0077679753, the jacobian loss is:0.13569178\n",
            "This is the iter 6083, the d1 loss is: 2737.539, the d2 loss is: -3003.0312, the g loss is: 3074.8125, the ae loss is: 0.0058514355, the jacobian loss is:0.12463609\n",
            "This is the iter 6084, the d1 loss is: 2701.3125, the d2 loss is: -2982.7266, the g loss is: 3051.4766, the ae loss is: 0.00548088, the jacobian loss is:0.09792759\n",
            "This is the iter 6085, the d1 loss is: 2878.9219, the d2 loss is: -3180.3438, the g loss is: 3148.8281, the ae loss is: 0.0060170665, the jacobian loss is:0.13176242\n",
            "This is the iter 6086, the d1 loss is: 2818.0078, the d2 loss is: -3139.0625, the g loss is: 3149.3281, the ae loss is: 0.0062723737, the jacobian loss is:0.1162929\n",
            "This is the iter 6087, the d1 loss is: 2750.2656, the d2 loss is: -3052.3438, the g loss is: 3052.6484, the ae loss is: 0.00500176, the jacobian loss is:0.15161614\n",
            "This is the iter 6088, the d1 loss is: 2665.0, the d2 loss is: -2922.9219, the g loss is: 2997.5469, the ae loss is: 0.0055572847, the jacobian loss is:0.18797965\n",
            "This is the iter 6089, the d1 loss is: 2782.5312, the d2 loss is: -3039.289, the g loss is: 3113.836, the ae loss is: 0.005809539, the jacobian loss is:0.1404409\n",
            "This is the iter 6090, the d1 loss is: 2874.9062, the d2 loss is: -3164.7188, the g loss is: 3202.0, the ae loss is: 0.005079318, the jacobian loss is:0.14818504\n",
            "This is the iter 6091, the d1 loss is: 2695.8203, the d2 loss is: -2978.9688, the g loss is: 3036.0312, the ae loss is: 0.005353242, the jacobian loss is:0.2353517\n",
            "This is the iter 6092, the d1 loss is: 2788.961, the d2 loss is: -3113.9922, the g loss is: 3031.7969, the ae loss is: 0.006102994, the jacobian loss is:0.10939876\n",
            "This is the iter 6093, the d1 loss is: 2774.2734, the d2 loss is: -3067.0156, the g loss is: 3049.3594, the ae loss is: 0.0057926625, the jacobian loss is:0.1626066\n",
            "This is the iter 6094, the d1 loss is: 2629.1328, the d2 loss is: -2940.461, the g loss is: 3019.086, the ae loss is: 0.009143841, the jacobian loss is:0.12498808\n",
            "This is the iter 6095, the d1 loss is: 2826.8281, the d2 loss is: -3104.336, the g loss is: 3089.9062, the ae loss is: 0.0063296445, the jacobian loss is:0.15626347\n",
            "This is the iter 6096, the d1 loss is: 2888.8906, the d2 loss is: -3152.6953, the g loss is: 3214.4922, the ae loss is: 0.009446787, the jacobian loss is:0.108382955\n",
            "This is the iter 6097, the d1 loss is: 2710.3125, the d2 loss is: -3039.9922, the g loss is: 3086.4062, the ae loss is: 0.006314097, the jacobian loss is:0.14043579\n",
            "This is the iter 6098, the d1 loss is: 2747.5703, the d2 loss is: -3058.7734, the g loss is: 3009.8516, the ae loss is: 0.006610926, the jacobian loss is:0.111831956\n",
            "This is the iter 6099, the d1 loss is: 2926.375, the d2 loss is: -3180.9062, the g loss is: 3211.5547, the ae loss is: 0.0056740665, the jacobian loss is:0.13429709\n",
            "This is the iter 6100, the d1 loss is: 2727.2656, the d2 loss is: -3019.8594, the g loss is: 2999.1953, the ae loss is: 0.0068696784, the jacobian loss is:0.19595608\n",
            "0.22610319\n",
            "0.86273086\n",
            "This is the iter 6101, the d1 loss is: 2782.625, the d2 loss is: -3078.0625, the g loss is: 3015.0312, the ae loss is: 0.0084372135, the jacobian loss is:0.158083\n",
            "This is the iter 6102, the d1 loss is: 2738.9922, the d2 loss is: -3036.9453, the g loss is: 3000.1719, the ae loss is: 0.0073881266, the jacobian loss is:0.17705423\n",
            "This is the iter 6103, the d1 loss is: 2722.3203, the d2 loss is: -3012.3438, the g loss is: 3027.461, the ae loss is: 0.00782036, the jacobian loss is:0.12870465\n",
            "This is the iter 6104, the d1 loss is: 2757.9922, the d2 loss is: -3061.3281, the g loss is: 3054.1406, the ae loss is: 0.0073917145, the jacobian loss is:0.106956124\n",
            "This is the iter 6105, the d1 loss is: 2747.875, the d2 loss is: -3029.6562, the g loss is: 3011.125, the ae loss is: 0.0059360573, the jacobian loss is:0.15723914\n",
            "This is the iter 6106, the d1 loss is: 2902.6094, the d2 loss is: -3158.4688, the g loss is: 3136.1328, the ae loss is: 0.010542208, the jacobian loss is:0.18711187\n",
            "This is the iter 6107, the d1 loss is: 2875.0938, the d2 loss is: -3147.6562, the g loss is: 3098.7734, the ae loss is: 0.007782206, the jacobian loss is:0.16223244\n",
            "This is the iter 6108, the d1 loss is: 2654.8828, the d2 loss is: -2936.336, the g loss is: 3001.5469, the ae loss is: 0.009323926, the jacobian loss is:0.17257085\n",
            "This is the iter 6109, the d1 loss is: 2765.8516, the d2 loss is: -3082.875, the g loss is: 3055.8438, the ae loss is: 0.0067163603, the jacobian loss is:0.16647045\n",
            "This is the iter 6110, the d1 loss is: 2775.664, the d2 loss is: -3067.164, the g loss is: 3032.1797, the ae loss is: 0.005757274, the jacobian loss is:0.08802577\n",
            "This is the iter 6111, the d1 loss is: 2771.2344, the d2 loss is: -3040.7578, the g loss is: 3045.6953, the ae loss is: 0.007891525, the jacobian loss is:0.23966737\n",
            "This is the iter 6112, the d1 loss is: 2782.3672, the d2 loss is: -3128.375, the g loss is: 3078.914, the ae loss is: 0.0082969675, the jacobian loss is:0.1565609\n",
            "This is the iter 6113, the d1 loss is: 2824.8047, the d2 loss is: -3134.9766, the g loss is: 3140.1562, the ae loss is: 0.0094278, the jacobian loss is:0.13772763\n",
            "This is the iter 6114, the d1 loss is: 2987.3828, the d2 loss is: -3308.8281, the g loss is: 3334.461, the ae loss is: 0.005616101, the jacobian loss is:0.107462734\n",
            "This is the iter 6115, the d1 loss is: 2708.7188, the d2 loss is: -2983.8047, the g loss is: 3004.6562, the ae loss is: 0.009702062, the jacobian loss is:0.13086241\n",
            "This is the iter 6116, the d1 loss is: 2755.2266, the d2 loss is: -3066.8047, the g loss is: 3087.0, the ae loss is: 0.008308105, the jacobian loss is:0.14977165\n",
            "This is the iter 6117, the d1 loss is: 2735.3438, the d2 loss is: -3035.5703, the g loss is: 3062.0469, the ae loss is: 0.006275943, the jacobian loss is:0.12005798\n",
            "This is the iter 6118, the d1 loss is: 3212.0156, the d2 loss is: -3491.8516, the g loss is: 3471.7031, the ae loss is: 0.006326374, the jacobian loss is:0.15941538\n",
            "This is the iter 6119, the d1 loss is: 2726.9531, the d2 loss is: -3025.6719, the g loss is: 3017.6016, the ae loss is: 0.0066053034, the jacobian loss is:0.2170266\n",
            "This is the iter 6120, the d1 loss is: 2752.7344, the d2 loss is: -3046.5781, the g loss is: 3065.1172, the ae loss is: 0.0074809208, the jacobian loss is:0.11590652\n",
            "This is the iter 6121, the d1 loss is: 2809.5625, the d2 loss is: -3117.2344, the g loss is: 3122.086, the ae loss is: 0.0066405805, the jacobian loss is:0.13924411\n",
            "This is the iter 6122, the d1 loss is: 2703.4219, the d2 loss is: -3028.8516, the g loss is: 3007.4453, the ae loss is: 0.008187136, the jacobian loss is:0.10870559\n",
            "This is the iter 6123, the d1 loss is: 2758.7656, the d2 loss is: -3091.9219, the g loss is: 3054.9297, the ae loss is: 0.0058751376, the jacobian loss is:0.1525603\n",
            "This is the iter 6124, the d1 loss is: 2617.914, the d2 loss is: -2917.4688, the g loss is: 2896.6797, the ae loss is: 0.0052709, the jacobian loss is:0.11153498\n",
            "This is the iter 6125, the d1 loss is: 2510.086, the d2 loss is: -2829.5234, the g loss is: 2869.625, the ae loss is: 0.005484679, the jacobian loss is:0.13552487\n",
            "This is the iter 6126, the d1 loss is: 2754.4766, the d2 loss is: -3058.3594, the g loss is: 3056.4531, the ae loss is: 0.0064949933, the jacobian loss is:0.16050975\n",
            "This is the iter 6127, the d1 loss is: 2775.7344, the d2 loss is: -3045.7812, the g loss is: 3102.6562, the ae loss is: 0.0066894433, the jacobian loss is:0.14326379\n",
            "This is the iter 6128, the d1 loss is: 2820.3828, the d2 loss is: -3115.789, the g loss is: 3136.3516, the ae loss is: 0.007179357, the jacobian loss is:0.16477416\n",
            "This is the iter 6129, the d1 loss is: 2822.2734, the d2 loss is: -3124.2734, the g loss is: 3143.0312, the ae loss is: 0.008234176, the jacobian loss is:0.13788697\n",
            "This is the iter 6130, the d1 loss is: 2721.7656, the d2 loss is: -3016.75, the g loss is: 3013.789, the ae loss is: 0.0068178317, the jacobian loss is:0.108597964\n",
            "This is the iter 6131, the d1 loss is: 2695.5156, the d2 loss is: -2986.9922, the g loss is: 3043.25, the ae loss is: 0.0076060733, the jacobian loss is:0.20757867\n",
            "This is the iter 6132, the d1 loss is: 2839.2188, the d2 loss is: -3131.3594, the g loss is: 3050.0078, the ae loss is: 0.0063173333, the jacobian loss is:0.15251179\n",
            "This is the iter 6133, the d1 loss is: 2788.0938, the d2 loss is: -3085.4844, the g loss is: 3119.9844, the ae loss is: 0.0069196527, the jacobian loss is:0.13874978\n",
            "This is the iter 6134, the d1 loss is: 2742.6406, the d2 loss is: -3034.4297, the g loss is: 3070.1797, the ae loss is: 0.0060896445, the jacobian loss is:0.158981\n",
            "This is the iter 6135, the d1 loss is: 2796.4688, the d2 loss is: -3100.1719, the g loss is: 3108.9766, the ae loss is: 0.0076684724, the jacobian loss is:0.14052485\n",
            "This is the iter 6136, the d1 loss is: 2798.8984, the d2 loss is: -3066.5469, the g loss is: 3077.9219, the ae loss is: 0.007963788, the jacobian loss is:0.16169663\n",
            "This is the iter 6137, the d1 loss is: 2807.3203, the d2 loss is: -3080.0469, the g loss is: 3111.9062, the ae loss is: 0.0070091207, the jacobian loss is:0.1279463\n",
            "This is the iter 6138, the d1 loss is: 2792.625, the d2 loss is: -3090.6094, the g loss is: 3121.3125, the ae loss is: 0.007515858, the jacobian loss is:0.1276521\n",
            "This is the iter 6139, the d1 loss is: 2834.3047, the d2 loss is: -3149.9062, the g loss is: 3137.25, the ae loss is: 0.0055209408, the jacobian loss is:0.14344475\n",
            "This is the iter 6140, the d1 loss is: 3167.3281, the d2 loss is: -3477.7969, the g loss is: 3463.5312, the ae loss is: 0.005409807, the jacobian loss is:0.13173433\n",
            "This is the iter 6141, the d1 loss is: 2806.8828, the d2 loss is: -3100.2578, the g loss is: 3133.7344, the ae loss is: 0.008512194, the jacobian loss is:0.17690025\n",
            "This is the iter 6142, the d1 loss is: 3055.1484, the d2 loss is: -3360.9062, the g loss is: 3323.6562, the ae loss is: 0.008506576, the jacobian loss is:0.20974644\n",
            "This is the iter 6143, the d1 loss is: 2887.8438, the d2 loss is: -3159.6797, the g loss is: 3181.8047, the ae loss is: 0.007895286, the jacobian loss is:0.14867605\n",
            "This is the iter 6144, the d1 loss is: 2852.5938, the d2 loss is: -3147.7734, the g loss is: 3150.3438, the ae loss is: 0.006265086, the jacobian loss is:0.18805611\n",
            "This is the iter 6145, the d1 loss is: 2863.125, the d2 loss is: -3183.0234, the g loss is: 3146.2344, the ae loss is: 0.008102156, the jacobian loss is:0.14188698\n",
            "This is the iter 6146, the d1 loss is: 2824.211, the d2 loss is: -3121.039, the g loss is: 3111.6953, the ae loss is: 0.0074198237, the jacobian loss is:0.17441815\n",
            "This is the iter 6147, the d1 loss is: 2846.7969, the d2 loss is: -3162.086, the g loss is: 3127.3438, the ae loss is: 0.00515101, the jacobian loss is:0.17294794\n",
            "This is the iter 6148, the d1 loss is: 2751.25, the d2 loss is: -3065.2031, the g loss is: 3096.4219, the ae loss is: 0.0077343257, the jacobian loss is:0.21368997\n",
            "This is the iter 6149, the d1 loss is: 2793.0312, the d2 loss is: -3068.8203, the g loss is: 3036.3984, the ae loss is: 0.0065579256, the jacobian loss is:0.14205691\n",
            "This is the iter 6150, the d1 loss is: 2844.4375, the d2 loss is: -3154.3828, the g loss is: 3119.9453, the ae loss is: 0.0076047005, the jacobian loss is:0.13570559\n",
            "This is the iter 6151, the d1 loss is: 2861.8984, the d2 loss is: -3151.5781, the g loss is: 3189.4219, the ae loss is: 0.0049113473, the jacobian loss is:0.14439203\n",
            "This is the iter 6152, the d1 loss is: 2716.9375, the d2 loss is: -3012.2031, the g loss is: 3007.039, the ae loss is: 0.006026513, the jacobian loss is:0.14533463\n",
            "This is the iter 6153, the d1 loss is: 2800.9297, the d2 loss is: -3115.0312, the g loss is: 3084.9219, the ae loss is: 0.009962525, the jacobian loss is:0.13730483\n",
            "This is the iter 6154, the d1 loss is: 2897.0938, the d2 loss is: -3181.0781, the g loss is: 3148.1875, the ae loss is: 0.0048424876, the jacobian loss is:0.15457699\n",
            "This is the iter 6155, the d1 loss is: 2777.414, the d2 loss is: -3068.9688, the g loss is: 3069.9531, the ae loss is: 0.0060461205, the jacobian loss is:0.1767291\n",
            "This is the iter 6156, the d1 loss is: 2853.25, the d2 loss is: -3151.4453, the g loss is: 3179.6562, the ae loss is: 0.008534428, the jacobian loss is:0.15173289\n",
            "This is the iter 6157, the d1 loss is: 2819.0625, the d2 loss is: -3089.3281, the g loss is: 3104.0, the ae loss is: 0.0066357264, the jacobian loss is:0.123602234\n",
            "This is the iter 6158, the d1 loss is: 2811.8438, the d2 loss is: -3124.1406, the g loss is: 3098.7344, the ae loss is: 0.0054479926, the jacobian loss is:0.14063674\n",
            "This is the iter 6159, the d1 loss is: 2888.7812, the d2 loss is: -3141.2344, the g loss is: 3161.0312, the ae loss is: 0.007505766, the jacobian loss is:0.15021709\n",
            "This is the iter 6160, the d1 loss is: 2863.2812, the d2 loss is: -3145.6328, the g loss is: 3107.4375, the ae loss is: 0.004934362, the jacobian loss is:0.111914515\n",
            "This is the iter 6161, the d1 loss is: 2830.9688, the d2 loss is: -3145.9219, the g loss is: 3138.5156, the ae loss is: 0.006116361, the jacobian loss is:0.11765176\n",
            "This is the iter 6162, the d1 loss is: 2787.8594, the d2 loss is: -3074.1016, the g loss is: 3054.5312, the ae loss is: 0.009960266, the jacobian loss is:0.14757583\n",
            "This is the iter 6163, the d1 loss is: 2800.8828, the d2 loss is: -3073.3672, the g loss is: 3061.4297, the ae loss is: 0.0050319834, the jacobian loss is:0.10584039\n",
            "This is the iter 6164, the d1 loss is: 2856.125, the d2 loss is: -3107.1094, the g loss is: 3154.4922, the ae loss is: 0.0077175926, the jacobian loss is:0.17905174\n",
            "This is the iter 6165, the d1 loss is: 2756.8047, the d2 loss is: -3064.6406, the g loss is: 3071.1484, the ae loss is: 0.005599251, the jacobian loss is:0.1354982\n",
            "This is the iter 6166, the d1 loss is: 2831.2344, the d2 loss is: -3131.0234, the g loss is: 3132.1875, the ae loss is: 0.0063921837, the jacobian loss is:0.11789\n",
            "This is the iter 6167, the d1 loss is: 2846.414, the d2 loss is: -3102.1562, the g loss is: 3168.5078, the ae loss is: 0.0072504277, the jacobian loss is:0.10094608\n",
            "This is the iter 6168, the d1 loss is: 2853.8047, the d2 loss is: -3129.2344, the g loss is: 3149.7578, the ae loss is: 0.005844024, the jacobian loss is:0.12302924\n",
            "This is the iter 6169, the d1 loss is: 3040.625, the d2 loss is: -3356.7734, the g loss is: 3423.6016, the ae loss is: 0.008793554, the jacobian loss is:0.17426416\n",
            "This is the iter 6170, the d1 loss is: 2619.914, the d2 loss is: -2915.3984, the g loss is: 2886.8906, the ae loss is: 0.0101687675, the jacobian loss is:0.12889719\n",
            "This is the iter 6171, the d1 loss is: 2846.711, the d2 loss is: -3116.7969, the g loss is: 3149.6875, the ae loss is: 0.0076341047, the jacobian loss is:0.17240164\n",
            "This is the iter 6172, the d1 loss is: 2789.0625, the d2 loss is: -3085.9688, the g loss is: 3099.6875, the ae loss is: 0.00834763, the jacobian loss is:0.13177872\n",
            "This is the iter 6173, the d1 loss is: 2730.5312, the d2 loss is: -2995.5625, the g loss is: 2996.5703, the ae loss is: 0.005946505, the jacobian loss is:0.13943803\n",
            "This is the iter 6174, the d1 loss is: 2858.1172, the d2 loss is: -3145.0781, the g loss is: 3135.25, the ae loss is: 0.0064063948, the jacobian loss is:0.15826015\n",
            "This is the iter 6175, the d1 loss is: 2881.211, the d2 loss is: -3155.9531, the g loss is: 3144.2266, the ae loss is: 0.008301022, the jacobian loss is:0.10651257\n",
            "This is the iter 6176, the d1 loss is: 2763.1328, the d2 loss is: -3054.211, the g loss is: 3121.4453, the ae loss is: 0.003528432, the jacobian loss is:0.077226385\n",
            "This is the iter 6177, the d1 loss is: 2743.4219, the d2 loss is: -3005.0938, the g loss is: 3095.6953, the ae loss is: 0.0046203607, the jacobian loss is:0.110781975\n",
            "This is the iter 6178, the d1 loss is: 2873.6406, the d2 loss is: -3169.1719, the g loss is: 3127.4531, the ae loss is: 0.0063741067, the jacobian loss is:0.16858132\n",
            "This is the iter 6179, the d1 loss is: 2719.539, the d2 loss is: -3044.5781, the g loss is: 3082.3047, the ae loss is: 0.0071147946, the jacobian loss is:0.16065584\n",
            "This is the iter 6180, the d1 loss is: 2846.6016, the d2 loss is: -3121.2656, the g loss is: 3071.0781, the ae loss is: 0.0058084168, the jacobian loss is:0.12882161\n",
            "This is the iter 6181, the d1 loss is: 3041.9297, the d2 loss is: -3337.5547, the g loss is: 3381.8047, the ae loss is: 0.006001017, the jacobian loss is:0.13806735\n",
            "This is the iter 6182, the d1 loss is: 2872.6875, the d2 loss is: -3157.8047, the g loss is: 3219.1094, the ae loss is: 0.006336282, the jacobian loss is:0.17773362\n",
            "This is the iter 6183, the d1 loss is: 2810.5938, the d2 loss is: -3098.4922, the g loss is: 3118.9766, the ae loss is: 0.007958095, the jacobian loss is:0.12638722\n",
            "This is the iter 6184, the d1 loss is: 2820.5, the d2 loss is: -3116.7031, the g loss is: 3086.5, the ae loss is: 0.006900595, the jacobian loss is:0.09873039\n",
            "This is the iter 6185, the d1 loss is: 2792.8984, the d2 loss is: -3106.9375, the g loss is: 3114.3125, the ae loss is: 0.008046268, the jacobian loss is:0.16104983\n",
            "This is the iter 6186, the d1 loss is: 2950.5078, the d2 loss is: -3229.5781, the g loss is: 3254.4922, the ae loss is: 0.0073149726, the jacobian loss is:0.1066446\n",
            "This is the iter 6187, the d1 loss is: 2836.8516, the d2 loss is: -3118.3281, the g loss is: 3152.1172, the ae loss is: 0.009304038, the jacobian loss is:0.14158928\n",
            "This is the iter 6188, the d1 loss is: 2816.3125, the d2 loss is: -3076.1016, the g loss is: 3122.5, the ae loss is: 0.0057987403, the jacobian loss is:0.1315389\n",
            "This is the iter 6189, the d1 loss is: 2862.7344, the d2 loss is: -3160.2656, the g loss is: 3156.8594, the ae loss is: 0.0070078066, the jacobian loss is:0.092456564\n",
            "This is the iter 6190, the d1 loss is: 2851.2188, the d2 loss is: -3138.6094, the g loss is: 3127.0469, the ae loss is: 0.0058738003, the jacobian loss is:0.2349206\n",
            "This is the iter 6191, the d1 loss is: 2808.1328, the d2 loss is: -3133.7344, the g loss is: 3121.3516, the ae loss is: 0.0057690814, the jacobian loss is:0.15228724\n",
            "This is the iter 6192, the d1 loss is: 2882.9922, the d2 loss is: -3183.4297, the g loss is: 3167.8125, the ae loss is: 0.006734673, the jacobian loss is:0.280049\n",
            "This is the iter 6193, the d1 loss is: 2861.7188, the d2 loss is: -3166.6484, the g loss is: 3197.1562, the ae loss is: 0.006025154, the jacobian loss is:0.13075404\n",
            "This is the iter 6194, the d1 loss is: 2832.4688, the d2 loss is: -3153.211, the g loss is: 3126.1484, the ae loss is: 0.008957798, the jacobian loss is:0.122436084\n",
            "This is the iter 6195, the d1 loss is: 2908.1328, the d2 loss is: -3187.5469, the g loss is: 3096.2031, the ae loss is: 0.007332716, the jacobian loss is:0.16910776\n",
            "This is the iter 6196, the d1 loss is: 2835.7188, the d2 loss is: -3113.6719, the g loss is: 3150.5703, the ae loss is: 0.005124614, the jacobian loss is:0.10896667\n",
            "This is the iter 6197, the d1 loss is: 2855.2656, the d2 loss is: -3159.2734, the g loss is: 3164.3516, the ae loss is: 0.0063516824, the jacobian loss is:0.10800342\n",
            "This is the iter 6198, the d1 loss is: 2848.2969, the d2 loss is: -3099.9766, the g loss is: 3055.4688, the ae loss is: 0.006446586, the jacobian loss is:0.1270873\n",
            "This is the iter 6199, the d1 loss is: 2800.6406, the d2 loss is: -3105.8516, the g loss is: 3088.2344, the ae loss is: 0.0051405225, the jacobian loss is:0.13914737\n",
            "This is the iter 6200, the d1 loss is: 2751.3438, the d2 loss is: -3035.7344, the g loss is: 2935.0, the ae loss is: 0.007672931, the jacobian loss is:0.1796171\n",
            "0.21258259\n",
            "0.8030916\n",
            "This is the iter 6201, the d1 loss is: 2572.164, the d2 loss is: -2871.0625, the g loss is: 2925.6094, the ae loss is: 0.006948917, the jacobian loss is:0.12653206\n",
            "This is the iter 6202, the d1 loss is: 2939.6797, the d2 loss is: -3252.375, the g loss is: 3193.1328, the ae loss is: 0.006005233, the jacobian loss is:0.08646205\n",
            "This is the iter 6203, the d1 loss is: 2822.5469, the d2 loss is: -3129.8516, the g loss is: 3098.2422, the ae loss is: 0.007841651, the jacobian loss is:0.28337988\n",
            "This is the iter 6204, the d1 loss is: 2757.2578, the d2 loss is: -3067.6875, the g loss is: 3044.7422, the ae loss is: 0.008058498, the jacobian loss is:0.27420905\n",
            "This is the iter 6205, the d1 loss is: 2807.9688, the d2 loss is: -3114.0781, the g loss is: 3086.5938, the ae loss is: 0.006076896, the jacobian loss is:0.08868562\n",
            "This is the iter 6206, the d1 loss is: 2859.8281, the d2 loss is: -3124.8125, the g loss is: 3108.1562, the ae loss is: 0.00739481, the jacobian loss is:0.13463554\n",
            "This is the iter 6207, the d1 loss is: 2787.8203, the d2 loss is: -3090.4844, the g loss is: 3071.8906, the ae loss is: 0.0073316917, the jacobian loss is:0.12460048\n",
            "This is the iter 6208, the d1 loss is: 2820.2266, the d2 loss is: -3078.414, the g loss is: 3098.0312, the ae loss is: 0.0066280756, the jacobian loss is:0.11917573\n",
            "This is the iter 6209, the d1 loss is: 2767.0, the d2 loss is: -3055.461, the g loss is: 3052.2734, the ae loss is: 0.0071059885, the jacobian loss is:0.1291338\n",
            "This is the iter 6210, the d1 loss is: 2749.9062, the d2 loss is: -3044.8203, the g loss is: 3059.9844, the ae loss is: 0.0050380775, the jacobian loss is:0.13701923\n",
            "This is the iter 6211, the d1 loss is: 2844.4531, the d2 loss is: -3125.2734, the g loss is: 3143.5234, the ae loss is: 0.0059283683, the jacobian loss is:0.105652176\n",
            "This is the iter 6212, the d1 loss is: 2785.3594, the d2 loss is: -3103.3672, the g loss is: 3089.7734, the ae loss is: 0.0061687105, the jacobian loss is:0.16660602\n",
            "This is the iter 6213, the d1 loss is: 2776.125, the d2 loss is: -3060.9219, the g loss is: 3052.5312, the ae loss is: 0.0066236523, the jacobian loss is:0.1257475\n",
            "This is the iter 6214, the d1 loss is: 2856.6875, the d2 loss is: -3159.8516, the g loss is: 3142.6484, the ae loss is: 0.007311172, the jacobian loss is:0.12339991\n",
            "This is the iter 6215, the d1 loss is: 2801.5781, the d2 loss is: -3111.5234, the g loss is: 3082.8438, the ae loss is: 0.0060976576, the jacobian loss is:0.1456329\n",
            "This is the iter 6216, the d1 loss is: 2860.086, the d2 loss is: -3153.4375, the g loss is: 3132.6094, the ae loss is: 0.0054915417, the jacobian loss is:0.15539832\n",
            "This is the iter 6217, the d1 loss is: 2818.1406, the d2 loss is: -3129.6797, the g loss is: 3094.9688, the ae loss is: 0.010279655, the jacobian loss is:0.13544357\n",
            "This is the iter 6218, the d1 loss is: 2839.7656, the d2 loss is: -3096.0078, the g loss is: 3073.9062, the ae loss is: 0.0058325194, the jacobian loss is:0.14267927\n",
            "This is the iter 6219, the d1 loss is: 3060.3203, the d2 loss is: -3392.8281, the g loss is: 3356.961, the ae loss is: 0.0072193258, the jacobian loss is:0.14584057\n",
            "This is the iter 6220, the d1 loss is: 2834.1172, the d2 loss is: -3130.3984, the g loss is: 3149.0312, the ae loss is: 0.005997799, the jacobian loss is:0.10287981\n",
            "This is the iter 6221, the d1 loss is: 2862.9766, the d2 loss is: -3168.461, the g loss is: 3186.9297, the ae loss is: 0.0064084614, the jacobian loss is:0.12540677\n",
            "This is the iter 6222, the d1 loss is: 2855.625, the d2 loss is: -3129.6953, the g loss is: 3085.0312, the ae loss is: 0.004944997, the jacobian loss is:0.10902189\n",
            "This is the iter 6223, the d1 loss is: 2818.039, the d2 loss is: -3136.039, the g loss is: 3148.8516, the ae loss is: 0.006000829, the jacobian loss is:0.12394216\n",
            "This is the iter 6224, the d1 loss is: 2963.0703, the d2 loss is: -3276.7031, the g loss is: 3306.625, the ae loss is: 0.0059017977, the jacobian loss is:0.1858\n",
            "This is the iter 6225, the d1 loss is: 3016.4531, the d2 loss is: -3317.0938, the g loss is: 3339.4531, the ae loss is: 0.006748524, the jacobian loss is:0.19922386\n",
            "This is the iter 6226, the d1 loss is: 2886.5078, the d2 loss is: -3186.3594, the g loss is: 3198.1719, the ae loss is: 0.0076520606, the jacobian loss is:0.1178584\n",
            "This is the iter 6227, the d1 loss is: 2881.625, the d2 loss is: -3179.25, the g loss is: 3193.3828, the ae loss is: 0.0054613086, the jacobian loss is:0.12337286\n",
            "This is the iter 6228, the d1 loss is: 2899.9219, the d2 loss is: -3189.5547, the g loss is: 3139.6719, the ae loss is: 0.0072900346, the jacobian loss is:0.14712541\n",
            "This is the iter 6229, the d1 loss is: 2824.4766, the d2 loss is: -3124.0, the g loss is: 3185.6719, the ae loss is: 0.007383831, the jacobian loss is:0.20193575\n",
            "This is the iter 6230, the d1 loss is: 2902.5625, the d2 loss is: -3217.0, the g loss is: 3184.75, the ae loss is: 0.0051669395, the jacobian loss is:0.09435178\n",
            "This is the iter 6231, the d1 loss is: 3029.2812, the d2 loss is: -3295.8672, the g loss is: 3338.9844, the ae loss is: 0.006775528, the jacobian loss is:0.09805356\n",
            "This is the iter 6232, the d1 loss is: 2679.086, the d2 loss is: -2977.0312, the g loss is: 2930.4297, the ae loss is: 0.0057794177, the jacobian loss is:0.07758073\n",
            "This is the iter 6233, the d1 loss is: 2787.039, the d2 loss is: -3076.836, the g loss is: 3033.4844, the ae loss is: 0.0061813043, the jacobian loss is:0.12925288\n",
            "This is the iter 6234, the d1 loss is: 2821.9062, the d2 loss is: -3127.1562, the g loss is: 3133.3047, the ae loss is: 0.006487155, the jacobian loss is:0.15210883\n",
            "This is the iter 6235, the d1 loss is: 2679.3906, the d2 loss is: -2984.9688, the g loss is: 3012.1875, the ae loss is: 0.006531655, the jacobian loss is:0.0883897\n",
            "This is the iter 6236, the d1 loss is: 2835.6797, the d2 loss is: -3165.7188, the g loss is: 3188.4219, the ae loss is: 0.006058858, the jacobian loss is:0.10026729\n",
            "This is the iter 6237, the d1 loss is: 2692.2344, the d2 loss is: -2995.2188, the g loss is: 2962.9688, the ae loss is: 0.007891597, the jacobian loss is:0.14388098\n",
            "This is the iter 6238, the d1 loss is: 2785.125, the d2 loss is: -3044.1875, the g loss is: 3065.9531, the ae loss is: 0.00698793, the jacobian loss is:0.09775773\n",
            "This is the iter 6239, the d1 loss is: 2867.0312, the d2 loss is: -3154.375, the g loss is: 3146.2734, the ae loss is: 0.0072490796, the jacobian loss is:0.11717982\n",
            "This is the iter 6240, the d1 loss is: 2846.5469, the d2 loss is: -3135.9453, the g loss is: 3137.0703, the ae loss is: 0.0069455113, the jacobian loss is:0.10450551\n",
            "This is the iter 6241, the d1 loss is: 2765.2344, the d2 loss is: -3089.1562, the g loss is: 3115.9062, the ae loss is: 0.005306323, the jacobian loss is:0.14385045\n",
            "This is the iter 6242, the d1 loss is: 2848.9766, the d2 loss is: -3122.5703, the g loss is: 3118.7969, the ae loss is: 0.007892044, the jacobian loss is:0.09992275\n",
            "This is the iter 6243, the d1 loss is: 2853.8125, the d2 loss is: -3121.75, the g loss is: 3148.3047, the ae loss is: 0.006752369, the jacobian loss is:0.10443007\n",
            "This is the iter 6244, the d1 loss is: 2826.375, the d2 loss is: -3162.4453, the g loss is: 3153.375, the ae loss is: 0.0060971505, the jacobian loss is:0.09350257\n",
            "This is the iter 6245, the d1 loss is: 2822.8984, the d2 loss is: -3106.3906, the g loss is: 3084.875, the ae loss is: 0.0046161427, the jacobian loss is:0.080856435\n",
            "This is the iter 6246, the d1 loss is: 2906.9453, the d2 loss is: -3177.0781, the g loss is: 3176.6328, the ae loss is: 0.0064776298, the jacobian loss is:0.12385071\n",
            "This is the iter 6247, the d1 loss is: 2877.25, the d2 loss is: -3139.9531, the g loss is: 3101.8203, the ae loss is: 0.009019603, the jacobian loss is:0.13337988\n",
            "This is the iter 6248, the d1 loss is: 2842.9766, the d2 loss is: -3143.4062, the g loss is: 3087.039, the ae loss is: 0.007884336, the jacobian loss is:0.09224072\n",
            "This is the iter 6249, the d1 loss is: 2694.8594, the d2 loss is: -2986.039, the g loss is: 2991.2266, the ae loss is: 0.0053174756, the jacobian loss is:0.12489598\n",
            "This is the iter 6250, the d1 loss is: 2837.211, the d2 loss is: -3127.5625, the g loss is: 3120.914, the ae loss is: 0.0070946077, the jacobian loss is:0.08382698\n",
            "This is the iter 6251, the d1 loss is: 2778.539, the d2 loss is: -3057.7031, the g loss is: 3085.164, the ae loss is: 0.006175096, the jacobian loss is:0.11165196\n",
            "This is the iter 6252, the d1 loss is: 2746.8906, the d2 loss is: -2997.9922, the g loss is: 3051.2422, the ae loss is: 0.0070665777, the jacobian loss is:0.11604421\n",
            "This is the iter 6253, the d1 loss is: 2767.0156, the d2 loss is: -3077.8984, the g loss is: 3086.664, the ae loss is: 0.0069731064, the jacobian loss is:0.17271766\n",
            "This is the iter 6254, the d1 loss is: 2807.6953, the d2 loss is: -3100.7656, the g loss is: 3049.2344, the ae loss is: 0.007608011, the jacobian loss is:0.11985311\n",
            "This is the iter 6255, the d1 loss is: 2846.1719, the d2 loss is: -3133.9375, the g loss is: 3138.4375, the ae loss is: 0.009673645, the jacobian loss is:0.14930333\n",
            "This is the iter 6256, the d1 loss is: 2676.9922, the d2 loss is: -3009.3047, the g loss is: 3028.6328, the ae loss is: 0.008126684, the jacobian loss is:0.14971183\n",
            "This is the iter 6257, the d1 loss is: 2728.461, the d2 loss is: -3005.5, the g loss is: 3136.4531, the ae loss is: 0.0062124757, the jacobian loss is:0.124748625\n",
            "This is the iter 6258, the d1 loss is: 2860.8828, the d2 loss is: -3144.9531, the g loss is: 3121.7969, the ae loss is: 0.003992123, the jacobian loss is:0.12229377\n",
            "This is the iter 6259, the d1 loss is: 2821.1484, the d2 loss is: -3106.8828, the g loss is: 3080.8672, the ae loss is: 0.0073116873, the jacobian loss is:0.107841834\n",
            "This is the iter 6260, the d1 loss is: 2804.2188, the d2 loss is: -3128.4219, the g loss is: 3094.0781, the ae loss is: 0.006617259, the jacobian loss is:0.10621283\n",
            "This is the iter 6261, the d1 loss is: 2765.7656, the d2 loss is: -3073.1094, the g loss is: 3060.8047, the ae loss is: 0.007829985, the jacobian loss is:0.10234194\n",
            "This is the iter 6262, the d1 loss is: 2820.9688, the d2 loss is: -3089.3516, the g loss is: 3100.25, the ae loss is: 0.0070098853, the jacobian loss is:0.15549624\n",
            "This is the iter 6263, the d1 loss is: 2893.9844, the d2 loss is: -3170.3906, the g loss is: 3125.3281, the ae loss is: 0.0067871227, the jacobian loss is:0.11111765\n",
            "This is the iter 6264, the d1 loss is: 2746.5469, the d2 loss is: -3091.5781, the g loss is: 3088.6484, the ae loss is: 0.006993951, the jacobian loss is:0.11322696\n",
            "This is the iter 6265, the d1 loss is: 2529.6328, the d2 loss is: -2842.1094, the g loss is: 2934.375, the ae loss is: 0.006025614, the jacobian loss is:0.13745983\n",
            "This is the iter 6266, the d1 loss is: 2770.1484, the d2 loss is: -3040.4844, the g loss is: 3066.1094, the ae loss is: 0.009656288, the jacobian loss is:0.14172453\n",
            "This is the iter 6267, the d1 loss is: 2848.836, the d2 loss is: -3145.4062, the g loss is: 3179.7969, the ae loss is: 0.0067311805, the jacobian loss is:0.149698\n",
            "This is the iter 6268, the d1 loss is: 2815.0469, the d2 loss is: -3122.5078, the g loss is: 3093.1953, the ae loss is: 0.0070229676, the jacobian loss is:0.17922682\n",
            "This is the iter 6269, the d1 loss is: 3047.625, the d2 loss is: -3300.5938, the g loss is: 3234.5625, the ae loss is: 0.005955468, the jacobian loss is:0.2234267\n",
            "This is the iter 6270, the d1 loss is: 3073.9297, the d2 loss is: -3342.7266, the g loss is: 3376.6875, the ae loss is: 0.005320384, the jacobian loss is:0.11418915\n",
            "This is the iter 6271, the d1 loss is: 2933.664, the d2 loss is: -3196.7422, the g loss is: 3178.3047, the ae loss is: 0.004643115, the jacobian loss is:0.14448206\n",
            "This is the iter 6272, the d1 loss is: 2895.1406, the d2 loss is: -3194.1094, the g loss is: 3147.3281, the ae loss is: 0.009796028, the jacobian loss is:0.17452997\n",
            "This is the iter 6273, the d1 loss is: 2752.2734, the d2 loss is: -3031.375, the g loss is: 3002.4375, the ae loss is: 0.00770909, the jacobian loss is:0.08331846\n",
            "This is the iter 6274, the d1 loss is: 2979.0781, the d2 loss is: -3298.1562, the g loss is: 3281.5703, the ae loss is: 0.0062041534, the jacobian loss is:0.097979076\n",
            "This is the iter 6275, the d1 loss is: 2651.3047, the d2 loss is: -2956.5156, the g loss is: 2950.9062, the ae loss is: 0.0061280206, the jacobian loss is:0.14876619\n",
            "This is the iter 6276, the d1 loss is: 2834.2812, the d2 loss is: -3120.6094, the g loss is: 3076.7969, the ae loss is: 0.006351223, the jacobian loss is:0.11511148\n",
            "This is the iter 6277, the d1 loss is: 2808.0703, the d2 loss is: -3104.3438, the g loss is: 3077.789, the ae loss is: 0.007479227, the jacobian loss is:0.14392711\n",
            "This is the iter 6278, the d1 loss is: 2882.7969, the d2 loss is: -3181.7969, the g loss is: 3218.3125, the ae loss is: 0.0071264966, the jacobian loss is:0.11292724\n",
            "This is the iter 6279, the d1 loss is: 2801.3281, the d2 loss is: -3110.2188, the g loss is: 3122.6484, the ae loss is: 0.005948213, the jacobian loss is:0.11832742\n",
            "This is the iter 6280, the d1 loss is: 2750.5, the d2 loss is: -3048.1484, the g loss is: 3091.4688, the ae loss is: 0.006759595, the jacobian loss is:0.11051933\n",
            "This is the iter 6281, the d1 loss is: 2761.039, the d2 loss is: -3074.6719, the g loss is: 3063.539, the ae loss is: 0.005948092, the jacobian loss is:0.12503766\n",
            "This is the iter 6282, the d1 loss is: 2832.2344, the d2 loss is: -3144.1016, the g loss is: 3136.914, the ae loss is: 0.004928901, the jacobian loss is:0.107194684\n",
            "This is the iter 6283, the d1 loss is: 2833.6797, the d2 loss is: -3111.5938, the g loss is: 3149.4062, the ae loss is: 0.0059101954, the jacobian loss is:0.10606234\n",
            "This is the iter 6284, the d1 loss is: 3030.1719, the d2 loss is: -3346.4453, the g loss is: 3383.5938, the ae loss is: 0.0066017313, the jacobian loss is:0.12252552\n",
            "This is the iter 6285, the d1 loss is: 2981.1484, the d2 loss is: -3250.6797, the g loss is: 3237.9844, the ae loss is: 0.0076090246, the jacobian loss is:0.140241\n",
            "This is the iter 6286, the d1 loss is: 2876.6484, the d2 loss is: -3166.9844, the g loss is: 3153.0312, the ae loss is: 0.0064667882, the jacobian loss is:0.16483772\n",
            "This is the iter 6287, the d1 loss is: 2928.3047, the d2 loss is: -3213.1797, the g loss is: 3145.4688, the ae loss is: 0.006193872, the jacobian loss is:0.09766268\n",
            "This is the iter 6288, the d1 loss is: 2924.1875, the d2 loss is: -3235.961, the g loss is: 3224.461, the ae loss is: 0.008245168, the jacobian loss is:0.15582837\n",
            "This is the iter 6289, the d1 loss is: 2649.1094, the d2 loss is: -2939.7578, the g loss is: 2916.9922, the ae loss is: 0.006777091, the jacobian loss is:0.09782116\n",
            "This is the iter 6290, the d1 loss is: 2867.1875, the d2 loss is: -3181.6719, the g loss is: 3145.2344, the ae loss is: 0.0059516146, the jacobian loss is:0.08644098\n",
            "This is the iter 6291, the d1 loss is: 2839.2812, the d2 loss is: -3129.3203, the g loss is: 3091.5, the ae loss is: 0.010257545, the jacobian loss is:0.1494713\n",
            "This is the iter 6292, the d1 loss is: 2806.2031, the d2 loss is: -3090.2734, the g loss is: 3081.8047, the ae loss is: 0.006577735, the jacobian loss is:0.13998283\n",
            "This is the iter 6293, the d1 loss is: 2654.2344, the d2 loss is: -2958.836, the g loss is: 2960.0078, the ae loss is: 0.0057759318, the jacobian loss is:0.11290343\n",
            "This is the iter 6294, the d1 loss is: 2813.6875, the d2 loss is: -3094.3125, the g loss is: 3070.7031, the ae loss is: 0.00549851, the jacobian loss is:0.12246667\n",
            "This is the iter 6295, the d1 loss is: 2869.5938, the d2 loss is: -3153.164, the g loss is: 3171.461, the ae loss is: 0.0061153425, the jacobian loss is:0.08833977\n",
            "This is the iter 6296, the d1 loss is: 2935.5703, the d2 loss is: -3208.5938, the g loss is: 3132.1406, the ae loss is: 0.006851052, the jacobian loss is:0.15414429\n",
            "This is the iter 6297, the d1 loss is: 3115.6016, the d2 loss is: -3414.2969, the g loss is: 3403.875, the ae loss is: 0.0058309278, the jacobian loss is:0.11714695\n",
            "This is the iter 6298, the d1 loss is: 2928.8516, the d2 loss is: -3230.289, the g loss is: 3183.3906, the ae loss is: 0.0046949447, the jacobian loss is:0.17716418\n",
            "This is the iter 6299, the d1 loss is: 3062.6484, the d2 loss is: -3378.5312, the g loss is: 3377.211, the ae loss is: 0.005388664, the jacobian loss is:0.11573392\n",
            "This is the iter 6300, the d1 loss is: 2938.3203, the d2 loss is: -3235.2969, the g loss is: 3203.3516, the ae loss is: 0.009081632, the jacobian loss is:0.09170249\n",
            "0.20677818\n",
            "0.77181065\n",
            "This is the iter 6301, the d1 loss is: 2949.2422, the d2 loss is: -3226.2656, the g loss is: 3205.3125, the ae loss is: 0.007416539, the jacobian loss is:0.10043117\n",
            "This is the iter 6302, the d1 loss is: 3098.211, the d2 loss is: -3340.9219, the g loss is: 3320.2344, the ae loss is: 0.006729936, the jacobian loss is:0.15526599\n",
            "This is the iter 6303, the d1 loss is: 2869.5234, the d2 loss is: -3164.5703, the g loss is: 3233.0312, the ae loss is: 0.005035226, the jacobian loss is:0.10258683\n",
            "This is the iter 6304, the d1 loss is: 2936.4688, the d2 loss is: -3187.2344, the g loss is: 3157.4688, the ae loss is: 0.008765967, the jacobian loss is:0.17012192\n",
            "This is the iter 6305, the d1 loss is: 2864.9297, the d2 loss is: -3180.1797, the g loss is: 3137.0547, the ae loss is: 0.0052994704, the jacobian loss is:0.123493694\n",
            "This is the iter 6306, the d1 loss is: 2847.3594, the d2 loss is: -3212.9219, the g loss is: 3157.1719, the ae loss is: 0.0064977407, the jacobian loss is:0.100712076\n",
            "This is the iter 6307, the d1 loss is: 2885.0781, the d2 loss is: -3171.6953, the g loss is: 3136.125, the ae loss is: 0.0043423874, the jacobian loss is:0.10574808\n",
            "This is the iter 6308, the d1 loss is: 2769.6562, the d2 loss is: -3062.3906, the g loss is: 3111.4297, the ae loss is: 0.006758379, the jacobian loss is:0.11873999\n",
            "This is the iter 6309, the d1 loss is: 2882.375, the d2 loss is: -3186.0, the g loss is: 3178.0469, the ae loss is: 0.004649633, the jacobian loss is:0.115530655\n",
            "This is the iter 6310, the d1 loss is: 2920.6875, the d2 loss is: -3205.6875, the g loss is: 3224.7266, the ae loss is: 0.0059484, the jacobian loss is:0.13522203\n",
            "This is the iter 6311, the d1 loss is: 2947.0078, the d2 loss is: -3250.8125, the g loss is: 3162.9062, the ae loss is: 0.00498461, the jacobian loss is:0.1372499\n",
            "This is the iter 6312, the d1 loss is: 2813.4922, the d2 loss is: -3124.1875, the g loss is: 3153.5547, the ae loss is: 0.006897612, the jacobian loss is:0.11092076\n",
            "This is the iter 6313, the d1 loss is: 2837.2656, the d2 loss is: -3091.6328, the g loss is: 3134.7344, the ae loss is: 0.0067411275, the jacobian loss is:0.1522135\n",
            "This is the iter 6314, the d1 loss is: 3135.0312, the d2 loss is: -3403.1562, the g loss is: 3423.0938, the ae loss is: 0.005592835, the jacobian loss is:0.098234534\n",
            "This is the iter 6315, the d1 loss is: 2850.3906, the d2 loss is: -3139.8281, the g loss is: 3162.0469, the ae loss is: 0.0054397583, the jacobian loss is:0.091554955\n",
            "This is the iter 6316, the d1 loss is: 2894.75, the d2 loss is: -3186.6562, the g loss is: 3156.4844, the ae loss is: 0.0072609065, the jacobian loss is:0.1396731\n",
            "This is the iter 6317, the d1 loss is: 2887.6875, the d2 loss is: -3199.539, the g loss is: 3208.2188, the ae loss is: 0.007110571, the jacobian loss is:0.11952051\n",
            "This is the iter 6318, the d1 loss is: 2653.5312, the d2 loss is: -2946.8594, the g loss is: 2924.3281, the ae loss is: 0.0066678715, the jacobian loss is:0.17373843\n",
            "This is the iter 6319, the d1 loss is: 2772.5469, the d2 loss is: -3057.5781, the g loss is: 3057.5469, the ae loss is: 0.00664186, the jacobian loss is:0.13624452\n",
            "This is the iter 6320, the d1 loss is: 2873.5469, the d2 loss is: -3150.961, the g loss is: 3129.9688, the ae loss is: 0.005474163, the jacobian loss is:0.18119074\n",
            "This is the iter 6321, the d1 loss is: 2851.8125, the d2 loss is: -3127.6016, the g loss is: 3131.9688, the ae loss is: 0.0069726654, the jacobian loss is:0.13448761\n",
            "This is the iter 6322, the d1 loss is: 2825.0547, the d2 loss is: -3095.3594, the g loss is: 3106.6797, the ae loss is: 0.00601157, the jacobian loss is:0.10264471\n",
            "This is the iter 6323, the d1 loss is: 2816.9844, the d2 loss is: -3088.3906, the g loss is: 3108.2188, the ae loss is: 0.005954573, the jacobian loss is:0.09544411\n",
            "This is the iter 6324, the d1 loss is: 2805.9922, the d2 loss is: -3140.8203, the g loss is: 3165.3125, the ae loss is: 0.004313845, the jacobian loss is:0.16249579\n",
            "This is the iter 6325, the d1 loss is: 2669.336, the d2 loss is: -2940.4219, the g loss is: 2968.4844, the ae loss is: 0.009719781, the jacobian loss is:0.10749105\n",
            "This is the iter 6326, the d1 loss is: 2765.7734, the d2 loss is: -3057.8516, the g loss is: 3069.9844, the ae loss is: 0.0051365164, the jacobian loss is:0.13095869\n",
            "This is the iter 6327, the d1 loss is: 2886.461, the d2 loss is: -3145.3594, the g loss is: 3137.8828, the ae loss is: 0.0066688256, the jacobian loss is:0.08135421\n",
            "This is the iter 6328, the d1 loss is: 3107.9922, the d2 loss is: -3371.4844, the g loss is: 3387.6406, the ae loss is: 0.0053830775, the jacobian loss is:0.108803056\n",
            "This is the iter 6329, the d1 loss is: 2924.9922, the d2 loss is: -3197.289, the g loss is: 3162.8672, the ae loss is: 0.0073680044, the jacobian loss is:0.10626666\n",
            "This is the iter 6330, the d1 loss is: 2847.7969, the d2 loss is: -3104.6172, the g loss is: 3083.0156, the ae loss is: 0.006871605, the jacobian loss is:0.1206747\n",
            "This is the iter 6331, the d1 loss is: 2807.4219, the d2 loss is: -3117.0547, the g loss is: 3116.1875, the ae loss is: 0.006585886, the jacobian loss is:0.095511116\n",
            "This is the iter 6332, the d1 loss is: 2766.5312, the d2 loss is: -3063.9766, the g loss is: 3130.6016, the ae loss is: 0.0044988566, the jacobian loss is:0.117085606\n",
            "This is the iter 6333, the d1 loss is: 2886.3047, the d2 loss is: -3181.5234, the g loss is: 3143.8438, the ae loss is: 0.0058565335, the jacobian loss is:0.10307685\n",
            "This is the iter 6334, the d1 loss is: 3072.1875, the d2 loss is: -3377.9062, the g loss is: 3404.414, the ae loss is: 0.006813507, the jacobian loss is:0.11628066\n",
            "This is the iter 6335, the d1 loss is: 2881.2344, the d2 loss is: -3189.1797, the g loss is: 3248.4688, the ae loss is: 0.005806161, the jacobian loss is:0.12684897\n",
            "This is the iter 6336, the d1 loss is: 2818.4766, the d2 loss is: -3143.086, the g loss is: 3147.0547, the ae loss is: 0.0067682834, the jacobian loss is:0.100029826\n",
            "This is the iter 6337, the d1 loss is: 2860.25, the d2 loss is: -3123.4219, the g loss is: 3110.4766, the ae loss is: 0.0059576416, the jacobian loss is:0.14787972\n",
            "This is the iter 6338, the d1 loss is: 2883.1719, the d2 loss is: -3184.2578, the g loss is: 3148.5156, the ae loss is: 0.0049177413, the jacobian loss is:0.09910379\n",
            "This is the iter 6339, the d1 loss is: 3145.125, the d2 loss is: -3429.3438, the g loss is: 3448.1875, the ae loss is: 0.008832745, the jacobian loss is:0.1247662\n",
            "This is the iter 6340, the d1 loss is: 2953.164, the d2 loss is: -3233.3438, the g loss is: 3228.7188, the ae loss is: 0.00650659, the jacobian loss is:0.13905528\n",
            "This is the iter 6341, the d1 loss is: 2907.2188, the d2 loss is: -3210.4062, the g loss is: 3186.1562, the ae loss is: 0.00810081, the jacobian loss is:0.14296745\n",
            "This is the iter 6342, the d1 loss is: 2865.625, the d2 loss is: -3151.8125, the g loss is: 3176.8281, the ae loss is: 0.0074847825, the jacobian loss is:0.13596709\n",
            "This is the iter 6343, the d1 loss is: 2937.3438, the d2 loss is: -3214.711, the g loss is: 3206.0078, the ae loss is: 0.0052117286, the jacobian loss is:0.12745927\n",
            "This is the iter 6344, the d1 loss is: 2807.5625, the d2 loss is: -3143.664, the g loss is: 3167.3672, the ae loss is: 0.007146499, the jacobian loss is:0.099680305\n",
            "This is the iter 6345, the d1 loss is: 2953.1094, the d2 loss is: -3230.1172, the g loss is: 3231.3984, the ae loss is: 0.004661093, the jacobian loss is:0.10168053\n",
            "This is the iter 6346, the d1 loss is: 2769.4219, the d2 loss is: -3093.7188, the g loss is: 3048.414, the ae loss is: 0.006323139, the jacobian loss is:0.14435098\n",
            "This is the iter 6347, the d1 loss is: 2794.1562, the d2 loss is: -3090.4062, the g loss is: 3100.5469, the ae loss is: 0.006125683, the jacobian loss is:0.12035008\n",
            "This is the iter 6348, the d1 loss is: 2910.914, the d2 loss is: -3191.3125, the g loss is: 3169.7656, the ae loss is: 0.007588314, the jacobian loss is:0.15303634\n",
            "This is the iter 6349, the d1 loss is: 2613.8203, the d2 loss is: -2897.3281, the g loss is: 2931.6094, the ae loss is: 0.0056749145, the jacobian loss is:0.09808453\n",
            "This is the iter 6350, the d1 loss is: 2718.4844, the d2 loss is: -3024.1328, the g loss is: 2994.6094, the ae loss is: 0.006135407, the jacobian loss is:0.15396984\n",
            "This is the iter 6351, the d1 loss is: 2786.5469, the d2 loss is: -3036.3203, the g loss is: 3040.0312, the ae loss is: 0.006013223, the jacobian loss is:0.11168314\n",
            "This is the iter 6352, the d1 loss is: 2656.4766, the d2 loss is: -2927.6719, the g loss is: 2912.414, the ae loss is: 0.0072830105, the jacobian loss is:0.10990187\n",
            "This is the iter 6353, the d1 loss is: 2844.1328, the d2 loss is: -3146.875, the g loss is: 3146.1719, the ae loss is: 0.0053069973, the jacobian loss is:0.10201744\n",
            "This is the iter 6354, the d1 loss is: 2862.8203, the d2 loss is: -3123.4453, the g loss is: 3130.7344, the ae loss is: 0.005389999, the jacobian loss is:0.0943638\n",
            "This is the iter 6355, the d1 loss is: 2824.2656, the d2 loss is: -3111.25, the g loss is: 3159.1406, the ae loss is: 0.008952632, the jacobian loss is:0.16089469\n",
            "This is the iter 6356, the d1 loss is: 2679.6094, the d2 loss is: -3018.461, the g loss is: 3056.0781, the ae loss is: 0.0071152556, the jacobian loss is:0.10805026\n",
            "This is the iter 6357, the d1 loss is: 2790.9922, the d2 loss is: -3092.539, the g loss is: 3072.539, the ae loss is: 0.0053771376, the jacobian loss is:0.09981273\n",
            "This is the iter 6358, the d1 loss is: 2860.7578, the d2 loss is: -3168.25, the g loss is: 3115.9219, the ae loss is: 0.0074171377, the jacobian loss is:0.09377076\n",
            "This is the iter 6359, the d1 loss is: 2877.0625, the d2 loss is: -3202.5938, the g loss is: 3191.25, the ae loss is: 0.0053196396, the jacobian loss is:0.14838907\n",
            "This is the iter 6360, the d1 loss is: 2757.6016, the d2 loss is: -3016.4062, the g loss is: 3071.2812, the ae loss is: 0.0058799656, the jacobian loss is:0.115868956\n",
            "This is the iter 6361, the d1 loss is: 2822.5938, the d2 loss is: -3084.461, the g loss is: 3166.2969, the ae loss is: 0.005006234, the jacobian loss is:0.10498425\n",
            "This is the iter 6362, the d1 loss is: 2722.4219, the d2 loss is: -3005.0156, the g loss is: 3029.8047, the ae loss is: 0.0056504626, the jacobian loss is:0.08510608\n",
            "This is the iter 6363, the d1 loss is: 2822.8906, the d2 loss is: -3136.8672, the g loss is: 3088.0234, the ae loss is: 0.0059048464, the jacobian loss is:0.10446622\n",
            "This is the iter 6364, the d1 loss is: 2806.3203, the d2 loss is: -3106.0156, the g loss is: 3105.6094, the ae loss is: 0.0078078634, the jacobian loss is:0.08355578\n",
            "This is the iter 6365, the d1 loss is: 2741.9844, the d2 loss is: -3044.8125, the g loss is: 3040.039, the ae loss is: 0.0070675183, the jacobian loss is:0.093955316\n",
            "This is the iter 6366, the d1 loss is: 2796.1875, the d2 loss is: -3088.9922, the g loss is: 3094.3125, the ae loss is: 0.0045915837, the jacobian loss is:0.14003593\n",
            "This is the iter 6367, the d1 loss is: 2870.2031, the d2 loss is: -3132.2969, the g loss is: 3115.2266, the ae loss is: 0.0075101363, the jacobian loss is:0.12737726\n",
            "This is the iter 6368, the d1 loss is: 2747.8828, the d2 loss is: -3011.8125, the g loss is: 3029.1953, the ae loss is: 0.004059305, the jacobian loss is:0.10627061\n",
            "This is the iter 6369, the d1 loss is: 2847.7969, the d2 loss is: -3120.7422, the g loss is: 3141.8047, the ae loss is: 0.006983219, the jacobian loss is:0.10870666\n",
            "This is the iter 6370, the d1 loss is: 2864.039, the d2 loss is: -3106.0, the g loss is: 3147.0234, the ae loss is: 0.0062430827, the jacobian loss is:0.087579034\n",
            "This is the iter 6371, the d1 loss is: 2795.6406, the d2 loss is: -3103.9531, the g loss is: 3139.5625, the ae loss is: 0.005603496, the jacobian loss is:0.0865201\n",
            "This is the iter 6372, the d1 loss is: 2712.3516, the d2 loss is: -3000.6406, the g loss is: 2922.4219, the ae loss is: 0.006813515, the jacobian loss is:0.10860144\n",
            "This is the iter 6373, the d1 loss is: 2747.0, the d2 loss is: -3019.1016, the g loss is: 3028.4766, the ae loss is: 0.00786156, the jacobian loss is:0.12558195\n",
            "This is the iter 6374, the d1 loss is: 2840.3672, the d2 loss is: -3125.0312, the g loss is: 3118.2578, the ae loss is: 0.006344933, the jacobian loss is:0.12084722\n",
            "This is the iter 6375, the d1 loss is: 2730.1875, the d2 loss is: -3045.6328, the g loss is: 3017.4219, the ae loss is: 0.00429273, the jacobian loss is:0.09155003\n",
            "This is the iter 6376, the d1 loss is: 2844.5078, the d2 loss is: -3133.7344, the g loss is: 3141.1875, the ae loss is: 0.0062103886, the jacobian loss is:0.10272601\n",
            "This is the iter 6377, the d1 loss is: 2837.2188, the d2 loss is: -3118.2031, the g loss is: 3139.2812, the ae loss is: 0.00758273, the jacobian loss is:0.1395589\n",
            "This is the iter 6378, the d1 loss is: 2864.7734, the d2 loss is: -3152.4297, the g loss is: 3154.0234, the ae loss is: 0.008251945, the jacobian loss is:0.12807252\n",
            "This is the iter 6379, the d1 loss is: 2718.3281, the d2 loss is: -3027.3203, the g loss is: 3118.9297, the ae loss is: 0.006111723, the jacobian loss is:0.08977445\n",
            "This is the iter 6380, the d1 loss is: 2875.3984, the d2 loss is: -3182.3125, the g loss is: 3192.1562, the ae loss is: 0.008818086, the jacobian loss is:0.096242175\n",
            "This is the iter 6381, the d1 loss is: 2873.1562, the d2 loss is: -3159.8594, the g loss is: 3150.2578, the ae loss is: 0.0060578366, the jacobian loss is:0.099775635\n",
            "This is the iter 6382, the d1 loss is: 2807.6797, the d2 loss is: -3088.3281, the g loss is: 3091.75, the ae loss is: 0.0044860244, the jacobian loss is:0.070815735\n",
            "This is the iter 6383, the d1 loss is: 2768.5703, the d2 loss is: -3039.2422, the g loss is: 3047.8672, the ae loss is: 0.008187283, the jacobian loss is:0.09151134\n",
            "This is the iter 6384, the d1 loss is: 2774.1953, the d2 loss is: -3053.539, the g loss is: 3016.7812, the ae loss is: 0.007850123, the jacobian loss is:0.108158365\n",
            "This is the iter 6385, the d1 loss is: 2815.2344, the d2 loss is: -3124.914, the g loss is: 3156.664, the ae loss is: 0.0058542443, the jacobian loss is:0.10513421\n",
            "This is the iter 6386, the d1 loss is: 2750.4219, the d2 loss is: -3052.914, the g loss is: 3059.375, the ae loss is: 0.0077380585, the jacobian loss is:0.11256707\n",
            "This is the iter 6387, the d1 loss is: 2950.2188, the d2 loss is: -3267.1719, the g loss is: 3255.25, the ae loss is: 0.006410502, the jacobian loss is:0.09576348\n",
            "This is the iter 6388, the d1 loss is: 2785.1875, the d2 loss is: -3093.7656, the g loss is: 3117.789, the ae loss is: 0.006611323, the jacobian loss is:0.13909775\n",
            "This is the iter 6389, the d1 loss is: 2903.0469, the d2 loss is: -3194.75, the g loss is: 3149.7031, the ae loss is: 0.007212557, the jacobian loss is:0.107384406\n",
            "This is the iter 6390, the d1 loss is: 2574.6406, the d2 loss is: -2852.2969, the g loss is: 2863.086, the ae loss is: 0.008765614, the jacobian loss is:0.11426466\n",
            "This is the iter 6391, the d1 loss is: 2683.164, the d2 loss is: -2953.0078, the g loss is: 3023.2031, the ae loss is: 0.0051921057, the jacobian loss is:0.11245418\n",
            "This is the iter 6392, the d1 loss is: 2820.7656, the d2 loss is: -3107.0156, the g loss is: 3127.1094, the ae loss is: 0.0049874196, the jacobian loss is:0.10488844\n",
            "This is the iter 6393, the d1 loss is: 2824.2344, the d2 loss is: -3091.8516, the g loss is: 3069.2656, the ae loss is: 0.007169215, the jacobian loss is:0.15621768\n",
            "This is the iter 6394, the d1 loss is: 2817.8125, the d2 loss is: -3136.1719, the g loss is: 3123.8906, the ae loss is: 0.007427395, the jacobian loss is:0.08626806\n",
            "This is the iter 6395, the d1 loss is: 2799.2188, the d2 loss is: -3038.2969, the g loss is: 3044.875, the ae loss is: 0.005172605, the jacobian loss is:0.10524268\n",
            "This is the iter 6396, the d1 loss is: 2685.6719, the d2 loss is: -2960.0938, the g loss is: 3010.164, the ae loss is: 0.00596024, the jacobian loss is:0.12625334\n",
            "This is the iter 6397, the d1 loss is: 2806.2656, the d2 loss is: -3120.7578, the g loss is: 3085.0781, the ae loss is: 0.0063788183, the jacobian loss is:0.1341681\n",
            "This is the iter 6398, the d1 loss is: 2808.836, the d2 loss is: -3114.625, the g loss is: 3138.8906, the ae loss is: 0.005736585, the jacobian loss is:0.09063726\n",
            "This is the iter 6399, the d1 loss is: 2782.1562, the d2 loss is: -3066.5469, the g loss is: 3075.0312, the ae loss is: 0.0053784926, the jacobian loss is:0.08762115\n",
            "This is the iter 6400, the d1 loss is: 2775.3125, the d2 loss is: -3082.6172, the g loss is: 3059.9453, the ae loss is: 0.004761403, the jacobian loss is:0.07404681\n",
            "0.20894182\n",
            "0.7814695\n",
            "This is the iter 6401, the d1 loss is: 2749.1406, the d2 loss is: -3032.1328, the g loss is: 3053.9688, the ae loss is: 0.00692453, the jacobian loss is:0.12842217\n",
            "This is the iter 6402, the d1 loss is: 2800.8438, the d2 loss is: -3106.2578, the g loss is: 3061.289, the ae loss is: 0.00733133, the jacobian loss is:0.14045793\n",
            "This is the iter 6403, the d1 loss is: 2813.414, the d2 loss is: -3102.2812, the g loss is: 3090.1875, the ae loss is: 0.0064312527, the jacobian loss is:0.08877682\n",
            "This is the iter 6404, the d1 loss is: 2698.3438, the d2 loss is: -3009.1562, the g loss is: 3025.1406, the ae loss is: 0.0072181774, the jacobian loss is:0.1609777\n",
            "This is the iter 6405, the d1 loss is: 2961.7812, the d2 loss is: -3284.9531, the g loss is: 3279.7188, the ae loss is: 0.010720452, the jacobian loss is:0.11004592\n",
            "This is the iter 6406, the d1 loss is: 2796.2031, the d2 loss is: -3102.8828, the g loss is: 3095.1953, the ae loss is: 0.008120109, the jacobian loss is:0.10636756\n",
            "This is the iter 6407, the d1 loss is: 2723.3906, the d2 loss is: -3021.8438, the g loss is: 3033.125, the ae loss is: 0.0066373646, the jacobian loss is:0.12920289\n",
            "This is the iter 6408, the d1 loss is: 2726.9453, the d2 loss is: -3006.5625, the g loss is: 3049.0312, the ae loss is: 0.0065202476, the jacobian loss is:0.071990766\n",
            "This is the iter 6409, the d1 loss is: 2837.0938, the d2 loss is: -3114.2656, the g loss is: 3065.961, the ae loss is: 0.0080449255, the jacobian loss is:0.09699665\n",
            "This is the iter 6410, the d1 loss is: 2816.3906, the d2 loss is: -3096.2344, the g loss is: 3081.7969, the ae loss is: 0.0059904894, the jacobian loss is:0.09291465\n",
            "This is the iter 6411, the d1 loss is: 2791.8047, the d2 loss is: -3052.164, the g loss is: 3113.1719, the ae loss is: 0.004309412, the jacobian loss is:0.11721203\n",
            "This is the iter 6412, the d1 loss is: 2741.7422, the d2 loss is: -3049.4531, the g loss is: 3051.789, the ae loss is: 0.0053036157, the jacobian loss is:0.06469628\n",
            "This is the iter 6413, the d1 loss is: 2841.8594, the d2 loss is: -3115.2969, the g loss is: 3112.3906, the ae loss is: 0.0055160024, the jacobian loss is:0.098312736\n",
            "This is the iter 6414, the d1 loss is: 2593.2969, the d2 loss is: -2875.6094, the g loss is: 2903.3828, the ae loss is: 0.009196172, the jacobian loss is:0.1909472\n",
            "This is the iter 6415, the d1 loss is: 2724.836, the d2 loss is: -2990.375, the g loss is: 3005.6484, the ae loss is: 0.0060975486, the jacobian loss is:0.09270386\n",
            "This is the iter 6416, the d1 loss is: 2799.8047, the d2 loss is: -3079.8594, the g loss is: 3042.8828, the ae loss is: 0.0047920244, the jacobian loss is:0.1814611\n",
            "This is the iter 6417, the d1 loss is: 2797.4375, the d2 loss is: -3095.6484, the g loss is: 3077.4453, the ae loss is: 0.006133048, the jacobian loss is:0.11495904\n",
            "This is the iter 6418, the d1 loss is: 2810.8281, the d2 loss is: -3094.8438, the g loss is: 3218.625, the ae loss is: 0.005886138, the jacobian loss is:0.08136462\n",
            "This is the iter 6419, the d1 loss is: 2731.0, the d2 loss is: -3002.3438, the g loss is: 3019.8906, the ae loss is: 0.0067403913, the jacobian loss is:0.09705527\n",
            "This is the iter 6420, the d1 loss is: 2746.336, the d2 loss is: -2989.0234, the g loss is: 3088.0, the ae loss is: 0.006997752, the jacobian loss is:0.11666196\n",
            "This is the iter 6421, the d1 loss is: 2833.289, the d2 loss is: -3145.5625, the g loss is: 3148.7734, the ae loss is: 0.0058421027, the jacobian loss is:0.11059446\n",
            "This is the iter 6422, the d1 loss is: 2786.5547, the d2 loss is: -3074.2734, the g loss is: 3015.336, the ae loss is: 0.011037981, the jacobian loss is:0.14776117\n",
            "This is the iter 6423, the d1 loss is: 2627.125, the d2 loss is: -2890.25, the g loss is: 2849.3438, the ae loss is: 0.0068225535, the jacobian loss is:0.08790876\n",
            "This is the iter 6424, the d1 loss is: 2637.9219, the d2 loss is: -2964.8828, the g loss is: 2913.3281, the ae loss is: 0.0060721016, the jacobian loss is:0.14978985\n",
            "This is the iter 6425, the d1 loss is: 2728.0938, the d2 loss is: -3052.125, the g loss is: 3031.5234, the ae loss is: 0.004777838, the jacobian loss is:0.08012173\n",
            "This is the iter 6426, the d1 loss is: 2812.4375, the d2 loss is: -3100.1172, the g loss is: 3047.375, the ae loss is: 0.0066602365, the jacobian loss is:0.14251707\n",
            "This is the iter 6427, the d1 loss is: 2766.7031, the d2 loss is: -3025.7266, the g loss is: 3109.1562, the ae loss is: 0.0076849787, the jacobian loss is:0.09738737\n",
            "This is the iter 6428, the d1 loss is: 2816.8281, the d2 loss is: -3088.4688, the g loss is: 3117.1016, the ae loss is: 0.0072041005, the jacobian loss is:0.08730177\n",
            "This is the iter 6429, the d1 loss is: 2773.8203, the d2 loss is: -3075.3828, the g loss is: 3071.039, the ae loss is: 0.007440988, the jacobian loss is:0.12115011\n",
            "This is the iter 6430, the d1 loss is: 2763.3828, the d2 loss is: -3041.4688, the g loss is: 2978.4062, the ae loss is: 0.006262439, the jacobian loss is:0.16575082\n",
            "This is the iter 6431, the d1 loss is: 3201.9219, the d2 loss is: -3468.4766, the g loss is: 3526.6562, the ae loss is: 0.0060341996, the jacobian loss is:0.10684212\n",
            "This is the iter 6432, the d1 loss is: 2771.1328, the d2 loss is: -3069.8672, the g loss is: 3060.2188, the ae loss is: 0.00753481, the jacobian loss is:0.10195764\n",
            "This is the iter 6433, the d1 loss is: 2773.5156, the d2 loss is: -3040.875, the g loss is: 3059.8828, the ae loss is: 0.0065218136, the jacobian loss is:0.093660794\n",
            "This is the iter 6434, the d1 loss is: 2826.3438, the d2 loss is: -3100.0078, the g loss is: 3154.4844, the ae loss is: 0.005714612, the jacobian loss is:0.13672428\n",
            "This is the iter 6435, the d1 loss is: 2745.7812, the d2 loss is: -3011.8594, the g loss is: 3025.0625, the ae loss is: 0.006141142, the jacobian loss is:0.09279338\n",
            "This is the iter 6436, the d1 loss is: 2799.414, the d2 loss is: -3089.164, the g loss is: 3087.3438, the ae loss is: 0.0066061, the jacobian loss is:0.10705288\n",
            "This is the iter 6437, the d1 loss is: 2768.25, the d2 loss is: -3039.5312, the g loss is: 3029.9219, the ae loss is: 0.0064241746, the jacobian loss is:0.088805996\n",
            "This is the iter 6438, the d1 loss is: 2725.6797, the d2 loss is: -2999.8281, the g loss is: 3054.3594, the ae loss is: 0.008748309, the jacobian loss is:0.06605488\n",
            "This is the iter 6439, the d1 loss is: 2754.9062, the d2 loss is: -3064.414, the g loss is: 3050.8438, the ae loss is: 0.007226665, the jacobian loss is:0.108855784\n",
            "This is the iter 6440, the d1 loss is: 2864.2422, the d2 loss is: -3097.7969, the g loss is: 3071.039, the ae loss is: 0.009031646, the jacobian loss is:0.11476175\n",
            "This is the iter 6441, the d1 loss is: 2738.4766, the d2 loss is: -3019.2969, the g loss is: 3045.8984, the ae loss is: 0.006757984, the jacobian loss is:0.106645375\n",
            "This is the iter 6442, the d1 loss is: 2818.6562, the d2 loss is: -3118.7344, the g loss is: 3058.75, the ae loss is: 0.0068840235, the jacobian loss is:0.091068864\n",
            "This is the iter 6443, the d1 loss is: 2807.1875, the d2 loss is: -3095.6016, the g loss is: 3057.7266, the ae loss is: 0.007746147, the jacobian loss is:0.13062902\n",
            "This is the iter 6444, the d1 loss is: 2973.0312, the d2 loss is: -3276.0078, the g loss is: 3318.961, the ae loss is: 0.0059529156, the jacobian loss is:0.07152153\n",
            "This is the iter 6445, the d1 loss is: 2784.4062, the d2 loss is: -3078.6875, the g loss is: 3088.6094, the ae loss is: 0.007659727, the jacobian loss is:0.09715888\n",
            "This is the iter 6446, the d1 loss is: 2816.5547, the d2 loss is: -3109.5625, the g loss is: 3124.4844, the ae loss is: 0.0058521195, the jacobian loss is:0.09825748\n",
            "This is the iter 6447, the d1 loss is: 2913.4062, the d2 loss is: -3178.7812, the g loss is: 3102.8984, the ae loss is: 0.005834517, the jacobian loss is:0.10100553\n",
            "This is the iter 6448, the d1 loss is: 2664.4922, the d2 loss is: -2961.6094, the g loss is: 2886.9531, the ae loss is: 0.006070938, the jacobian loss is:0.080669105\n",
            "This is the iter 6449, the d1 loss is: 2857.0312, the d2 loss is: -3112.9922, the g loss is: 3126.5938, the ae loss is: 0.008005455, the jacobian loss is:0.07568285\n",
            "This is the iter 6450, the d1 loss is: 2823.4062, the d2 loss is: -3097.7656, the g loss is: 3149.914, the ae loss is: 0.0074861525, the jacobian loss is:0.08170379\n",
            "This is the iter 6451, the d1 loss is: 2525.0469, the d2 loss is: -2841.5, the g loss is: 2820.6094, the ae loss is: 0.0058509083, the jacobian loss is:0.083437115\n",
            "This is the iter 6452, the d1 loss is: 2826.5, the d2 loss is: -3114.3438, the g loss is: 3063.8594, the ae loss is: 0.0062790457, the jacobian loss is:0.11761227\n",
            "This is the iter 6453, the d1 loss is: 3029.0938, the d2 loss is: -3291.6875, the g loss is: 3267.0156, the ae loss is: 0.006826204, the jacobian loss is:0.09835593\n",
            "This is the iter 6454, the d1 loss is: 2741.2969, the d2 loss is: -3034.3438, the g loss is: 3082.1562, the ae loss is: 0.0056783585, the jacobian loss is:0.11374671\n",
            "This is the iter 6455, the d1 loss is: 2770.5, the d2 loss is: -3022.2031, the g loss is: 3052.5156, the ae loss is: 0.008158931, the jacobian loss is:0.11726183\n",
            "This is the iter 6456, the d1 loss is: 2863.8203, the d2 loss is: -3142.9297, the g loss is: 3120.875, the ae loss is: 0.0049049337, the jacobian loss is:0.09706382\n",
            "This is the iter 6457, the d1 loss is: 2767.5156, the d2 loss is: -3047.9844, the g loss is: 3077.9766, the ae loss is: 0.0055396818, the jacobian loss is:0.07966596\n",
            "This is the iter 6458, the d1 loss is: 2915.4219, the d2 loss is: -3179.2969, the g loss is: 3211.5547, the ae loss is: 0.0040751067, the jacobian loss is:0.112706445\n",
            "This is the iter 6459, the d1 loss is: 2736.8672, the d2 loss is: -3009.3984, the g loss is: 3001.125, the ae loss is: 0.005278821, the jacobian loss is:0.100122966\n",
            "This is the iter 6460, the d1 loss is: 2717.7812, the d2 loss is: -3058.9297, the g loss is: 3117.9375, the ae loss is: 0.006903597, the jacobian loss is:0.08719794\n",
            "This is the iter 6461, the d1 loss is: 2767.3594, the d2 loss is: -3004.7812, the g loss is: 3100.7422, the ae loss is: 0.006308429, the jacobian loss is:0.08439689\n",
            "This is the iter 6462, the d1 loss is: 2631.9688, the d2 loss is: -2930.4375, the g loss is: 2902.8281, the ae loss is: 0.0056641726, the jacobian loss is:0.11024123\n",
            "This is the iter 6463, the d1 loss is: 2713.25, the d2 loss is: -2995.7734, the g loss is: 2982.375, the ae loss is: 0.007922927, the jacobian loss is:0.08993325\n",
            "This is the iter 6464, the d1 loss is: 2697.3281, the d2 loss is: -3004.0312, the g loss is: 3014.3516, the ae loss is: 0.00429264, the jacobian loss is:0.1010105\n",
            "This is the iter 6465, the d1 loss is: 2751.789, the d2 loss is: -3013.5625, the g loss is: 3043.2188, the ae loss is: 0.008097282, the jacobian loss is:0.12353878\n",
            "This is the iter 6466, the d1 loss is: 2811.8984, the d2 loss is: -3070.4531, the g loss is: 3109.7969, the ae loss is: 0.005793734, the jacobian loss is:0.11061099\n",
            "This is the iter 6467, the d1 loss is: 2752.3828, the d2 loss is: -3019.8828, the g loss is: 3028.4766, the ae loss is: 0.008423391, the jacobian loss is:0.115318656\n",
            "This is the iter 6468, the d1 loss is: 2902.25, the d2 loss is: -3169.461, the g loss is: 3165.6172, the ae loss is: 0.00588389, the jacobian loss is:0.12552878\n",
            "This is the iter 6469, the d1 loss is: 2950.1172, the d2 loss is: -3259.6406, the g loss is: 3230.0078, the ae loss is: 0.005710382, the jacobian loss is:0.11695307\n",
            "This is the iter 6470, the d1 loss is: 2736.3906, the d2 loss is: -3036.2188, the g loss is: 3055.8281, the ae loss is: 0.0058596446, the jacobian loss is:0.0947437\n",
            "This is the iter 6471, the d1 loss is: 2741.8672, the d2 loss is: -3048.9453, the g loss is: 3088.0156, the ae loss is: 0.0051498795, the jacobian loss is:0.07330986\n",
            "This is the iter 6472, the d1 loss is: 2824.8906, the d2 loss is: -3095.0312, the g loss is: 3067.3984, the ae loss is: 0.0061438535, the jacobian loss is:0.11108569\n",
            "This is the iter 6473, the d1 loss is: 2831.2344, the d2 loss is: -3112.7266, the g loss is: 3120.6094, the ae loss is: 0.006495976, the jacobian loss is:0.10944777\n",
            "This is the iter 6474, the d1 loss is: 2749.3125, the d2 loss is: -3057.836, the g loss is: 2991.1406, the ae loss is: 0.006152656, the jacobian loss is:0.08804883\n",
            "This is the iter 6475, the d1 loss is: 2633.1562, the d2 loss is: -2899.75, the g loss is: 2870.4453, the ae loss is: 0.0054512043, the jacobian loss is:0.103242986\n",
            "This is the iter 6476, the d1 loss is: 2980.1094, the d2 loss is: -3250.6875, the g loss is: 3276.9219, the ae loss is: 0.006116217, the jacobian loss is:0.09783686\n",
            "This is the iter 6477, the d1 loss is: 2855.3672, the d2 loss is: -3138.0234, the g loss is: 3112.8672, the ae loss is: 0.004860691, the jacobian loss is:0.104012296\n",
            "This is the iter 6478, the d1 loss is: 2918.461, the d2 loss is: -3163.4297, the g loss is: 3173.5234, the ae loss is: 0.0064641694, the jacobian loss is:0.105641395\n",
            "This is the iter 6479, the d1 loss is: 2809.0, the d2 loss is: -3078.4922, the g loss is: 3096.9453, the ae loss is: 0.0061324323, the jacobian loss is:0.11591857\n",
            "This is the iter 6480, the d1 loss is: 2838.3281, the d2 loss is: -3102.2656, the g loss is: 3084.7344, the ae loss is: 0.0055440976, the jacobian loss is:0.095079176\n",
            "This is the iter 6481, the d1 loss is: 2860.6094, the d2 loss is: -3132.625, the g loss is: 3134.0938, the ae loss is: 0.0074949404, the jacobian loss is:0.079536766\n",
            "This is the iter 6482, the d1 loss is: 2898.8516, the d2 loss is: -3158.5469, the g loss is: 3158.164, the ae loss is: 0.0074789496, the jacobian loss is:0.08440866\n",
            "This is the iter 6483, the d1 loss is: 2751.461, the d2 loss is: -3052.1797, the g loss is: 3056.6172, the ae loss is: 0.0074793687, the jacobian loss is:0.124126896\n",
            "This is the iter 6484, the d1 loss is: 2785.6094, the d2 loss is: -3042.289, the g loss is: 3121.914, the ae loss is: 0.006633266, the jacobian loss is:0.10329628\n",
            "This is the iter 6485, the d1 loss is: 2886.375, the d2 loss is: -3155.9375, the g loss is: 3156.4531, the ae loss is: 0.0061162612, the jacobian loss is:0.124960095\n",
            "This is the iter 6486, the d1 loss is: 2636.4688, the d2 loss is: -2928.2578, the g loss is: 2902.1094, the ae loss is: 0.004699448, the jacobian loss is:0.08238401\n",
            "This is the iter 6487, the d1 loss is: 2850.6797, the d2 loss is: -3111.9062, the g loss is: 3103.6953, the ae loss is: 0.006582171, the jacobian loss is:0.099502176\n",
            "This is the iter 6488, the d1 loss is: 2807.9688, the d2 loss is: -3119.8906, the g loss is: 3070.586, the ae loss is: 0.0060080346, the jacobian loss is:0.11386162\n",
            "This is the iter 6489, the d1 loss is: 2851.6562, the d2 loss is: -3156.375, the g loss is: 3178.586, the ae loss is: 0.005846184, the jacobian loss is:0.08349257\n",
            "This is the iter 6490, the d1 loss is: 2845.6094, the d2 loss is: -3133.9453, the g loss is: 3142.9531, the ae loss is: 0.0067676273, the jacobian loss is:0.10424452\n",
            "This is the iter 6491, the d1 loss is: 2787.1484, the d2 loss is: -3056.5625, the g loss is: 3017.4766, the ae loss is: 0.006685978, the jacobian loss is:0.11084413\n",
            "This is the iter 6492, the d1 loss is: 2833.3516, the d2 loss is: -3120.3438, the g loss is: 3132.2266, the ae loss is: 0.00589509, the jacobian loss is:0.10559064\n",
            "This is the iter 6493, the d1 loss is: 2845.211, the d2 loss is: -3084.5078, the g loss is: 3113.2031, the ae loss is: 0.008088921, the jacobian loss is:0.08733368\n",
            "This is the iter 6494, the d1 loss is: 2801.7734, the d2 loss is: -3077.6562, the g loss is: 3111.8125, the ae loss is: 0.0056913164, the jacobian loss is:0.120146975\n",
            "This is the iter 6495, the d1 loss is: 2823.1875, the d2 loss is: -3090.5234, the g loss is: 3101.6797, the ae loss is: 0.006499106, the jacobian loss is:0.11597697\n",
            "This is the iter 6496, the d1 loss is: 2810.4453, the d2 loss is: -3098.2734, the g loss is: 3056.5781, the ae loss is: 0.0046931785, the jacobian loss is:0.08880661\n",
            "This is the iter 6497, the d1 loss is: 2993.7188, the d2 loss is: -3263.0312, the g loss is: 3303.6875, the ae loss is: 0.0060878247, the jacobian loss is:0.09056261\n",
            "This is the iter 6498, the d1 loss is: 2589.8281, the d2 loss is: -2864.4062, the g loss is: 2901.25, the ae loss is: 0.0060170526, the jacobian loss is:0.11278561\n",
            "This is the iter 6499, the d1 loss is: 2860.289, the d2 loss is: -3121.3906, the g loss is: 3121.625, the ae loss is: 0.008656403, the jacobian loss is:0.092251666\n",
            "This is the iter 6500, the d1 loss is: 3196.3438, the d2 loss is: -3491.625, the g loss is: 3468.8672, the ae loss is: 0.005514461, the jacobian loss is:0.10024479\n",
            "0.2083968\n",
            "0.78156346\n",
            "This is the iter 6501, the d1 loss is: 2825.7734, the d2 loss is: -3112.461, the g loss is: 3101.6875, the ae loss is: 0.006275971, the jacobian loss is:0.08985063\n",
            "This is the iter 6502, the d1 loss is: 3230.5781, the d2 loss is: -3500.3984, the g loss is: 3520.125, the ae loss is: 0.0048683556, the jacobian loss is:0.094154365\n",
            "This is the iter 6503, the d1 loss is: 2891.1016, the d2 loss is: -3151.3438, the g loss is: 3096.0781, the ae loss is: 0.005914897, the jacobian loss is:0.14979003\n",
            "This is the iter 6504, the d1 loss is: 2807.5781, the d2 loss is: -3082.1094, the g loss is: 3062.5469, the ae loss is: 0.0068406644, the jacobian loss is:0.12253291\n",
            "This is the iter 6505, the d1 loss is: 2974.3984, the d2 loss is: -3197.7188, the g loss is: 3110.1016, the ae loss is: 0.0061246846, the jacobian loss is:0.123787455\n",
            "This is the iter 6506, the d1 loss is: 2817.3594, the d2 loss is: -3097.6875, the g loss is: 3129.6953, the ae loss is: 0.007808135, the jacobian loss is:0.116038814\n",
            "This is the iter 6507, the d1 loss is: 2729.8828, the d2 loss is: -3015.6172, the g loss is: 3072.8438, the ae loss is: 0.0076085865, the jacobian loss is:0.1254502\n",
            "This is the iter 6508, the d1 loss is: 2903.039, the d2 loss is: -3191.4922, the g loss is: 3154.6406, the ae loss is: 0.007677553, the jacobian loss is:0.08882964\n",
            "This is the iter 6509, the d1 loss is: 2908.5312, the d2 loss is: -3185.6406, the g loss is: 3186.8828, the ae loss is: 0.0061133374, the jacobian loss is:0.07829409\n",
            "This is the iter 6510, the d1 loss is: 2641.6953, the d2 loss is: -2900.4219, the g loss is: 2958.0234, the ae loss is: 0.005299487, the jacobian loss is:0.0873391\n",
            "This is the iter 6511, the d1 loss is: 2790.1719, the d2 loss is: -3031.75, the g loss is: 2947.7578, the ae loss is: 0.007927715, the jacobian loss is:0.087338805\n",
            "This is the iter 6512, the d1 loss is: 2849.5312, the d2 loss is: -3128.0625, the g loss is: 3078.3438, the ae loss is: 0.006879156, the jacobian loss is:0.09025666\n",
            "This is the iter 6513, the d1 loss is: 2847.0234, the d2 loss is: -3119.2266, the g loss is: 3136.9297, the ae loss is: 0.0068104663, the jacobian loss is:0.07738865\n",
            "This is the iter 6514, the d1 loss is: 2851.6719, the d2 loss is: -3119.8906, the g loss is: 3201.3281, the ae loss is: 0.004929583, the jacobian loss is:0.12908581\n",
            "This is the iter 6515, the d1 loss is: 3007.7578, the d2 loss is: -3273.7734, the g loss is: 3279.4922, the ae loss is: 0.006731684, the jacobian loss is:0.12127807\n",
            "This is the iter 6516, the d1 loss is: 3056.0156, the d2 loss is: -3337.5781, the g loss is: 3335.875, the ae loss is: 0.008855005, the jacobian loss is:0.11355136\n",
            "This is the iter 6517, the d1 loss is: 2848.8047, the d2 loss is: -3107.0781, the g loss is: 3125.5234, the ae loss is: 0.0080781095, the jacobian loss is:0.108564176\n",
            "This is the iter 6518, the d1 loss is: 2948.3203, the d2 loss is: -3218.3984, the g loss is: 3237.7734, the ae loss is: 0.008135174, the jacobian loss is:0.090537235\n",
            "This is the iter 6519, the d1 loss is: 2893.5234, the d2 loss is: -3137.414, the g loss is: 3200.7656, the ae loss is: 0.004135464, the jacobian loss is:0.12495947\n",
            "This is the iter 6520, the d1 loss is: 2831.1719, the d2 loss is: -3107.2422, the g loss is: 3080.9375, the ae loss is: 0.0054767146, the jacobian loss is:0.09974108\n",
            "This is the iter 6521, the d1 loss is: 2892.7344, the d2 loss is: -3182.3125, the g loss is: 3155.75, the ae loss is: 0.0063961428, the jacobian loss is:0.08559953\n",
            "This is the iter 6522, the d1 loss is: 2844.2188, the d2 loss is: -3144.8672, the g loss is: 3159.3125, the ae loss is: 0.005511942, the jacobian loss is:0.08773975\n",
            "This is the iter 6523, the d1 loss is: 2834.9531, the d2 loss is: -3095.0703, the g loss is: 3119.0156, the ae loss is: 0.005775219, the jacobian loss is:0.08631738\n",
            "This is the iter 6524, the d1 loss is: 2859.6562, the d2 loss is: -3151.3906, the g loss is: 3180.5234, the ae loss is: 0.005755298, the jacobian loss is:0.08506272\n",
            "This is the iter 6525, the d1 loss is: 2895.2812, the d2 loss is: -3160.9766, the g loss is: 3147.0156, the ae loss is: 0.006077039, the jacobian loss is:0.09206655\n",
            "This is the iter 6526, the d1 loss is: 2867.0625, the d2 loss is: -3123.5469, the g loss is: 3102.5, the ae loss is: 0.006557943, the jacobian loss is:0.115137175\n",
            "This is the iter 6527, the d1 loss is: 2909.0312, the d2 loss is: -3164.6953, the g loss is: 3214.5234, the ae loss is: 0.005630825, the jacobian loss is:0.07872182\n",
            "This is the iter 6528, the d1 loss is: 2972.2188, the d2 loss is: -3233.6016, the g loss is: 3211.1094, the ae loss is: 0.0058464557, the jacobian loss is:0.092826925\n",
            "This is the iter 6529, the d1 loss is: 2931.5, the d2 loss is: -3219.8906, the g loss is: 3186.1016, the ae loss is: 0.00781662, the jacobian loss is:0.121413715\n",
            "This is the iter 6530, the d1 loss is: 2933.5312, the d2 loss is: -3199.289, the g loss is: 3196.586, the ae loss is: 0.005830733, the jacobian loss is:0.09255587\n",
            "This is the iter 6531, the d1 loss is: 2907.2656, the d2 loss is: -3188.789, the g loss is: 3191.9844, the ae loss is: 0.006765218, the jacobian loss is:0.112060614\n",
            "This is the iter 6532, the d1 loss is: 2917.3438, the d2 loss is: -3171.1797, the g loss is: 3178.8828, the ae loss is: 0.006803627, the jacobian loss is:0.08595442\n",
            "This is the iter 6533, the d1 loss is: 2898.7188, the d2 loss is: -3189.7188, the g loss is: 3212.0625, the ae loss is: 0.006311527, the jacobian loss is:0.1147207\n",
            "This is the iter 6534, the d1 loss is: 2846.0469, the d2 loss is: -3118.8594, the g loss is: 3041.2188, the ae loss is: 0.008681601, the jacobian loss is:0.10202801\n",
            "This is the iter 6535, the d1 loss is: 2860.9766, the d2 loss is: -3130.0469, the g loss is: 3157.4688, the ae loss is: 0.0068109226, the jacobian loss is:0.10005075\n",
            "This is the iter 6536, the d1 loss is: 2887.711, the d2 loss is: -3150.8906, the g loss is: 3153.8281, the ae loss is: 0.008142034, the jacobian loss is:0.10178534\n",
            "This is the iter 6537, the d1 loss is: 2889.2656, the d2 loss is: -3210.9219, the g loss is: 3255.6719, the ae loss is: 0.004790007, the jacobian loss is:0.098554775\n",
            "This is the iter 6538, the d1 loss is: 2674.9375, the d2 loss is: -2970.7812, the g loss is: 2937.7344, the ae loss is: 0.00441192, the jacobian loss is:0.106366076\n",
            "This is the iter 6539, the d1 loss is: 2899.6094, the d2 loss is: -3124.5, the g loss is: 3125.211, the ae loss is: 0.0054730037, the jacobian loss is:0.08968605\n",
            "This is the iter 6540, the d1 loss is: 2813.9922, the d2 loss is: -3094.4844, the g loss is: 3086.961, the ae loss is: 0.008124264, the jacobian loss is:0.09194557\n",
            "This is the iter 6541, the d1 loss is: 3199.1875, the d2 loss is: -3465.3672, the g loss is: 3496.0156, the ae loss is: 0.007791332, the jacobian loss is:0.110844396\n",
            "This is the iter 6542, the d1 loss is: 2866.461, the d2 loss is: -3161.8438, the g loss is: 3165.7344, the ae loss is: 0.0061822953, the jacobian loss is:0.087753765\n",
            "This is the iter 6543, the d1 loss is: 2613.4922, the d2 loss is: -2914.9844, the g loss is: 2927.7656, the ae loss is: 0.0057173977, the jacobian loss is:0.10791235\n",
            "This is the iter 6544, the d1 loss is: 2888.1953, the d2 loss is: -3171.0312, the g loss is: 3136.3438, the ae loss is: 0.007963054, the jacobian loss is:0.09149655\n",
            "This is the iter 6545, the d1 loss is: 2661.4844, the d2 loss is: -2936.2656, the g loss is: 2943.9062, the ae loss is: 0.00583491, the jacobian loss is:0.090114996\n",
            "This is the iter 6546, the d1 loss is: 2892.8281, the d2 loss is: -3193.3984, the g loss is: 3164.3281, the ae loss is: 0.006406712, the jacobian loss is:0.10347691\n",
            "This is the iter 6547, the d1 loss is: 2831.3594, the d2 loss is: -3093.2734, the g loss is: 3084.6719, the ae loss is: 0.006705188, the jacobian loss is:0.087926775\n",
            "This is the iter 6548, the d1 loss is: 3175.1172, the d2 loss is: -3460.461, the g loss is: 3479.5781, the ae loss is: 0.006786803, the jacobian loss is:0.143933\n",
            "This is the iter 6549, the d1 loss is: 3091.25, the d2 loss is: -3368.7031, the g loss is: 3411.086, the ae loss is: 0.006544237, the jacobian loss is:0.12379988\n",
            "This is the iter 6550, the d1 loss is: 2953.9688, the d2 loss is: -3233.8516, the g loss is: 3190.9844, the ae loss is: 0.0062591983, the jacobian loss is:0.07590126\n",
            "This is the iter 6551, the d1 loss is: 2936.25, the d2 loss is: -3201.1484, the g loss is: 3178.4531, the ae loss is: 0.006696312, the jacobian loss is:0.08701091\n",
            "This is the iter 6552, the d1 loss is: 2877.7266, the d2 loss is: -3148.3125, the g loss is: 3136.3672, the ae loss is: 0.0051161405, the jacobian loss is:0.073511906\n",
            "This is the iter 6553, the d1 loss is: 2940.5625, the d2 loss is: -3166.7031, the g loss is: 3133.914, the ae loss is: 0.006313283, the jacobian loss is:0.17067906\n",
            "This is the iter 6554, the d1 loss is: 2761.7344, the d2 loss is: -3046.8281, the g loss is: 3102.0547, the ae loss is: 0.00585442, the jacobian loss is:0.12380585\n",
            "This is the iter 6555, the d1 loss is: 2925.8516, the d2 loss is: -3167.2031, the g loss is: 3154.1484, the ae loss is: 0.0063769436, the jacobian loss is:0.11936933\n",
            "This is the iter 6556, the d1 loss is: 2821.8281, the d2 loss is: -3080.3125, the g loss is: 3040.9062, the ae loss is: 0.009194599, the jacobian loss is:0.10338247\n",
            "This is the iter 6557, the d1 loss is: 2918.7734, the d2 loss is: -3161.414, the g loss is: 3158.8438, the ae loss is: 0.0069325967, the jacobian loss is:0.0991421\n",
            "This is the iter 6558, the d1 loss is: 3033.289, the d2 loss is: -3276.7969, the g loss is: 3312.5625, the ae loss is: 0.006246791, the jacobian loss is:0.075449\n",
            "This is the iter 6559, the d1 loss is: 2866.7656, the d2 loss is: -3095.4922, the g loss is: 3164.5, the ae loss is: 0.0069918255, the jacobian loss is:0.07796672\n",
            "This is the iter 6560, the d1 loss is: 2793.7656, the d2 loss is: -3055.125, the g loss is: 3080.1328, the ae loss is: 0.008952031, the jacobian loss is:0.069217816\n",
            "This is the iter 6561, the d1 loss is: 2825.0469, the d2 loss is: -3114.6953, the g loss is: 3069.9297, the ae loss is: 0.006625821, the jacobian loss is:0.113157816\n",
            "This is the iter 6562, the d1 loss is: 2848.7188, the d2 loss is: -3098.9375, the g loss is: 3092.0312, the ae loss is: 0.0077433847, the jacobian loss is:0.15315244\n",
            "This is the iter 6563, the d1 loss is: 2728.8281, the d2 loss is: -2988.1562, the g loss is: 3016.7344, the ae loss is: 0.006474858, the jacobian loss is:0.16289812\n",
            "This is the iter 6564, the d1 loss is: 2877.1016, the d2 loss is: -3112.3281, the g loss is: 3169.3672, the ae loss is: 0.0060561467, the jacobian loss is:0.0656601\n",
            "This is the iter 6565, the d1 loss is: 2857.2578, the d2 loss is: -3189.3281, the g loss is: 3186.7578, the ae loss is: 0.0047131786, the jacobian loss is:0.06754716\n",
            "This is the iter 6566, the d1 loss is: 2921.0469, the d2 loss is: -3211.7734, the g loss is: 3242.0312, the ae loss is: 0.005380608, the jacobian loss is:0.10829465\n",
            "This is the iter 6567, the d1 loss is: 2891.3906, the d2 loss is: -3165.0, the g loss is: 3175.7656, the ae loss is: 0.007303128, the jacobian loss is:0.08781407\n",
            "This is the iter 6568, the d1 loss is: 2885.8906, the d2 loss is: -3159.3203, the g loss is: 3145.4531, the ae loss is: 0.006528928, the jacobian loss is:0.08884405\n",
            "This is the iter 6569, the d1 loss is: 2923.8906, the d2 loss is: -3183.914, the g loss is: 3117.3672, the ae loss is: 0.006294684, the jacobian loss is:0.092964105\n",
            "This is the iter 6570, the d1 loss is: 2896.1719, the d2 loss is: -3159.4219, the g loss is: 3167.3516, the ae loss is: 0.0054140305, the jacobian loss is:0.07874523\n",
            "This is the iter 6571, the d1 loss is: 2940.3516, the d2 loss is: -3198.1719, the g loss is: 3225.4453, the ae loss is: 0.0058242376, the jacobian loss is:0.09624074\n",
            "This is the iter 6572, the d1 loss is: 2821.4531, the d2 loss is: -3081.5547, the g loss is: 3093.586, the ae loss is: 0.0061000083, the jacobian loss is:0.10655764\n",
            "This is the iter 6573, the d1 loss is: 2892.7656, the d2 loss is: -3176.2656, the g loss is: 3156.6406, the ae loss is: 0.008408823, the jacobian loss is:0.123345\n",
            "This is the iter 6574, the d1 loss is: 2910.4844, the d2 loss is: -3180.5156, the g loss is: 3193.8281, the ae loss is: 0.0062881224, the jacobian loss is:0.11865704\n",
            "This is the iter 6575, the d1 loss is: 2869.2266, the d2 loss is: -3118.2266, the g loss is: 3132.5469, the ae loss is: 0.007986488, the jacobian loss is:0.11575432\n",
            "This is the iter 6576, the d1 loss is: 2912.4531, the d2 loss is: -3181.0625, the g loss is: 3134.789, the ae loss is: 0.007859542, the jacobian loss is:0.07144501\n",
            "This is the iter 6577, the d1 loss is: 2955.5938, the d2 loss is: -3226.8438, the g loss is: 3140.1953, the ae loss is: 0.007647944, the jacobian loss is:0.10103783\n",
            "This is the iter 6578, the d1 loss is: 2941.0156, the d2 loss is: -3216.9844, the g loss is: 3257.4375, the ae loss is: 0.0051239557, the jacobian loss is:0.08136947\n",
            "This is the iter 6579, the d1 loss is: 2906.5, the d2 loss is: -3183.1875, the g loss is: 3174.4531, the ae loss is: 0.004380102, the jacobian loss is:0.07555825\n",
            "This is the iter 6580, the d1 loss is: 2887.4766, the d2 loss is: -3158.3828, the g loss is: 3174.289, the ae loss is: 0.008178184, the jacobian loss is:0.10095415\n",
            "This is the iter 6581, the d1 loss is: 3034.961, the d2 loss is: -3317.7969, the g loss is: 3349.4531, the ae loss is: 0.0052601807, the jacobian loss is:0.15488356\n",
            "This is the iter 6582, the d1 loss is: 2975.9766, the d2 loss is: -3193.7422, the g loss is: 3140.086, the ae loss is: 0.00853298, the jacobian loss is:0.07642137\n",
            "This is the iter 6583, the d1 loss is: 2930.2734, the d2 loss is: -3189.6406, the g loss is: 3232.5, the ae loss is: 0.006257827, the jacobian loss is:0.13515662\n",
            "This is the iter 6584, the d1 loss is: 2912.414, the d2 loss is: -3213.8906, the g loss is: 3181.5469, the ae loss is: 0.00469666, the jacobian loss is:0.07334354\n",
            "This is the iter 6585, the d1 loss is: 2941.6719, the d2 loss is: -3167.0312, the g loss is: 3265.3984, the ae loss is: 0.0055887136, the jacobian loss is:0.10614021\n",
            "This is the iter 6586, the d1 loss is: 3012.9375, the d2 loss is: -3284.8984, the g loss is: 3269.6797, the ae loss is: 0.0056282105, the jacobian loss is:0.05886407\n",
            "This is the iter 6587, the d1 loss is: 2814.1406, the d2 loss is: -3097.1406, the g loss is: 3037.8984, the ae loss is: 0.005929797, the jacobian loss is:0.11186789\n",
            "This is the iter 6588, the d1 loss is: 2902.2266, the d2 loss is: -3164.1953, the g loss is: 3144.8594, the ae loss is: 0.006556751, the jacobian loss is:0.13041511\n",
            "This is the iter 6589, the d1 loss is: 2893.5625, the d2 loss is: -3157.125, the g loss is: 3150.7344, the ae loss is: 0.0073475298, the jacobian loss is:0.1399195\n",
            "This is the iter 6590, the d1 loss is: 2931.7188, the d2 loss is: -3204.2969, the g loss is: 3238.086, the ae loss is: 0.007831419, the jacobian loss is:0.11825875\n",
            "This is the iter 6591, the d1 loss is: 2877.1094, the d2 loss is: -3165.0547, the g loss is: 3212.4766, the ae loss is: 0.0068423664, the jacobian loss is:0.10662231\n",
            "This is the iter 6592, the d1 loss is: 3276.3594, the d2 loss is: -3575.6328, the g loss is: 3585.2969, the ae loss is: 0.006899517, the jacobian loss is:0.117413744\n",
            "This is the iter 6593, the d1 loss is: 2953.7031, the d2 loss is: -3204.9766, the g loss is: 3179.2734, the ae loss is: 0.0060318923, the jacobian loss is:0.10368378\n",
            "This is the iter 6594, the d1 loss is: 2993.8594, the d2 loss is: -3270.8203, the g loss is: 3286.4844, the ae loss is: 0.0040732147, the jacobian loss is:0.10398848\n",
            "This is the iter 6595, the d1 loss is: 2981.5, the d2 loss is: -3220.5938, the g loss is: 3281.9688, the ae loss is: 0.0049479445, the jacobian loss is:0.10513949\n",
            "This is the iter 6596, the d1 loss is: 2902.3984, the d2 loss is: -3137.289, the g loss is: 3191.7812, the ae loss is: 0.007308971, the jacobian loss is:0.085971765\n",
            "This is the iter 6597, the d1 loss is: 2846.3281, the d2 loss is: -3147.4062, the g loss is: 3177.836, the ae loss is: 0.0057754717, the jacobian loss is:0.08854186\n",
            "This is the iter 6598, the d1 loss is: 2938.7031, the d2 loss is: -3198.8125, the g loss is: 3183.8594, the ae loss is: 0.0065656635, the jacobian loss is:0.19666283\n",
            "This is the iter 6599, the d1 loss is: 2939.1719, the d2 loss is: -3205.8047, the g loss is: 3184.461, the ae loss is: 0.0074850386, the jacobian loss is:0.09817679\n",
            "This is the iter 6600, the d1 loss is: 2896.9922, the d2 loss is: -3172.4688, the g loss is: 3198.8906, the ae loss is: 0.010403497, the jacobian loss is:0.07856529\n",
            "0.2167209\n",
            "0.8244188\n",
            "This is the iter 6601, the d1 loss is: 2943.9219, the d2 loss is: -3217.3672, the g loss is: 3185.125, the ae loss is: 0.005878226, the jacobian loss is:0.107229635\n",
            "This is the iter 6602, the d1 loss is: 2897.3672, the d2 loss is: -3118.7344, the g loss is: 3171.1719, the ae loss is: 0.0060939537, the jacobian loss is:0.11330148\n",
            "This is the iter 6603, the d1 loss is: 2675.2344, the d2 loss is: -2942.0781, the g loss is: 2893.2344, the ae loss is: 0.0067773485, the jacobian loss is:0.12009377\n",
            "This is the iter 6604, the d1 loss is: 2836.2656, the d2 loss is: -3108.664, the g loss is: 3121.039, the ae loss is: 0.0060153203, the jacobian loss is:0.09668842\n",
            "This is the iter 6605, the d1 loss is: 2901.5938, the d2 loss is: -3170.0625, the g loss is: 3144.6797, the ae loss is: 0.007043657, the jacobian loss is:0.13214472\n",
            "This is the iter 6606, the d1 loss is: 2913.4531, the d2 loss is: -3199.3672, the g loss is: 3143.3203, the ae loss is: 0.008277565, the jacobian loss is:0.11596706\n",
            "This is the iter 6607, the d1 loss is: 2977.75, the d2 loss is: -3241.586, the g loss is: 3196.5, the ae loss is: 0.0065042125, the jacobian loss is:0.098143786\n",
            "This is the iter 6608, the d1 loss is: 2857.8281, the d2 loss is: -3114.414, the g loss is: 3067.5625, the ae loss is: 0.0062690666, the jacobian loss is:0.085965954\n",
            "This is the iter 6609, the d1 loss is: 2906.4688, the d2 loss is: -3177.0234, the g loss is: 3148.0625, the ae loss is: 0.007325854, the jacobian loss is:0.105817996\n",
            "This is the iter 6610, the d1 loss is: 3035.7812, the d2 loss is: -3312.289, the g loss is: 3300.5312, the ae loss is: 0.0066627692, the jacobian loss is:0.07248128\n",
            "This is the iter 6611, the d1 loss is: 3270.9688, the d2 loss is: -3522.1094, the g loss is: 3520.1016, the ae loss is: 0.0066097854, the jacobian loss is:0.13675141\n",
            "This is the iter 6612, the d1 loss is: 2958.6094, the d2 loss is: -3213.5156, the g loss is: 3255.75, the ae loss is: 0.006994188, the jacobian loss is:0.08344629\n",
            "This is the iter 6613, the d1 loss is: 2946.2969, the d2 loss is: -3192.789, the g loss is: 3220.3047, the ae loss is: 0.0070716403, the jacobian loss is:0.07606182\n",
            "This is the iter 6614, the d1 loss is: 2942.9531, the d2 loss is: -3188.25, the g loss is: 3234.9922, the ae loss is: 0.00539869, the jacobian loss is:0.10750201\n",
            "This is the iter 6615, the d1 loss is: 2921.0781, the d2 loss is: -3207.125, the g loss is: 3167.125, the ae loss is: 0.006177636, the jacobian loss is:0.084112726\n",
            "This is the iter 6616, the d1 loss is: 2964.9922, the d2 loss is: -3217.3594, the g loss is: 3155.3438, the ae loss is: 0.006123048, the jacobian loss is:0.09113465\n",
            "This is the iter 6617, the d1 loss is: 2931.1875, the d2 loss is: -3219.0078, the g loss is: 3274.7031, the ae loss is: 0.006398496, the jacobian loss is:0.13455106\n",
            "This is the iter 6618, the d1 loss is: 3018.6172, the d2 loss is: -3283.5781, the g loss is: 3289.4062, the ae loss is: 0.007757863, the jacobian loss is:0.121879116\n",
            "This is the iter 6619, the d1 loss is: 2994.289, the d2 loss is: -3256.5234, the g loss is: 3248.6719, the ae loss is: 0.0060328366, the jacobian loss is:0.11620625\n",
            "This is the iter 6620, the d1 loss is: 2960.7578, the d2 loss is: -3227.5078, the g loss is: 3253.5078, the ae loss is: 0.006614011, the jacobian loss is:0.08115017\n",
            "This is the iter 6621, the d1 loss is: 2916.6406, the d2 loss is: -3201.961, the g loss is: 3235.0469, the ae loss is: 0.0046466216, the jacobian loss is:0.10852059\n",
            "This is the iter 6622, the d1 loss is: 2995.7188, the d2 loss is: -3231.5703, the g loss is: 3235.461, the ae loss is: 0.007535211, the jacobian loss is:0.14346343\n",
            "This is the iter 6623, the d1 loss is: 2925.789, the d2 loss is: -3198.539, the g loss is: 3244.2031, the ae loss is: 0.0067715836, the jacobian loss is:0.09476561\n",
            "This is the iter 6624, the d1 loss is: 2836.1719, the d2 loss is: -3090.7344, the g loss is: 3101.2266, the ae loss is: 0.0054643806, the jacobian loss is:0.106391005\n",
            "This is the iter 6625, the d1 loss is: 2883.664, the d2 loss is: -3091.1797, the g loss is: 3141.6797, the ae loss is: 0.0048894784, the jacobian loss is:0.075926594\n",
            "This is the iter 6626, the d1 loss is: 2922.1406, the d2 loss is: -3128.875, the g loss is: 3147.9688, the ae loss is: 0.0070675407, the jacobian loss is:0.08218005\n",
            "This is the iter 6627, the d1 loss is: 2976.3594, the d2 loss is: -3238.5547, the g loss is: 3234.8125, the ae loss is: 0.0065073436, the jacobian loss is:0.089654066\n",
            "This is the iter 6628, the d1 loss is: 2870.2031, the d2 loss is: -3126.2422, the g loss is: 3215.7969, the ae loss is: 0.005399168, the jacobian loss is:0.13527848\n",
            "This is the iter 6629, the d1 loss is: 2828.2656, the d2 loss is: -3118.7031, the g loss is: 3059.6406, the ae loss is: 0.0077125207, the jacobian loss is:0.10176108\n",
            "This is the iter 6630, the d1 loss is: 2944.4922, the d2 loss is: -3193.6953, the g loss is: 3152.8438, the ae loss is: 0.005637012, the jacobian loss is:0.10869292\n",
            "This is the iter 6631, the d1 loss is: 2977.9922, the d2 loss is: -3205.7344, the g loss is: 3253.3594, the ae loss is: 0.006400572, the jacobian loss is:0.10015864\n",
            "This is the iter 6632, the d1 loss is: 2926.961, the d2 loss is: -3168.3203, the g loss is: 3155.3906, the ae loss is: 0.0067666536, the jacobian loss is:0.093127124\n",
            "This is the iter 6633, the d1 loss is: 2998.4453, the d2 loss is: -3225.7812, the g loss is: 3210.0781, the ae loss is: 0.006729949, the jacobian loss is:0.095472805\n",
            "This is the iter 6634, the d1 loss is: 2813.0, the d2 loss is: -3091.5156, the g loss is: 3191.6875, the ae loss is: 0.005660108, the jacobian loss is:0.13115072\n",
            "This is the iter 6635, the d1 loss is: 2805.8047, the d2 loss is: -3082.5156, the g loss is: 3042.1016, the ae loss is: 0.0066375663, the jacobian loss is:0.09020899\n",
            "This is the iter 6636, the d1 loss is: 2919.3984, the d2 loss is: -3169.4766, the g loss is: 3211.4688, the ae loss is: 0.0052622324, the jacobian loss is:0.07903305\n",
            "This is the iter 6637, the d1 loss is: 2947.4844, the d2 loss is: -3217.9922, the g loss is: 3211.6797, the ae loss is: 0.0059880326, the jacobian loss is:0.09230457\n",
            "This is the iter 6638, the d1 loss is: 2982.2969, the d2 loss is: -3265.6328, the g loss is: 3181.7578, the ae loss is: 0.007229494, the jacobian loss is:0.097968556\n",
            "This is the iter 6639, the d1 loss is: 3129.1875, the d2 loss is: -3390.3281, the g loss is: 3392.6875, the ae loss is: 0.004724456, the jacobian loss is:0.09244964\n",
            "This is the iter 6640, the d1 loss is: 2989.3828, the d2 loss is: -3255.9219, the g loss is: 3230.3047, the ae loss is: 0.0070814444, the jacobian loss is:0.18851571\n",
            "This is the iter 6641, the d1 loss is: 2878.9062, the d2 loss is: -3182.7188, the g loss is: 3226.8672, the ae loss is: 0.006419752, the jacobian loss is:0.10745732\n",
            "This is the iter 6642, the d1 loss is: 2908.7812, the d2 loss is: -3181.7812, the g loss is: 3089.7031, the ae loss is: 0.0064918003, the jacobian loss is:0.10693801\n",
            "This is the iter 6643, the d1 loss is: 2966.2734, the d2 loss is: -3250.2578, the g loss is: 3240.6484, the ae loss is: 0.00648075, the jacobian loss is:0.09296087\n",
            "This is the iter 6644, the d1 loss is: 2944.1797, the d2 loss is: -3201.5703, the g loss is: 3239.7969, the ae loss is: 0.005916247, the jacobian loss is:0.09011322\n",
            "This is the iter 6645, the d1 loss is: 3280.5781, the d2 loss is: -3522.7734, the g loss is: 3501.1016, the ae loss is: 0.009996831, the jacobian loss is:0.10758074\n",
            "This is the iter 6646, the d1 loss is: 3092.7656, the d2 loss is: -3346.7266, the g loss is: 3447.7578, the ae loss is: 0.006580833, the jacobian loss is:0.093720324\n",
            "This is the iter 6647, the d1 loss is: 3015.0078, the d2 loss is: -3305.6328, the g loss is: 3251.375, the ae loss is: 0.007356889, the jacobian loss is:0.098185755\n",
            "This is the iter 6648, the d1 loss is: 2946.6953, the d2 loss is: -3204.7422, the g loss is: 3212.4453, the ae loss is: 0.0041840407, the jacobian loss is:0.082003206\n",
            "This is the iter 6649, the d1 loss is: 3034.3906, the d2 loss is: -3316.2656, the g loss is: 3326.1719, the ae loss is: 0.0080202585, the jacobian loss is:0.14301226\n",
            "This is the iter 6650, the d1 loss is: 2995.875, the d2 loss is: -3234.461, the g loss is: 3268.5469, the ae loss is: 0.0057597714, the jacobian loss is:0.102359466\n",
            "This is the iter 6651, the d1 loss is: 2877.625, the d2 loss is: -3161.289, the g loss is: 3175.7422, the ae loss is: 0.0045144656, the jacobian loss is:0.09684781\n",
            "This is the iter 6652, the d1 loss is: 2989.9844, the d2 loss is: -3243.0547, the g loss is: 3234.6328, the ae loss is: 0.006376392, the jacobian loss is:0.081226125\n",
            "This is the iter 6653, the d1 loss is: 2959.7344, the d2 loss is: -3235.5312, the g loss is: 3244.4922, the ae loss is: 0.0053391075, the jacobian loss is:0.1063922\n",
            "This is the iter 6654, the d1 loss is: 3072.5781, the d2 loss is: -3313.211, the g loss is: 3309.4375, the ae loss is: 0.0052999966, the jacobian loss is:0.10449238\n",
            "This is the iter 6655, the d1 loss is: 3107.8438, the d2 loss is: -3329.4844, the g loss is: 3325.2656, the ae loss is: 0.0038114272, the jacobian loss is:0.07103366\n",
            "This is the iter 6656, the d1 loss is: 2890.1953, the d2 loss is: -3153.5078, the g loss is: 3163.9297, the ae loss is: 0.0059103975, the jacobian loss is:0.14600492\n",
            "This is the iter 6657, the d1 loss is: 2950.461, the d2 loss is: -3245.5078, the g loss is: 3199.211, the ae loss is: 0.0049047535, the jacobian loss is:0.073074415\n",
            "This is the iter 6658, the d1 loss is: 2862.9531, the d2 loss is: -3119.625, the g loss is: 3201.2578, the ae loss is: 0.007006943, the jacobian loss is:0.08513827\n",
            "This is the iter 6659, the d1 loss is: 3010.25, the d2 loss is: -3258.9297, the g loss is: 3257.3438, the ae loss is: 0.007762503, the jacobian loss is:0.1463826\n",
            "This is the iter 6660, the d1 loss is: 2777.5078, the d2 loss is: -3035.6719, the g loss is: 3012.3906, the ae loss is: 0.008401713, the jacobian loss is:0.09005104\n",
            "This is the iter 6661, the d1 loss is: 2981.8047, the d2 loss is: -3236.0781, the g loss is: 3183.039, the ae loss is: 0.0062850313, the jacobian loss is:0.098848395\n",
            "This is the iter 6662, the d1 loss is: 2824.6016, the d2 loss is: -3094.5078, the g loss is: 3155.1875, the ae loss is: 0.0040711234, the jacobian loss is:0.075137876\n",
            "This is the iter 6663, the d1 loss is: 2815.3516, the d2 loss is: -3088.4453, the g loss is: 3120.8828, the ae loss is: 0.005717769, the jacobian loss is:0.10813713\n",
            "This is the iter 6664, the d1 loss is: 2924.836, the d2 loss is: -3218.5, the g loss is: 3225.961, the ae loss is: 0.0050828164, the jacobian loss is:0.09989973\n",
            "This is the iter 6665, the d1 loss is: 2789.75, the d2 loss is: -3055.5547, the g loss is: 3065.0938, the ae loss is: 0.005313647, the jacobian loss is:0.087149315\n",
            "This is the iter 6666, the d1 loss is: 2896.9375, the d2 loss is: -3145.1797, the g loss is: 3187.8828, the ae loss is: 0.007627615, the jacobian loss is:0.1165033\n",
            "This is the iter 6667, the d1 loss is: 3011.8672, the d2 loss is: -3290.5, the g loss is: 3272.7812, the ae loss is: 0.0062342496, the jacobian loss is:0.12083243\n",
            "This is the iter 6668, the d1 loss is: 2915.2031, the d2 loss is: -3166.9453, the g loss is: 3147.9844, the ae loss is: 0.0047475984, the jacobian loss is:0.08272511\n",
            "This is the iter 6669, the d1 loss is: 2901.0625, the d2 loss is: -3133.4062, the g loss is: 3170.6094, the ae loss is: 0.006014429, the jacobian loss is:0.09423545\n",
            "This is the iter 6670, the d1 loss is: 2872.3125, the d2 loss is: -3151.5469, the g loss is: 3184.1172, the ae loss is: 0.0053875265, the jacobian loss is:0.06798553\n",
            "This is the iter 6671, the d1 loss is: 2859.5078, the d2 loss is: -3104.5781, the g loss is: 3172.9844, the ae loss is: 0.005956035, the jacobian loss is:0.090781726\n",
            "This is the iter 6672, the d1 loss is: 2904.1094, the d2 loss is: -3146.4375, the g loss is: 3163.5078, the ae loss is: 0.007228338, the jacobian loss is:0.09273102\n",
            "This is the iter 6673, the d1 loss is: 2908.7422, the d2 loss is: -3142.3594, the g loss is: 3162.3672, the ae loss is: 0.005571399, the jacobian loss is:0.08259356\n",
            "This is the iter 6674, the d1 loss is: 2718.1562, the d2 loss is: -2971.664, the g loss is: 3005.9297, the ae loss is: 0.0070154658, the jacobian loss is:0.0864295\n",
            "This is the iter 6675, the d1 loss is: 2903.461, the d2 loss is: -3174.5156, the g loss is: 3094.6562, the ae loss is: 0.007986966, the jacobian loss is:0.1882644\n",
            "This is the iter 6676, the d1 loss is: 3130.6797, the d2 loss is: -3389.3125, the g loss is: 3380.3438, the ae loss is: 0.0060631637, the jacobian loss is:0.12817451\n",
            "This is the iter 6677, the d1 loss is: 2953.1484, the d2 loss is: -3190.4688, the g loss is: 3235.3984, the ae loss is: 0.004909651, the jacobian loss is:0.07271864\n",
            "This is the iter 6678, the d1 loss is: 2942.8594, the d2 loss is: -3245.6797, the g loss is: 3241.7188, the ae loss is: 0.006675906, the jacobian loss is:0.16635868\n",
            "This is the iter 6679, the d1 loss is: 2911.0156, the d2 loss is: -3146.8203, the g loss is: 3207.9844, the ae loss is: 0.0057000825, the jacobian loss is:0.1558906\n",
            "This is the iter 6680, the d1 loss is: 2808.3594, the d2 loss is: -3078.3047, the g loss is: 3033.6875, the ae loss is: 0.00676609, the jacobian loss is:0.088586554\n",
            "This is the iter 6681, the d1 loss is: 2904.3906, the d2 loss is: -3161.0078, the g loss is: 3160.6172, the ae loss is: 0.0074431123, the jacobian loss is:0.2018539\n",
            "This is the iter 6682, the d1 loss is: 2834.211, the d2 loss is: -3108.0312, the g loss is: 3093.8906, the ae loss is: 0.0045608315, the jacobian loss is:0.06902303\n",
            "This is the iter 6683, the d1 loss is: 2965.2031, the d2 loss is: -3199.164, the g loss is: 3200.7578, the ae loss is: 0.0067076115, the jacobian loss is:0.1022403\n",
            "This is the iter 6684, the d1 loss is: 2923.9219, the d2 loss is: -3175.375, the g loss is: 3215.8906, the ae loss is: 0.0066838763, the jacobian loss is:0.099385366\n",
            "This is the iter 6685, the d1 loss is: 2938.961, the d2 loss is: -3196.9844, the g loss is: 3130.1875, the ae loss is: 0.006288519, the jacobian loss is:0.13396657\n",
            "This is the iter 6686, the d1 loss is: 2893.0781, the d2 loss is: -3148.711, the g loss is: 3146.2812, the ae loss is: 0.0064771622, the jacobian loss is:0.09219692\n",
            "This is the iter 6687, the d1 loss is: 2800.0312, the d2 loss is: -3058.211, the g loss is: 3072.2031, the ae loss is: 0.005988989, the jacobian loss is:0.15811408\n",
            "This is the iter 6688, the d1 loss is: 2985.2812, the d2 loss is: -3260.7344, the g loss is: 3241.7656, the ae loss is: 0.0050895154, the jacobian loss is:0.096202545\n",
            "This is the iter 6689, the d1 loss is: 2968.0156, the d2 loss is: -3193.3906, the g loss is: 3197.1172, the ae loss is: 0.005135741, the jacobian loss is:0.08719242\n",
            "This is the iter 6690, the d1 loss is: 2991.039, the d2 loss is: -3230.7578, the g loss is: 3160.8281, the ae loss is: 0.0067427033, the jacobian loss is:0.12342928\n",
            "This is the iter 6691, the d1 loss is: 2993.25, the d2 loss is: -3260.9062, the g loss is: 3319.625, the ae loss is: 0.0060405466, the jacobian loss is:0.18080992\n",
            "This is the iter 6692, the d1 loss is: 3019.6484, the d2 loss is: -3277.914, the g loss is: 3249.1094, the ae loss is: 0.007105784, the jacobian loss is:0.11612574\n",
            "This is the iter 6693, the d1 loss is: 3007.5938, the d2 loss is: -3268.289, the g loss is: 3263.3125, the ae loss is: 0.007215342, the jacobian loss is:0.14722672\n",
            "This is the iter 6694, the d1 loss is: 2955.4062, the d2 loss is: -3214.0625, the g loss is: 3225.6719, the ae loss is: 0.008427132, the jacobian loss is:0.15633845\n",
            "This is the iter 6695, the d1 loss is: 2974.125, the d2 loss is: -3219.4375, the g loss is: 3182.4062, the ae loss is: 0.0063400166, the jacobian loss is:0.1340281\n",
            "This is the iter 6696, the d1 loss is: 2980.0234, the d2 loss is: -3200.2266, the g loss is: 3244.6875, the ae loss is: 0.005136434, the jacobian loss is:0.20985125\n",
            "This is the iter 6697, the d1 loss is: 2976.4297, the d2 loss is: -3238.0312, the g loss is: 3227.3906, the ae loss is: 0.0058374805, the jacobian loss is:0.13249466\n",
            "This is the iter 6698, the d1 loss is: 2765.539, the d2 loss is: -2994.125, the g loss is: 3057.8281, the ae loss is: 0.0058603995, the jacobian loss is:0.14622313\n",
            "This is the iter 6699, the d1 loss is: 2891.5, the d2 loss is: -3166.586, the g loss is: 3135.0, the ae loss is: 0.007297115, the jacobian loss is:0.13775903\n",
            "This is the iter 6700, the d1 loss is: 2923.8281, the d2 loss is: -3200.3516, the g loss is: 3215.2266, the ae loss is: 0.006303776, the jacobian loss is:0.09301645\n",
            "0.21345387\n",
            "0.8138941\n",
            "This is the iter 6701, the d1 loss is: 2922.1016, the d2 loss is: -3199.125, the g loss is: 3245.836, the ae loss is: 0.0046013286, the jacobian loss is:0.17544945\n",
            "This is the iter 6702, the d1 loss is: 2739.9844, the d2 loss is: -2998.9922, the g loss is: 3031.4062, the ae loss is: 0.0047274395, the jacobian loss is:0.11594265\n",
            "This is the iter 6703, the d1 loss is: 2806.1953, the d2 loss is: -3076.5156, the g loss is: 3111.6094, the ae loss is: 0.0045580063, the jacobian loss is:0.0961057\n",
            "This is the iter 6704, the d1 loss is: 2897.2188, the d2 loss is: -3159.8906, the g loss is: 3112.5625, the ae loss is: 0.008512406, the jacobian loss is:0.12274545\n",
            "This is the iter 6705, the d1 loss is: 2924.5938, the d2 loss is: -3196.4844, the g loss is: 3186.3906, the ae loss is: 0.0056220302, the jacobian loss is:0.23684208\n",
            "This is the iter 6706, the d1 loss is: 3005.375, the d2 loss is: -3279.75, the g loss is: 3265.4922, the ae loss is: 0.007103918, the jacobian loss is:0.27111742\n",
            "This is the iter 6707, the d1 loss is: 2963.711, the d2 loss is: -3240.0469, the g loss is: 3223.6094, the ae loss is: 0.0056868116, the jacobian loss is:0.1437298\n",
            "This is the iter 6708, the d1 loss is: 2866.0625, the d2 loss is: -3145.8672, the g loss is: 3132.5078, the ae loss is: 0.006343933, the jacobian loss is:0.14118388\n",
            "This is the iter 6709, the d1 loss is: 2792.039, the d2 loss is: -3025.7266, the g loss is: 3026.6016, the ae loss is: 0.0064918236, the jacobian loss is:0.26611412\n",
            "This is the iter 6710, the d1 loss is: 2908.8281, the d2 loss is: -3151.875, the g loss is: 3174.3672, the ae loss is: 0.005002754, the jacobian loss is:0.19223173\n",
            "This is the iter 6711, the d1 loss is: 2834.9062, the d2 loss is: -3085.9922, the g loss is: 3115.4766, the ae loss is: 0.0052747773, the jacobian loss is:0.22152308\n",
            "This is the iter 6712, the d1 loss is: 2811.9297, the d2 loss is: -3056.2969, the g loss is: 3049.0781, the ae loss is: 0.0053022206, the jacobian loss is:0.2389685\n",
            "This is the iter 6713, the d1 loss is: 2907.3984, the d2 loss is: -3187.25, the g loss is: 3200.539, the ae loss is: 0.007699228, the jacobian loss is:0.17496827\n",
            "This is the iter 6714, the d1 loss is: 2948.961, the d2 loss is: -3211.6875, the g loss is: 3200.6875, the ae loss is: 0.0044696666, the jacobian loss is:0.15243599\n",
            "This is the iter 6715, the d1 loss is: 2855.9688, the d2 loss is: -3089.0625, the g loss is: 3129.0156, the ae loss is: 0.0067575816, the jacobian loss is:0.22028743\n",
            "This is the iter 6716, the d1 loss is: 3140.4531, the d2 loss is: -3406.8594, the g loss is: 3445.4219, the ae loss is: 0.0061069783, the jacobian loss is:0.26299766\n",
            "This is the iter 6717, the d1 loss is: 2824.7656, the d2 loss is: -3075.0078, the g loss is: 3004.2188, the ae loss is: 0.0063876524, the jacobian loss is:0.31727114\n",
            "This is the iter 6718, the d1 loss is: 2951.9062, the d2 loss is: -3191.9531, the g loss is: 3168.3281, the ae loss is: 0.007943428, the jacobian loss is:0.32679877\n",
            "This is the iter 6719, the d1 loss is: 2850.7422, the d2 loss is: -3124.7188, the g loss is: 3150.7344, the ae loss is: 0.006552711, the jacobian loss is:0.19288495\n",
            "This is the iter 6720, the d1 loss is: 2894.0781, the d2 loss is: -3115.1094, the g loss is: 3135.586, the ae loss is: 0.005068452, the jacobian loss is:0.19587384\n",
            "This is the iter 6721, the d1 loss is: 2855.0547, the d2 loss is: -3094.7031, the g loss is: 3152.539, the ae loss is: 0.006984453, the jacobian loss is:0.2851993\n",
            "This is the iter 6722, the d1 loss is: 2830.3438, the d2 loss is: -3091.625, the g loss is: 3138.0469, the ae loss is: 0.0056416215, the jacobian loss is:0.26499403\n",
            "This is the iter 6723, the d1 loss is: 2990.875, the d2 loss is: -3220.6406, the g loss is: 3231.2344, the ae loss is: 0.0047110976, the jacobian loss is:0.30228502\n",
            "This is the iter 6724, the d1 loss is: 2904.2734, the d2 loss is: -3183.9531, the g loss is: 3161.75, the ae loss is: 0.005991037, the jacobian loss is:0.24123493\n",
            "This is the iter 6725, the d1 loss is: 2989.9844, the d2 loss is: -3203.6797, the g loss is: 3171.961, the ae loss is: 0.005198653, the jacobian loss is:0.23989755\n",
            "This is the iter 6726, the d1 loss is: 2922.4531, the d2 loss is: -3162.8203, the g loss is: 3137.8203, the ae loss is: 0.0051505216, the jacobian loss is:0.1831184\n",
            "This is the iter 6727, the d1 loss is: 2740.6172, the d2 loss is: -3016.9844, the g loss is: 3001.539, the ae loss is: 0.006600461, the jacobian loss is:0.3562805\n",
            "This is the iter 6728, the d1 loss is: 2901.9297, the d2 loss is: -3125.0469, the g loss is: 3133.8594, the ae loss is: 0.006366158, the jacobian loss is:0.14736983\n",
            "This is the iter 6729, the d1 loss is: 2936.3281, the d2 loss is: -3176.7266, the g loss is: 3174.2344, the ae loss is: 0.0067176446, the jacobian loss is:0.42385712\n",
            "This is the iter 6730, the d1 loss is: 2987.0, the d2 loss is: -3253.414, the g loss is: 3190.2969, the ae loss is: 0.005095445, the jacobian loss is:0.29078695\n",
            "This is the iter 6731, the d1 loss is: 2921.9219, the d2 loss is: -3164.0078, the g loss is: 3200.8438, the ae loss is: 0.007721593, the jacobian loss is:0.2843136\n",
            "This is the iter 6732, the d1 loss is: 2794.4688, the d2 loss is: -3054.789, the g loss is: 3080.0156, the ae loss is: 0.00665343, the jacobian loss is:0.21598169\n",
            "This is the iter 6733, the d1 loss is: 2934.0781, the d2 loss is: -3192.4453, the g loss is: 3218.125, the ae loss is: 0.005516448, the jacobian loss is:0.2407866\n",
            "This is the iter 6734, the d1 loss is: 2883.2031, the d2 loss is: -3153.7422, the g loss is: 3134.3906, the ae loss is: 0.0055416487, the jacobian loss is:0.24568011\n",
            "This is the iter 6735, the d1 loss is: 2926.336, the d2 loss is: -3190.0781, the g loss is: 3180.6875, the ae loss is: 0.006346098, the jacobian loss is:0.22893353\n",
            "This is the iter 6736, the d1 loss is: 2892.6875, the d2 loss is: -3156.2422, the g loss is: 3139.0625, the ae loss is: 0.009149507, the jacobian loss is:0.21730599\n",
            "This is the iter 6737, the d1 loss is: 2905.5781, the d2 loss is: -3176.6875, the g loss is: 3174.9062, the ae loss is: 0.008622383, the jacobian loss is:0.32796454\n",
            "This is the iter 6738, the d1 loss is: 2874.4453, the d2 loss is: -3090.9922, the g loss is: 3046.0156, the ae loss is: 0.007016155, the jacobian loss is:0.19437046\n",
            "This is the iter 6739, the d1 loss is: 2913.9531, the d2 loss is: -3123.75, the g loss is: 3116.2266, the ae loss is: 0.0059517873, the jacobian loss is:0.26251757\n",
            "This is the iter 6740, the d1 loss is: 2912.1328, the d2 loss is: -3152.6016, the g loss is: 3202.0312, the ae loss is: 0.0069096396, the jacobian loss is:0.21688642\n",
            "This is the iter 6741, the d1 loss is: 2766.1719, the d2 loss is: -3034.711, the g loss is: 3015.0547, the ae loss is: 0.0055761645, the jacobian loss is:0.26704797\n",
            "This is the iter 6742, the d1 loss is: 2958.9219, the d2 loss is: -3224.5156, the g loss is: 3199.8906, the ae loss is: 0.005876528, the jacobian loss is:0.22712536\n",
            "This is the iter 6743, the d1 loss is: 2887.6875, the d2 loss is: -3137.961, the g loss is: 3120.3125, the ae loss is: 0.006327757, the jacobian loss is:0.26052415\n",
            "This is the iter 6744, the d1 loss is: 2817.8281, the d2 loss is: -3118.164, the g loss is: 3087.2422, the ae loss is: 0.00848203, the jacobian loss is:0.21107426\n",
            "This is the iter 6745, the d1 loss is: 2767.8984, the d2 loss is: -3047.1016, the g loss is: 3084.586, the ae loss is: 0.006510364, the jacobian loss is:0.24615166\n",
            "This is the iter 6746, the d1 loss is: 3188.2266, the d2 loss is: -3437.2031, the g loss is: 3450.539, the ae loss is: 0.0052964617, the jacobian loss is:0.208497\n",
            "This is the iter 6747, the d1 loss is: 2985.4688, the d2 loss is: -3270.3516, the g loss is: 3312.3828, the ae loss is: 0.0073183235, the jacobian loss is:0.26378798\n",
            "This is the iter 6748, the d1 loss is: 2782.289, the d2 loss is: -3005.25, the g loss is: 3018.1094, the ae loss is: 0.0072482405, the jacobian loss is:0.22552507\n",
            "This is the iter 6749, the d1 loss is: 2788.0312, the d2 loss is: -3023.7031, the g loss is: 3023.6328, the ae loss is: 0.0075306194, the jacobian loss is:0.22816318\n",
            "This is the iter 6750, the d1 loss is: 3151.289, the d2 loss is: -3408.9375, the g loss is: 3397.1172, the ae loss is: 0.0051735714, the jacobian loss is:0.16198124\n",
            "This is the iter 6751, the d1 loss is: 2922.3203, the d2 loss is: -3184.3125, the g loss is: 3206.789, the ae loss is: 0.008546812, the jacobian loss is:0.19648021\n",
            "This is the iter 6752, the d1 loss is: 2901.9453, the d2 loss is: -3127.0312, the g loss is: 3177.0781, the ae loss is: 0.005727304, the jacobian loss is:0.18158324\n",
            "This is the iter 6753, the d1 loss is: 3305.125, the d2 loss is: -3540.4766, the g loss is: 3508.3828, the ae loss is: 0.005451901, the jacobian loss is:0.20358166\n",
            "This is the iter 6754, the d1 loss is: 2934.961, the d2 loss is: -3169.7266, the g loss is: 3163.7188, the ae loss is: 0.0060351444, the jacobian loss is:0.18711473\n",
            "This is the iter 6755, the d1 loss is: 3073.2656, the d2 loss is: -3296.8438, the g loss is: 3308.2266, the ae loss is: 0.008580934, the jacobian loss is:0.23201054\n",
            "This is the iter 6756, the d1 loss is: 2934.9062, the d2 loss is: -3185.8516, the g loss is: 3168.6875, the ae loss is: 0.0071339733, the jacobian loss is:0.20419829\n",
            "This is the iter 6757, the d1 loss is: 3151.6172, the d2 loss is: -3371.3125, the g loss is: 3341.5938, the ae loss is: 0.00856064, the jacobian loss is:0.26258504\n",
            "This is the iter 6758, the d1 loss is: 3184.4922, the d2 loss is: -3444.0156, the g loss is: 3480.0312, the ae loss is: 0.006995188, the jacobian loss is:0.15963651\n",
            "This is the iter 6759, the d1 loss is: 3357.2578, the d2 loss is: -3615.086, the g loss is: 3606.6562, the ae loss is: 0.0037357537, the jacobian loss is:0.15106475\n",
            "This is the iter 6760, the d1 loss is: 2759.2656, the d2 loss is: -3021.375, the g loss is: 3004.2578, the ae loss is: 0.008604423, the jacobian loss is:0.20530078\n",
            "This is the iter 6761, the d1 loss is: 3229.0547, the d2 loss is: -3447.5078, the g loss is: 3471.3906, the ae loss is: 0.006152926, the jacobian loss is:0.16329026\n",
            "This is the iter 6762, the d1 loss is: 2986.3281, the d2 loss is: -3262.3594, the g loss is: 3261.3125, the ae loss is: 0.0051384885, the jacobian loss is:0.18069549\n",
            "This is the iter 6763, the d1 loss is: 2840.2344, the d2 loss is: -3085.4922, the g loss is: 3052.0156, the ae loss is: 0.008236582, the jacobian loss is:0.24081767\n",
            "This is the iter 6764, the d1 loss is: 3050.1953, the d2 loss is: -3271.0469, the g loss is: 3228.0938, the ae loss is: 0.008029556, the jacobian loss is:0.19141197\n",
            "This is the iter 6765, the d1 loss is: 2970.9844, the d2 loss is: -3225.164, the g loss is: 3193.1719, the ae loss is: 0.0077951043, the jacobian loss is:0.18664016\n",
            "This is the iter 6766, the d1 loss is: 3030.6328, the d2 loss is: -3254.3047, the g loss is: 3207.8828, the ae loss is: 0.006040258, the jacobian loss is:0.21104315\n",
            "This is the iter 6767, the d1 loss is: 2987.5469, the d2 loss is: -3268.0781, the g loss is: 3192.5781, the ae loss is: 0.008176077, the jacobian loss is:0.20701936\n",
            "This is the iter 6768, the d1 loss is: 2988.2188, the d2 loss is: -3260.9062, the g loss is: 3258.0078, the ae loss is: 0.004567395, the jacobian loss is:0.17791131\n",
            "This is the iter 6769, the d1 loss is: 3023.5625, the d2 loss is: -3245.2266, the g loss is: 3258.0938, the ae loss is: 0.0064912927, the jacobian loss is:0.16944468\n",
            "This is the iter 6770, the d1 loss is: 2893.8203, the d2 loss is: -3139.7266, the g loss is: 3206.3594, the ae loss is: 0.0055915546, the jacobian loss is:0.17321761\n",
            "This is the iter 6771, the d1 loss is: 3032.2188, the d2 loss is: -3295.9453, the g loss is: 3254.2969, the ae loss is: 0.0079926625, the jacobian loss is:0.16782063\n",
            "This is the iter 6772, the d1 loss is: 2993.6562, the d2 loss is: -3254.9922, the g loss is: 3243.3438, the ae loss is: 0.007821379, the jacobian loss is:0.20151708\n",
            "This is the iter 6773, the d1 loss is: 2834.4531, the d2 loss is: -3050.8281, the g loss is: 3058.9688, the ae loss is: 0.006725275, the jacobian loss is:0.1488685\n",
            "This is the iter 6774, the d1 loss is: 2893.7656, the d2 loss is: -3164.4453, the g loss is: 3080.6484, the ae loss is: 0.00698112, the jacobian loss is:0.14836095\n",
            "This is the iter 6775, the d1 loss is: 2914.9531, the d2 loss is: -3154.25, the g loss is: 3165.4688, the ae loss is: 0.011282657, the jacobian loss is:0.22916806\n",
            "This is the iter 6776, the d1 loss is: 3008.3281, the d2 loss is: -3283.4844, the g loss is: 3291.5703, the ae loss is: 0.00919462, the jacobian loss is:0.15833732\n",
            "This is the iter 6777, the d1 loss is: 2851.6719, the d2 loss is: -3093.6094, the g loss is: 3061.3438, the ae loss is: 0.0072521297, the jacobian loss is:0.1863202\n",
            "This is the iter 6778, the d1 loss is: 3008.8828, the d2 loss is: -3234.6875, the g loss is: 3207.6797, the ae loss is: 0.006685478, the jacobian loss is:0.179604\n",
            "This is the iter 6779, the d1 loss is: 3017.3438, the d2 loss is: -3233.6328, the g loss is: 3226.8672, the ae loss is: 0.008741681, the jacobian loss is:0.18424225\n",
            "This is the iter 6780, the d1 loss is: 2957.4766, the d2 loss is: -3195.4766, the g loss is: 3131.6328, the ae loss is: 0.008408621, the jacobian loss is:0.16349436\n",
            "This is the iter 6781, the d1 loss is: 2982.2969, the d2 loss is: -3192.0469, the g loss is: 3160.4375, the ae loss is: 0.0067413435, the jacobian loss is:0.18305986\n",
            "This is the iter 6782, the d1 loss is: 2881.3438, the d2 loss is: -3146.414, the g loss is: 3156.4453, the ae loss is: 0.0068080667, the jacobian loss is:0.16181228\n",
            "This is the iter 6783, the d1 loss is: 2982.3594, the d2 loss is: -3236.3125, the g loss is: 3252.1172, the ae loss is: 0.007993869, the jacobian loss is:0.14399518\n",
            "This is the iter 6784, the d1 loss is: 2816.1719, the d2 loss is: -3056.4531, the g loss is: 3054.6328, the ae loss is: 0.0055251685, the jacobian loss is:0.13729268\n",
            "This is the iter 6785, the d1 loss is: 3237.8828, the d2 loss is: -3458.2578, the g loss is: 3521.3281, the ae loss is: 0.007436555, the jacobian loss is:0.15799937\n",
            "This is the iter 6786, the d1 loss is: 2995.25, the d2 loss is: -3240.5469, the g loss is: 3216.9531, the ae loss is: 0.0064886706, the jacobian loss is:0.18106711\n",
            "This is the iter 6787, the d1 loss is: 2982.3125, the d2 loss is: -3233.875, the g loss is: 3193.1328, the ae loss is: 0.005894138, the jacobian loss is:0.14702548\n",
            "This is the iter 6788, the d1 loss is: 2758.375, the d2 loss is: -3006.2656, the g loss is: 2974.4688, the ae loss is: 0.008086014, the jacobian loss is:0.18088754\n",
            "This is the iter 6789, the d1 loss is: 2947.3125, the d2 loss is: -3212.8516, the g loss is: 3222.961, the ae loss is: 0.0070263054, the jacobian loss is:0.15579389\n",
            "This is the iter 6790, the d1 loss is: 2937.5312, the d2 loss is: -3198.2578, the g loss is: 3155.5469, the ae loss is: 0.012164529, the jacobian loss is:0.1594835\n",
            "This is the iter 6791, the d1 loss is: 3021.7344, the d2 loss is: -3230.5703, the g loss is: 3210.6875, the ae loss is: 0.007412103, the jacobian loss is:0.13844123\n",
            "This is the iter 6792, the d1 loss is: 3016.6016, the d2 loss is: -3258.8828, the g loss is: 3232.8594, the ae loss is: 0.008265969, the jacobian loss is:0.15345767\n",
            "This is the iter 6793, the d1 loss is: 3037.961, the d2 loss is: -3338.1562, the g loss is: 3357.9375, the ae loss is: 0.005917102, the jacobian loss is:0.12988229\n",
            "This is the iter 6794, the d1 loss is: 2945.8516, the d2 loss is: -3207.211, the g loss is: 3197.789, the ae loss is: 0.0074864533, the jacobian loss is:0.16382542\n",
            "This is the iter 6795, the d1 loss is: 2912.336, the d2 loss is: -3160.6094, the g loss is: 3116.2344, the ae loss is: 0.0061867433, the jacobian loss is:0.18471168\n",
            "This is the iter 6796, the d1 loss is: 2932.664, the d2 loss is: -3210.0781, the g loss is: 3193.2969, the ae loss is: 0.0049583763, the jacobian loss is:0.16329409\n",
            "This is the iter 6797, the d1 loss is: 3089.5625, the d2 loss is: -3332.6953, the g loss is: 3350.7578, the ae loss is: 0.007366002, the jacobian loss is:0.1380409\n",
            "This is the iter 6798, the d1 loss is: 2968.9766, the d2 loss is: -3204.461, the g loss is: 3183.3984, the ae loss is: 0.0058502536, the jacobian loss is:0.16630691\n",
            "This is the iter 6799, the d1 loss is: 2839.5312, the d2 loss is: -3097.1016, the g loss is: 3186.961, the ae loss is: 0.008362154, the jacobian loss is:0.16787755\n",
            "This is the iter 6800, the d1 loss is: 2944.1406, the d2 loss is: -3203.4062, the g loss is: 3168.6328, the ae loss is: 0.0073634745, the jacobian loss is:0.13881725\n",
            "0.22104403\n",
            "0.8571012\n",
            "This is the iter 6801, the d1 loss is: 2934.0625, the d2 loss is: -3210.5938, the g loss is: 3225.9453, the ae loss is: 0.007838477, the jacobian loss is:0.15826261\n",
            "This is the iter 6802, the d1 loss is: 2856.6719, the d2 loss is: -3103.6484, the g loss is: 3071.2344, the ae loss is: 0.0071858405, the jacobian loss is:0.11395728\n",
            "This is the iter 6803, the d1 loss is: 2895.2031, the d2 loss is: -3126.375, the g loss is: 3138.0781, the ae loss is: 0.006917561, the jacobian loss is:0.18503346\n",
            "This is the iter 6804, the d1 loss is: 2909.4453, the d2 loss is: -3174.4062, the g loss is: 3189.8125, the ae loss is: 0.0051883184, the jacobian loss is:0.19411136\n",
            "This is the iter 6805, the d1 loss is: 2859.5, the d2 loss is: -3074.4297, the g loss is: 3064.9531, the ae loss is: 0.008773111, the jacobian loss is:0.19352777\n",
            "This is the iter 6806, the d1 loss is: 2923.5781, the d2 loss is: -3132.7656, the g loss is: 3129.3281, the ae loss is: 0.0073172436, the jacobian loss is:0.1704661\n",
            "This is the iter 6807, the d1 loss is: 2981.1406, the d2 loss is: -3226.6406, the g loss is: 3214.3828, the ae loss is: 0.0055749584, the jacobian loss is:0.12654628\n",
            "This is the iter 6808, the d1 loss is: 3194.1172, the d2 loss is: -3447.0703, the g loss is: 3428.914, the ae loss is: 0.007793418, the jacobian loss is:0.21859702\n",
            "This is the iter 6809, the d1 loss is: 2814.4219, the d2 loss is: -3079.375, the g loss is: 3082.875, the ae loss is: 0.005500347, the jacobian loss is:0.121856585\n",
            "This is the iter 6810, the d1 loss is: 3015.9297, the d2 loss is: -3255.1094, the g loss is: 3189.0312, the ae loss is: 0.007905487, the jacobian loss is:0.15838559\n",
            "This is the iter 6811, the d1 loss is: 2934.6094, the d2 loss is: -3177.6875, the g loss is: 3183.6406, the ae loss is: 0.0056467988, the jacobian loss is:0.14539312\n",
            "This is the iter 6812, the d1 loss is: 3011.4766, the d2 loss is: -3242.1953, the g loss is: 3221.3281, the ae loss is: 0.005979382, the jacobian loss is:0.1667611\n",
            "This is the iter 6813, the d1 loss is: 2778.2734, the d2 loss is: -3034.5156, the g loss is: 3077.1719, the ae loss is: 0.0077444557, the jacobian loss is:0.12932691\n",
            "This is the iter 6814, the d1 loss is: 2823.0469, the d2 loss is: -3058.6328, the g loss is: 3065.4688, the ae loss is: 0.0056389663, the jacobian loss is:0.20043957\n",
            "This is the iter 6815, the d1 loss is: 2938.6484, the d2 loss is: -3175.3516, the g loss is: 3234.8906, the ae loss is: 0.0054250928, the jacobian loss is:0.11548586\n",
            "This is the iter 6816, the d1 loss is: 2872.8594, the d2 loss is: -3092.2578, the g loss is: 3145.8984, the ae loss is: 0.008047574, the jacobian loss is:0.10187007\n",
            "This is the iter 6817, the d1 loss is: 2933.7812, the d2 loss is: -3207.8984, the g loss is: 3236.1953, the ae loss is: 0.0078143515, the jacobian loss is:0.1418216\n",
            "This is the iter 6818, the d1 loss is: 2875.3906, the d2 loss is: -3151.4219, the g loss is: 3180.9297, the ae loss is: 0.009397105, the jacobian loss is:0.10698372\n",
            "This is the iter 6819, the d1 loss is: 2995.0156, the d2 loss is: -3275.9219, the g loss is: 3245.2031, the ae loss is: 0.007747428, the jacobian loss is:0.13295376\n",
            "This is the iter 6820, the d1 loss is: 2955.539, the d2 loss is: -3206.3828, the g loss is: 3188.3594, the ae loss is: 0.005062744, the jacobian loss is:0.12630036\n",
            "This is the iter 6821, the d1 loss is: 2886.789, the d2 loss is: -3088.125, the g loss is: 3182.3906, the ae loss is: 0.0074976133, the jacobian loss is:0.13532043\n",
            "This is the iter 6822, the d1 loss is: 2920.2031, the d2 loss is: -3175.3672, the g loss is: 3187.289, the ae loss is: 0.0056760437, the jacobian loss is:0.14480034\n",
            "This is the iter 6823, the d1 loss is: 2911.1016, the d2 loss is: -3131.6328, the g loss is: 3206.5469, the ae loss is: 0.00789976, the jacobian loss is:0.16188738\n",
            "This is the iter 6824, the d1 loss is: 2961.961, the d2 loss is: -3187.4453, the g loss is: 3206.1562, the ae loss is: 0.0055354014, the jacobian loss is:0.15777671\n",
            "This is the iter 6825, the d1 loss is: 2896.711, the d2 loss is: -3179.3438, the g loss is: 3148.8438, the ae loss is: 0.00772374, the jacobian loss is:0.17908643\n",
            "This is the iter 6826, the d1 loss is: 3147.9766, the d2 loss is: -3394.7969, the g loss is: 3360.8594, the ae loss is: 0.0097898785, the jacobian loss is:0.17428945\n",
            "This is the iter 6827, the d1 loss is: 2911.414, the d2 loss is: -3176.6953, the g loss is: 3134.2422, the ae loss is: 0.01166849, the jacobian loss is:0.19523054\n",
            "This is the iter 6828, the d1 loss is: 2962.7266, the d2 loss is: -3258.6875, the g loss is: 3280.0469, the ae loss is: 0.0062198136, the jacobian loss is:0.15281713\n",
            "This is the iter 6829, the d1 loss is: 2960.6875, the d2 loss is: -3172.789, the g loss is: 3237.3125, the ae loss is: 0.0072552264, the jacobian loss is:0.13963376\n",
            "This is the iter 6830, the d1 loss is: 2891.1484, the d2 loss is: -3135.7188, the g loss is: 3201.8594, the ae loss is: 0.004841509, the jacobian loss is:0.136512\n",
            "This is the iter 6831, the d1 loss is: 2940.1797, the d2 loss is: -3218.3594, the g loss is: 3233.0156, the ae loss is: 0.0075727124, the jacobian loss is:0.14093378\n",
            "This is the iter 6832, the d1 loss is: 2891.2344, the d2 loss is: -3159.7422, the g loss is: 3191.4219, the ae loss is: 0.0077521945, the jacobian loss is:0.1116547\n",
            "This is the iter 6833, the d1 loss is: 2994.9297, the d2 loss is: -3244.7422, the g loss is: 3244.2578, the ae loss is: 0.0055114124, the jacobian loss is:0.100544944\n",
            "This is the iter 6834, the d1 loss is: 2956.0703, the d2 loss is: -3208.4688, the g loss is: 3180.8906, the ae loss is: 0.0076080384, the jacobian loss is:0.13622607\n",
            "This is the iter 6835, the d1 loss is: 3075.7656, the d2 loss is: -3325.8906, the g loss is: 3368.2656, the ae loss is: 0.0066992478, the jacobian loss is:0.1315701\n",
            "This is the iter 6836, the d1 loss is: 2850.5312, the d2 loss is: -3077.8047, the g loss is: 3175.0703, the ae loss is: 0.0070101465, the jacobian loss is:0.13899909\n",
            "This is the iter 6837, the d1 loss is: 2894.664, the d2 loss is: -3176.3594, the g loss is: 3171.1094, the ae loss is: 0.006453462, the jacobian loss is:0.14666978\n",
            "This is the iter 6838, the d1 loss is: 3079.7812, the d2 loss is: -3303.5469, the g loss is: 3302.6484, the ae loss is: 0.00788039, the jacobian loss is:0.18369879\n",
            "This is the iter 6839, the d1 loss is: 3320.8203, the d2 loss is: -3549.2266, the g loss is: 3516.8594, the ae loss is: 0.008368577, the jacobian loss is:0.11543886\n",
            "This is the iter 6840, the d1 loss is: 2959.9297, the d2 loss is: -3205.8125, the g loss is: 3175.1562, the ae loss is: 0.007166862, the jacobian loss is:0.11599963\n",
            "This is the iter 6841, the d1 loss is: 2957.5156, the d2 loss is: -3241.1875, the g loss is: 3225.125, the ae loss is: 0.005447938, the jacobian loss is:0.11240119\n",
            "This is the iter 6842, the d1 loss is: 2938.9844, the d2 loss is: -3182.7656, the g loss is: 3170.8203, the ae loss is: 0.007127928, the jacobian loss is:0.16888787\n",
            "This is the iter 6843, the d1 loss is: 2914.3203, the d2 loss is: -3184.2656, the g loss is: 3201.3281, the ae loss is: 0.006006373, the jacobian loss is:0.13329338\n",
            "This is the iter 6844, the d1 loss is: 3013.5078, the d2 loss is: -3243.8594, the g loss is: 3231.664, the ae loss is: 0.007725983, the jacobian loss is:0.16929774\n",
            "This is the iter 6845, the d1 loss is: 3016.289, the d2 loss is: -3240.086, the g loss is: 3255.1406, the ae loss is: 0.0070985295, the jacobian loss is:0.13377146\n",
            "This is the iter 6846, the d1 loss is: 2986.5234, the d2 loss is: -3232.7344, the g loss is: 3232.8594, the ae loss is: 0.0052108467, the jacobian loss is:0.119375505\n",
            "This is the iter 6847, the d1 loss is: 3015.1797, the d2 loss is: -3260.0547, the g loss is: 3208.2656, the ae loss is: 0.007311008, the jacobian loss is:0.12620805\n",
            "This is the iter 6848, the d1 loss is: 3099.7344, the d2 loss is: -3373.1406, the g loss is: 3376.7734, the ae loss is: 0.0059671705, the jacobian loss is:0.14133279\n",
            "This is the iter 6849, the d1 loss is: 2947.9453, the d2 loss is: -3199.0703, the g loss is: 3169.25, the ae loss is: 0.010324169, the jacobian loss is:0.12367989\n",
            "This is the iter 6850, the d1 loss is: 2912.8984, the d2 loss is: -3154.125, the g loss is: 3184.4062, the ae loss is: 0.008058444, the jacobian loss is:0.12994531\n",
            "This is the iter 6851, the d1 loss is: 2908.9688, the d2 loss is: -3152.7344, the g loss is: 3245.7188, the ae loss is: 0.006653268, the jacobian loss is:0.11160949\n",
            "This is the iter 6852, the d1 loss is: 2942.7344, the d2 loss is: -3189.125, the g loss is: 3174.8516, the ae loss is: 0.0052759447, the jacobian loss is:0.09837246\n",
            "This is the iter 6853, the d1 loss is: 2925.6562, the d2 loss is: -3192.4766, the g loss is: 3141.6562, the ae loss is: 0.0069654034, the jacobian loss is:0.11105616\n",
            "This is the iter 6854, the d1 loss is: 2935.2422, the d2 loss is: -3196.9531, the g loss is: 3180.3047, the ae loss is: 0.007376282, the jacobian loss is:0.14249739\n",
            "This is the iter 6855, the d1 loss is: 2967.9688, the d2 loss is: -3176.5078, the g loss is: 3188.0312, the ae loss is: 0.00794767, the jacobian loss is:0.113041766\n",
            "This is the iter 6856, the d1 loss is: 2987.5156, the d2 loss is: -3229.6328, the g loss is: 3249.0, the ae loss is: 0.0055378964, the jacobian loss is:0.13178888\n",
            "This is the iter 6857, the d1 loss is: 3006.9453, the d2 loss is: -3231.8594, the g loss is: 3206.4297, the ae loss is: 0.0070886454, the jacobian loss is:0.18528469\n",
            "This is the iter 6858, the d1 loss is: 2920.6172, the d2 loss is: -3125.836, the g loss is: 3109.711, the ae loss is: 0.0083704805, the jacobian loss is:0.1439381\n",
            "This is the iter 6859, the d1 loss is: 2940.9375, the d2 loss is: -3195.9375, the g loss is: 3205.3125, the ae loss is: 0.005832292, the jacobian loss is:0.12700242\n",
            "This is the iter 6860, the d1 loss is: 2977.7188, the d2 loss is: -3227.75, the g loss is: 3211.6094, the ae loss is: 0.0078621395, the jacobian loss is:0.104437314\n",
            "This is the iter 6861, the d1 loss is: 2996.539, the d2 loss is: -3263.1484, the g loss is: 3220.7266, the ae loss is: 0.0073144655, the jacobian loss is:0.124880195\n",
            "This is the iter 6862, the d1 loss is: 2746.75, the d2 loss is: -2995.75, the g loss is: 2970.7969, the ae loss is: 0.0067706564, the jacobian loss is:0.14993273\n",
            "This is the iter 6863, the d1 loss is: 2984.6797, the d2 loss is: -3211.8672, the g loss is: 3226.7812, the ae loss is: 0.006828729, the jacobian loss is:0.11656722\n",
            "This is the iter 6864, the d1 loss is: 2977.7734, the d2 loss is: -3230.5938, the g loss is: 3233.0781, the ae loss is: 0.006034586, the jacobian loss is:0.13266069\n",
            "This is the iter 6865, the d1 loss is: 3016.414, the d2 loss is: -3268.8984, the g loss is: 3201.4297, the ae loss is: 0.007890911, the jacobian loss is:0.13502723\n",
            "This is the iter 6866, the d1 loss is: 2869.8672, the d2 loss is: -3139.086, the g loss is: 3134.2031, the ae loss is: 0.0071746996, the jacobian loss is:0.12164412\n",
            "This is the iter 6867, the d1 loss is: 2931.7266, the d2 loss is: -3195.8438, the g loss is: 3187.3047, the ae loss is: 0.0063648326, the jacobian loss is:0.12212503\n",
            "This is the iter 6868, the d1 loss is: 2929.5234, the d2 loss is: -3180.1953, the g loss is: 3156.6875, the ae loss is: 0.00643797, the jacobian loss is:0.1699038\n",
            "This is the iter 6869, the d1 loss is: 2957.0312, the d2 loss is: -3193.75, the g loss is: 3143.5938, the ae loss is: 0.004981999, the jacobian loss is:0.14646669\n",
            "This is the iter 6870, the d1 loss is: 2948.461, the d2 loss is: -3213.9688, the g loss is: 3134.7812, the ae loss is: 0.007584887, the jacobian loss is:0.123908676\n",
            "This is the iter 6871, the d1 loss is: 2868.8281, the d2 loss is: -3143.125, the g loss is: 3179.125, the ae loss is: 0.0068269973, the jacobian loss is:0.101340674\n",
            "This is the iter 6872, the d1 loss is: 2815.4062, the d2 loss is: -3043.086, the g loss is: 3017.914, the ae loss is: 0.00618652, the jacobian loss is:0.111196786\n",
            "This is the iter 6873, the d1 loss is: 2944.4688, the d2 loss is: -3184.2344, the g loss is: 3168.6406, the ae loss is: 0.0059649553, the jacobian loss is:0.104475446\n",
            "This is the iter 6874, the d1 loss is: 2851.4766, the d2 loss is: -3109.539, the g loss is: 3082.2812, the ae loss is: 0.006598901, the jacobian loss is:0.09068935\n",
            "This is the iter 6875, the d1 loss is: 3058.3828, the d2 loss is: -3342.2578, the g loss is: 3338.9531, the ae loss is: 0.0055351676, the jacobian loss is:0.099283256\n",
            "This is the iter 6876, the d1 loss is: 2937.4531, the d2 loss is: -3149.1875, the g loss is: 3099.164, the ae loss is: 0.006467659, the jacobian loss is:0.16059025\n",
            "This is the iter 6877, the d1 loss is: 2972.7188, the d2 loss is: -3174.7734, the g loss is: 3178.5938, the ae loss is: 0.006016708, the jacobian loss is:0.14713493\n",
            "This is the iter 6878, the d1 loss is: 2826.6797, the d2 loss is: -3052.4453, the g loss is: 3086.0312, the ae loss is: 0.006864463, the jacobian loss is:0.13121991\n",
            "This is the iter 6879, the d1 loss is: 2999.875, the d2 loss is: -3240.1094, the g loss is: 3204.4375, the ae loss is: 0.006144212, the jacobian loss is:0.13451849\n",
            "This is the iter 6880, the d1 loss is: 2897.25, the d2 loss is: -3140.086, the g loss is: 3118.8125, the ae loss is: 0.005579283, the jacobian loss is:0.109494455\n",
            "This is the iter 6881, the d1 loss is: 2852.8281, the d2 loss is: -3096.5312, the g loss is: 3099.6562, the ae loss is: 0.007226011, the jacobian loss is:0.097408004\n",
            "This is the iter 6882, the d1 loss is: 2828.1797, the d2 loss is: -3085.6016, the g loss is: 3120.6875, the ae loss is: 0.0064151343, the jacobian loss is:0.13535476\n",
            "This is the iter 6883, the d1 loss is: 2941.0938, the d2 loss is: -3171.7344, the g loss is: 3157.25, the ae loss is: 0.0066339476, the jacobian loss is:0.12512474\n",
            "This is the iter 6884, the d1 loss is: 3022.5234, the d2 loss is: -3277.6172, the g loss is: 3296.9375, the ae loss is: 0.005810708, the jacobian loss is:0.0840428\n",
            "This is the iter 6885, the d1 loss is: 2931.3438, the d2 loss is: -3159.9688, the g loss is: 3224.3125, the ae loss is: 0.0057144444, the jacobian loss is:0.09853935\n",
            "This is the iter 6886, the d1 loss is: 2830.289, the d2 loss is: -3064.25, the g loss is: 3135.8125, the ae loss is: 0.008254466, the jacobian loss is:0.11889661\n",
            "This is the iter 6887, the d1 loss is: 2937.5312, the d2 loss is: -3212.4531, the g loss is: 3192.7422, the ae loss is: 0.0071570734, the jacobian loss is:0.10054912\n",
            "This is the iter 6888, the d1 loss is: 2929.5938, the d2 loss is: -3177.789, the g loss is: 3196.7578, the ae loss is: 0.0071072336, the jacobian loss is:0.11398209\n",
            "This is the iter 6889, the d1 loss is: 2943.8047, the d2 loss is: -3178.6875, the g loss is: 3140.9688, the ae loss is: 0.0081876265, the jacobian loss is:0.11779539\n",
            "This is the iter 6890, the d1 loss is: 2937.961, the d2 loss is: -3174.2188, the g loss is: 3179.9453, the ae loss is: 0.0062007876, the jacobian loss is:0.10742823\n",
            "This is the iter 6891, the d1 loss is: 3171.211, the d2 loss is: -3404.1875, the g loss is: 3379.2734, the ae loss is: 0.0061151097, the jacobian loss is:0.13544393\n",
            "This is the iter 6892, the d1 loss is: 2977.2188, the d2 loss is: -3235.5, the g loss is: 3231.6172, the ae loss is: 0.008696174, the jacobian loss is:0.15361987\n",
            "This is the iter 6893, the d1 loss is: 2983.2344, the d2 loss is: -3228.1328, the g loss is: 3168.0469, the ae loss is: 0.00811088, the jacobian loss is:0.15487713\n",
            "This is the iter 6894, the d1 loss is: 2960.9844, the d2 loss is: -3195.1406, the g loss is: 3215.711, the ae loss is: 0.0061600003, the jacobian loss is:0.29019442\n",
            "This is the iter 6895, the d1 loss is: 2917.9688, the d2 loss is: -3180.4297, the g loss is: 3182.0938, the ae loss is: 0.005676117, the jacobian loss is:0.106564626\n",
            "This is the iter 6896, the d1 loss is: 2889.0625, the d2 loss is: -3136.3594, the g loss is: 3161.6094, the ae loss is: 0.006070054, the jacobian loss is:0.13235484\n",
            "This is the iter 6897, the d1 loss is: 2885.1875, the d2 loss is: -3140.086, the g loss is: 3177.0703, the ae loss is: 0.006710589, the jacobian loss is:0.10369366\n",
            "This is the iter 6898, the d1 loss is: 2980.6406, the d2 loss is: -3211.7344, the g loss is: 3210.1172, the ae loss is: 0.007093495, the jacobian loss is:0.114043884\n",
            "This is the iter 6899, the d1 loss is: 3099.5703, the d2 loss is: -3364.961, the g loss is: 3455.9766, the ae loss is: 0.006647287, the jacobian loss is:0.1279331\n",
            "This is the iter 6900, the d1 loss is: 2944.5078, the d2 loss is: -3218.461, the g loss is: 3247.289, the ae loss is: 0.006326591, the jacobian loss is:0.08696717\n",
            "0.22383846\n",
            "0.85898566\n",
            "This is the iter 6901, the d1 loss is: 3021.289, the d2 loss is: -3277.1875, the g loss is: 3287.4531, the ae loss is: 0.0048003974, the jacobian loss is:0.120953694\n",
            "This is the iter 6902, the d1 loss is: 3406.914, the d2 loss is: -3664.2422, the g loss is: 3640.4844, the ae loss is: 0.005500351, the jacobian loss is:0.1227915\n",
            "This is the iter 6903, the d1 loss is: 2734.5156, the d2 loss is: -2927.961, the g loss is: 2958.1484, the ae loss is: 0.0063609104, the jacobian loss is:0.11078326\n",
            "This is the iter 6904, the d1 loss is: 3074.6875, the d2 loss is: -3315.3984, the g loss is: 3319.1719, the ae loss is: 0.0073183235, the jacobian loss is:0.18137771\n",
            "This is the iter 6905, the d1 loss is: 3038.25, the d2 loss is: -3283.7969, the g loss is: 3245.4531, the ae loss is: 0.0042842333, the jacobian loss is:0.12757337\n",
            "This is the iter 6906, the d1 loss is: 3042.7969, the d2 loss is: -3292.836, the g loss is: 3300.25, the ae loss is: 0.008471161, the jacobian loss is:0.14930534\n",
            "This is the iter 6907, the d1 loss is: 2926.664, the d2 loss is: -3164.5625, the g loss is: 3215.25, the ae loss is: 0.008476652, the jacobian loss is:0.13917923\n",
            "This is the iter 6908, the d1 loss is: 3063.5, the d2 loss is: -3267.0156, the g loss is: 3245.0469, the ae loss is: 0.0048313495, the jacobian loss is:0.20224002\n",
            "This is the iter 6909, the d1 loss is: 3252.4922, the d2 loss is: -3461.211, the g loss is: 3465.5312, the ae loss is: 0.008160235, the jacobian loss is:0.08829345\n",
            "This is the iter 6910, the d1 loss is: 3002.4375, the d2 loss is: -3260.3906, the g loss is: 3247.5312, the ae loss is: 0.0047110664, the jacobian loss is:0.13790242\n",
            "This is the iter 6911, the d1 loss is: 2974.3906, the d2 loss is: -3219.0625, the g loss is: 3238.5312, the ae loss is: 0.0051260553, the jacobian loss is:0.18991177\n",
            "This is the iter 6912, the d1 loss is: 3040.6875, the d2 loss is: -3298.3906, the g loss is: 3314.6953, the ae loss is: 0.006428675, the jacobian loss is:0.093034744\n",
            "This is the iter 6913, the d1 loss is: 2715.8594, the d2 loss is: -2992.586, the g loss is: 3014.4922, the ae loss is: 0.0046760673, the jacobian loss is:0.14179592\n",
            "This is the iter 6914, the d1 loss is: 3249.9375, the d2 loss is: -3526.6328, the g loss is: 3501.3828, the ae loss is: 0.0059313467, the jacobian loss is:0.10317442\n",
            "This is the iter 6915, the d1 loss is: 2966.6562, the d2 loss is: -3218.8047, the g loss is: 3240.414, the ae loss is: 0.0073267017, the jacobian loss is:0.1392765\n",
            "This is the iter 6916, the d1 loss is: 3023.1328, the d2 loss is: -3275.9062, the g loss is: 3180.3281, the ae loss is: 0.0077363155, the jacobian loss is:0.12175578\n",
            "This is the iter 6917, the d1 loss is: 2799.0547, the d2 loss is: -3079.4844, the g loss is: 3084.0312, the ae loss is: 0.005948987, the jacobian loss is:0.11779995\n",
            "This is the iter 6918, the d1 loss is: 2864.9062, the d2 loss is: -3110.4375, the g loss is: 3099.3672, the ae loss is: 0.0075287716, the jacobian loss is:0.10796761\n",
            "This is the iter 6919, the d1 loss is: 2970.3203, the d2 loss is: -3208.2734, the g loss is: 3216.3047, the ae loss is: 0.008521866, the jacobian loss is:0.13387287\n",
            "This is the iter 6920, the d1 loss is: 3016.8438, the d2 loss is: -3254.1484, the g loss is: 3266.6094, the ae loss is: 0.0056452313, the jacobian loss is:0.09220657\n",
            "This is the iter 6921, the d1 loss is: 3054.1172, the d2 loss is: -3291.836, the g loss is: 3271.3516, the ae loss is: 0.0069035273, the jacobian loss is:0.11526134\n",
            "This is the iter 6922, the d1 loss is: 3019.1406, the d2 loss is: -3249.8828, the g loss is: 3234.7969, the ae loss is: 0.0071363905, the jacobian loss is:0.14883834\n",
            "This is the iter 6923, the d1 loss is: 2948.9531, the d2 loss is: -3206.3047, the g loss is: 3214.75, the ae loss is: 0.0065358235, the jacobian loss is:0.11402229\n",
            "This is the iter 6924, the d1 loss is: 2968.9062, the d2 loss is: -3234.1172, the g loss is: 3130.6797, the ae loss is: 0.0076682516, the jacobian loss is:0.22290078\n",
            "This is the iter 6925, the d1 loss is: 2916.6406, the d2 loss is: -3152.7344, the g loss is: 3189.7969, the ae loss is: 0.007382664, the jacobian loss is:0.11849786\n",
            "This is the iter 6926, the d1 loss is: 2991.4375, the d2 loss is: -3260.1562, the g loss is: 3212.8281, the ae loss is: 0.0068894783, the jacobian loss is:0.1098835\n",
            "This is the iter 6927, the d1 loss is: 3183.8438, the d2 loss is: -3421.3203, the g loss is: 3492.961, the ae loss is: 0.007186854, the jacobian loss is:0.11836431\n",
            "This is the iter 6928, the d1 loss is: 3201.3828, the d2 loss is: -3430.6094, the g loss is: 3503.8281, the ae loss is: 0.0074008843, the jacobian loss is:0.13164651\n",
            "This is the iter 6929, the d1 loss is: 3014.836, the d2 loss is: -3262.1875, the g loss is: 3277.5469, the ae loss is: 0.0067439065, the jacobian loss is:0.091906965\n",
            "This is the iter 6930, the d1 loss is: 3027.0156, the d2 loss is: -3278.0469, the g loss is: 3243.1875, the ae loss is: 0.005114992, the jacobian loss is:0.10190922\n",
            "This is the iter 6931, the d1 loss is: 3054.4219, the d2 loss is: -3302.1875, the g loss is: 3330.0156, the ae loss is: 0.008657232, the jacobian loss is:0.11500864\n",
            "This is the iter 6932, the d1 loss is: 2729.5703, the d2 loss is: -2986.3281, the g loss is: 2905.6797, the ae loss is: 0.010788659, the jacobian loss is:0.10945687\n",
            "This is the iter 6933, the d1 loss is: 3014.836, the d2 loss is: -3236.5469, the g loss is: 3257.4375, the ae loss is: 0.009010531, the jacobian loss is:0.12209998\n",
            "This is the iter 6934, the d1 loss is: 3048.1953, the d2 loss is: -3299.2188, the g loss is: 3233.7422, the ae loss is: 0.007047502, the jacobian loss is:0.13768005\n",
            "This is the iter 6935, the d1 loss is: 2810.336, the d2 loss is: -3084.7656, the g loss is: 3025.414, the ae loss is: 0.0077249925, the jacobian loss is:0.11570932\n",
            "This is the iter 6936, the d1 loss is: 3054.6406, the d2 loss is: -3296.7188, the g loss is: 3290.5469, the ae loss is: 0.0102449, the jacobian loss is:0.10224353\n",
            "This is the iter 6937, the d1 loss is: 2998.3438, the d2 loss is: -3243.75, the g loss is: 3250.1875, the ae loss is: 0.005653262, the jacobian loss is:0.087984376\n",
            "This is the iter 6938, the d1 loss is: 2874.375, the d2 loss is: -3119.1172, the g loss is: 3178.8203, the ae loss is: 0.0057071988, the jacobian loss is:0.13142452\n",
            "This is the iter 6939, the d1 loss is: 2936.5938, the d2 loss is: -3172.1719, the g loss is: 3171.75, the ae loss is: 0.0056337123, the jacobian loss is:0.10241506\n",
            "This is the iter 6940, the d1 loss is: 2751.875, the d2 loss is: -3030.211, the g loss is: 3041.2422, the ae loss is: 0.0066537526, the jacobian loss is:0.09597094\n",
            "This is the iter 6941, the d1 loss is: 2938.5625, the d2 loss is: -3216.2188, the g loss is: 3207.0, the ae loss is: 0.0072135245, the jacobian loss is:0.10765992\n",
            "This is the iter 6942, the d1 loss is: 2719.5312, the d2 loss is: -2949.75, the g loss is: 2936.7031, the ae loss is: 0.0071962075, the jacobian loss is:0.12780225\n",
            "This is the iter 6943, the d1 loss is: 2976.4375, the d2 loss is: -3237.1094, the g loss is: 3241.4766, the ae loss is: 0.0056102937, the jacobian loss is:0.09534551\n",
            "This is the iter 6944, the d1 loss is: 2747.6953, the d2 loss is: -2999.8125, the g loss is: 2995.8984, the ae loss is: 0.0063601965, the jacobian loss is:0.1483096\n",
            "This is the iter 6945, the d1 loss is: 2791.2656, the d2 loss is: -3005.5234, the g loss is: 2952.2969, the ae loss is: 0.0071724807, the jacobian loss is:0.13113892\n",
            "This is the iter 6946, the d1 loss is: 2913.9844, the d2 loss is: -3164.461, the g loss is: 3181.289, the ae loss is: 0.007648559, the jacobian loss is:0.12793054\n",
            "This is the iter 6947, the d1 loss is: 3146.961, the d2 loss is: -3408.7031, the g loss is: 3400.0312, the ae loss is: 0.0061315764, the jacobian loss is:0.14821076\n",
            "This is the iter 6948, the d1 loss is: 2847.5703, the d2 loss is: -3105.3672, the g loss is: 3156.6562, the ae loss is: 0.005277629, the jacobian loss is:0.11588518\n",
            "This is the iter 6949, the d1 loss is: 2908.164, the d2 loss is: -3157.6875, the g loss is: 3104.1719, the ae loss is: 0.007196393, the jacobian loss is:0.13073154\n",
            "This is the iter 6950, the d1 loss is: 2946.4531, the d2 loss is: -3168.2734, the g loss is: 3158.0625, the ae loss is: 0.0050882595, the jacobian loss is:0.12181739\n",
            "This is the iter 6951, the d1 loss is: 2941.1016, the d2 loss is: -3215.8438, the g loss is: 3116.8594, the ae loss is: 0.0076101026, the jacobian loss is:0.11221558\n",
            "This is the iter 6952, the d1 loss is: 3229.9297, the d2 loss is: -3473.8438, the g loss is: 3484.4062, the ae loss is: 0.0053342395, the jacobian loss is:0.12058061\n",
            "This is the iter 6953, the d1 loss is: 2723.7812, the d2 loss is: -2949.7422, the g loss is: 2942.1562, the ae loss is: 0.007975023, the jacobian loss is:0.09928241\n",
            "This is the iter 6954, the d1 loss is: 3015.8203, the d2 loss is: -3251.7031, the g loss is: 3248.5156, the ae loss is: 0.006961678, the jacobian loss is:0.108342044\n",
            "This is the iter 6955, the d1 loss is: 3058.1562, the d2 loss is: -3298.4297, the g loss is: 3280.9844, the ae loss is: 0.006987681, the jacobian loss is:0.09174811\n",
            "This is the iter 6956, the d1 loss is: 3070.8594, the d2 loss is: -3306.6719, the g loss is: 3226.4297, the ae loss is: 0.009858834, the jacobian loss is:0.10777645\n",
            "This is the iter 6957, the d1 loss is: 2946.414, the d2 loss is: -3177.9844, the g loss is: 3201.4375, the ae loss is: 0.006814184, the jacobian loss is:0.14354219\n",
            "This is the iter 6958, the d1 loss is: 2925.7031, the d2 loss is: -3202.6719, the g loss is: 3127.6406, the ae loss is: 0.009461401, the jacobian loss is:0.07879771\n",
            "This is the iter 6959, the d1 loss is: 2896.0, the d2 loss is: -3145.5781, the g loss is: 3153.3438, the ae loss is: 0.007278026, the jacobian loss is:0.13112402\n",
            "This is the iter 6960, the d1 loss is: 2875.0938, the d2 loss is: -3122.086, the g loss is: 3161.5625, the ae loss is: 0.0052290154, the jacobian loss is:0.10153447\n",
            "This is the iter 6961, the d1 loss is: 2969.6797, the d2 loss is: -3184.2266, the g loss is: 3185.9688, the ae loss is: 0.0070729638, the jacobian loss is:0.12188352\n",
            "This is the iter 6962, the d1 loss is: 2781.4375, the d2 loss is: -3038.3984, the g loss is: 3027.164, the ae loss is: 0.003941283, the jacobian loss is:0.09896497\n",
            "This is the iter 6963, the d1 loss is: 3008.4844, the d2 loss is: -3244.4453, the g loss is: 3230.164, the ae loss is: 0.00480851, the jacobian loss is:0.097276784\n",
            "This is the iter 6964, the d1 loss is: 3223.2734, the d2 loss is: -3424.8594, the g loss is: 3385.1406, the ae loss is: 0.0059597683, the jacobian loss is:0.082440004\n",
            "This is the iter 6965, the d1 loss is: 3032.5312, the d2 loss is: -3266.7656, the g loss is: 3280.5234, the ae loss is: 0.006165508, the jacobian loss is:0.18191229\n",
            "This is the iter 6966, the d1 loss is: 2908.8438, the d2 loss is: -3166.2266, the g loss is: 3122.6797, the ae loss is: 0.0077723423, the jacobian loss is:0.14368232\n",
            "This is the iter 6967, the d1 loss is: 2890.414, the d2 loss is: -3180.8203, the g loss is: 3161.5469, the ae loss is: 0.0113033205, the jacobian loss is:0.14138907\n",
            "This is the iter 6968, the d1 loss is: 2710.7188, the d2 loss is: -2986.6406, the g loss is: 2987.4688, the ae loss is: 0.005198652, the jacobian loss is:0.09578902\n",
            "This is the iter 6969, the d1 loss is: 3238.5625, the d2 loss is: -3469.2266, the g loss is: 3486.586, the ae loss is: 0.005570055, the jacobian loss is:0.14179854\n",
            "This is the iter 6970, the d1 loss is: 2920.0703, the d2 loss is: -3187.0234, the g loss is: 3166.3594, the ae loss is: 0.007804288, the jacobian loss is:0.13841008\n",
            "This is the iter 6971, the d1 loss is: 3059.9531, the d2 loss is: -3312.6172, the g loss is: 3316.0781, the ae loss is: 0.008165114, the jacobian loss is:0.11763855\n",
            "This is the iter 6972, the d1 loss is: 2907.9531, the d2 loss is: -3170.211, the g loss is: 3131.8047, the ae loss is: 0.007851757, the jacobian loss is:0.1081751\n",
            "This is the iter 6973, the d1 loss is: 2974.7969, the d2 loss is: -3170.5312, the g loss is: 3231.25, the ae loss is: 0.008380234, the jacobian loss is:0.11461082\n",
            "This is the iter 6974, the d1 loss is: 3028.7031, the d2 loss is: -3229.711, the g loss is: 3165.6953, the ae loss is: 0.0073925504, the jacobian loss is:0.09569024\n",
            "This is the iter 6975, the d1 loss is: 2899.3984, the d2 loss is: -3128.8906, the g loss is: 3151.7656, the ae loss is: 0.0054221316, the jacobian loss is:0.13344762\n",
            "This is the iter 6976, the d1 loss is: 2952.2812, the d2 loss is: -3204.3203, the g loss is: 3220.5547, the ae loss is: 0.0054915342, the jacobian loss is:0.10351339\n",
            "This is the iter 6977, the d1 loss is: 3009.336, the d2 loss is: -3234.7812, the g loss is: 3285.8594, the ae loss is: 0.004966082, the jacobian loss is:0.085593805\n",
            "This is the iter 6978, the d1 loss is: 2911.4688, the d2 loss is: -3172.6797, the g loss is: 3179.6953, the ae loss is: 0.0057142135, the jacobian loss is:0.09922556\n",
            "This is the iter 6979, the d1 loss is: 3008.1016, the d2 loss is: -3226.586, the g loss is: 3231.1406, the ae loss is: 0.00623962, the jacobian loss is:0.17218414\n",
            "This is the iter 6980, the d1 loss is: 2937.1719, the d2 loss is: -3186.1016, the g loss is: 3138.4062, the ae loss is: 0.007360474, the jacobian loss is:0.09912613\n",
            "This is the iter 6981, the d1 loss is: 2954.8984, the d2 loss is: -3166.8438, the g loss is: 3107.6406, the ae loss is: 0.0070054997, the jacobian loss is:0.24006827\n",
            "This is the iter 6982, the d1 loss is: 3083.5781, the d2 loss is: -3315.8047, the g loss is: 3311.2578, the ae loss is: 0.0045525706, the jacobian loss is:0.145732\n",
            "This is the iter 6983, the d1 loss is: 2985.7344, the d2 loss is: -3217.9844, the g loss is: 3255.9062, the ae loss is: 0.0072261295, the jacobian loss is:0.09069318\n",
            "This is the iter 6984, the d1 loss is: 2930.5078, the d2 loss is: -3171.6719, the g loss is: 3210.6953, the ae loss is: 0.007437393, the jacobian loss is:0.12892465\n",
            "This is the iter 6985, the d1 loss is: 3016.1875, the d2 loss is: -3242.1719, the g loss is: 3245.5156, the ae loss is: 0.007370756, the jacobian loss is:0.12408606\n",
            "This is the iter 6986, the d1 loss is: 3013.375, the d2 loss is: -3248.375, the g loss is: 3250.9766, the ae loss is: 0.006314909, the jacobian loss is:0.13698156\n",
            "This is the iter 6987, the d1 loss is: 3020.1484, the d2 loss is: -3245.4297, the g loss is: 3218.4531, the ae loss is: 0.0053598722, the jacobian loss is:0.10302304\n",
            "This is the iter 6988, the d1 loss is: 2982.2344, the d2 loss is: -3172.7656, the g loss is: 3215.3125, the ae loss is: 0.007090717, the jacobian loss is:0.112597756\n",
            "This is the iter 6989, the d1 loss is: 2918.5469, the d2 loss is: -3109.2969, the g loss is: 3194.2266, the ae loss is: 0.006399202, the jacobian loss is:0.12534264\n",
            "This is the iter 6990, the d1 loss is: 2929.5625, the d2 loss is: -3168.539, the g loss is: 3133.0156, the ae loss is: 0.00618221, the jacobian loss is:0.10355774\n",
            "This is the iter 6991, the d1 loss is: 2986.3828, the d2 loss is: -3220.539, the g loss is: 3252.4375, the ae loss is: 0.007634671, the jacobian loss is:0.0972049\n",
            "This is the iter 6992, the d1 loss is: 2808.9219, the d2 loss is: -3025.3438, the g loss is: 3044.375, the ae loss is: 0.008559209, the jacobian loss is:0.108639896\n",
            "This is the iter 6993, the d1 loss is: 2860.2188, the d2 loss is: -3101.7812, the g loss is: 3180.7031, the ae loss is: 0.0052922396, the jacobian loss is:0.120289765\n",
            "This is the iter 6994, the d1 loss is: 2938.4453, the d2 loss is: -3157.5312, the g loss is: 3180.0, the ae loss is: 0.0065537454, the jacobian loss is:0.09620702\n",
            "This is the iter 6995, the d1 loss is: 2991.789, the d2 loss is: -3242.2031, the g loss is: 3209.875, the ae loss is: 0.006531926, the jacobian loss is:0.1228696\n",
            "This is the iter 6996, the d1 loss is: 2928.0469, the d2 loss is: -3194.3438, the g loss is: 3221.914, the ae loss is: 0.007271719, the jacobian loss is:0.12807882\n",
            "This is the iter 6997, the d1 loss is: 2936.5469, the d2 loss is: -3207.0938, the g loss is: 3177.0234, the ae loss is: 0.006502557, the jacobian loss is:0.11024437\n",
            "This is the iter 6998, the d1 loss is: 2961.336, the d2 loss is: -3196.3203, the g loss is: 3181.0469, the ae loss is: 0.006484038, the jacobian loss is:0.11976057\n",
            "This is the iter 6999, the d1 loss is: 2920.5312, the d2 loss is: -3136.1172, the g loss is: 3164.461, the ae loss is: 0.007948102, the jacobian loss is:0.17425272\n",
            "This is the iter 7000, the d1 loss is: 2969.2734, the d2 loss is: -3220.3828, the g loss is: 3215.4375, the ae loss is: 0.0057800324, the jacobian loss is:0.1715113\n",
            "0.22783908\n",
            "0.87641233\n",
            "This is the iter 7001, the d1 loss is: 3075.2188, the d2 loss is: -3289.8125, the g loss is: 3275.3984, the ae loss is: 0.0065218126, the jacobian loss is:0.11573488\n",
            "This is the iter 7002, the d1 loss is: 2896.1719, the d2 loss is: -3138.3984, the g loss is: 3223.375, the ae loss is: 0.008185737, the jacobian loss is:0.12498621\n",
            "This is the iter 7003, the d1 loss is: 2980.5234, the d2 loss is: -3241.3672, the g loss is: 3183.7578, the ae loss is: 0.0041709365, the jacobian loss is:0.11684905\n",
            "This is the iter 7004, the d1 loss is: 2918.2969, the d2 loss is: -3175.8906, the g loss is: 3171.5234, the ae loss is: 0.007295454, the jacobian loss is:0.13796668\n",
            "This is the iter 7005, the d1 loss is: 2946.5, the d2 loss is: -3169.6406, the g loss is: 3162.664, the ae loss is: 0.007042372, the jacobian loss is:0.13582666\n",
            "This is the iter 7006, the d1 loss is: 2894.1094, the d2 loss is: -3121.2656, the g loss is: 3164.2812, the ae loss is: 0.00732352, the jacobian loss is:0.112977535\n",
            "This is the iter 7007, the d1 loss is: 2975.1797, the d2 loss is: -3234.9375, the g loss is: 3189.9297, the ae loss is: 0.00766405, the jacobian loss is:0.13307273\n",
            "This is the iter 7008, the d1 loss is: 2952.3047, the d2 loss is: -3181.0312, the g loss is: 3198.0156, the ae loss is: 0.006657466, the jacobian loss is:0.10660519\n",
            "This is the iter 7009, the d1 loss is: 2944.1406, the d2 loss is: -3207.1094, the g loss is: 3281.375, the ae loss is: 0.0069350745, the jacobian loss is:0.11990866\n",
            "This is the iter 7010, the d1 loss is: 3056.211, the d2 loss is: -3260.8906, the g loss is: 3281.164, the ae loss is: 0.006999462, the jacobian loss is:0.10628392\n",
            "This is the iter 7011, the d1 loss is: 2783.2969, the d2 loss is: -3054.0781, the g loss is: 3022.7422, the ae loss is: 0.008268825, the jacobian loss is:0.11192277\n",
            "This is the iter 7012, the d1 loss is: 2923.9375, the d2 loss is: -3172.6875, the g loss is: 3176.5234, the ae loss is: 0.01111822, the jacobian loss is:0.12195167\n",
            "This is the iter 7013, the d1 loss is: 2996.0312, the d2 loss is: -3245.0234, the g loss is: 3214.7344, the ae loss is: 0.008623227, the jacobian loss is:0.123473585\n",
            "This is the iter 7014, the d1 loss is: 3016.7266, the d2 loss is: -3238.3281, the g loss is: 3220.8828, the ae loss is: 0.008342536, the jacobian loss is:0.22003277\n",
            "This is the iter 7015, the d1 loss is: 2985.3594, the d2 loss is: -3226.961, the g loss is: 3178.4688, the ae loss is: 0.0049936953, the jacobian loss is:0.12776858\n",
            "This is the iter 7016, the d1 loss is: 2974.4219, the d2 loss is: -3249.4375, the g loss is: 3204.3438, the ae loss is: 0.007099036, the jacobian loss is:0.13392122\n",
            "This is the iter 7017, the d1 loss is: 2950.8125, the d2 loss is: -3224.164, the g loss is: 3188.0469, the ae loss is: 0.005859507, the jacobian loss is:0.104252964\n",
            "This is the iter 7018, the d1 loss is: 2751.6406, the d2 loss is: -2994.6406, the g loss is: 3031.4531, the ae loss is: 0.003449168, the jacobian loss is:0.06421339\n",
            "This is the iter 7019, the d1 loss is: 2826.7734, the d2 loss is: -3059.1719, the g loss is: 3119.789, the ae loss is: 0.007719304, the jacobian loss is:0.13167202\n",
            "This is the iter 7020, the d1 loss is: 2928.3828, the d2 loss is: -3164.5938, the g loss is: 3157.0469, the ae loss is: 0.007831219, the jacobian loss is:0.11720356\n",
            "This is the iter 7021, the d1 loss is: 2983.7812, the d2 loss is: -3216.211, the g loss is: 3197.7656, the ae loss is: 0.0064253663, the jacobian loss is:0.1314043\n",
            "This is the iter 7022, the d1 loss is: 2952.4844, the d2 loss is: -3154.7422, the g loss is: 3192.6875, the ae loss is: 0.0062226932, the jacobian loss is:0.14037882\n",
            "This is the iter 7023, the d1 loss is: 2858.7734, the d2 loss is: -3099.5781, the g loss is: 3124.7812, the ae loss is: 0.007606145, the jacobian loss is:0.1526207\n",
            "This is the iter 7024, the d1 loss is: 2947.1562, the d2 loss is: -3169.0234, the g loss is: 3118.9062, the ae loss is: 0.008185575, the jacobian loss is:0.104332745\n",
            "This is the iter 7025, the d1 loss is: 2833.8281, the d2 loss is: -3079.0781, the g loss is: 3096.375, the ae loss is: 0.006485088, the jacobian loss is:0.18851344\n",
            "This is the iter 7026, the d1 loss is: 2835.25, the d2 loss is: -3082.9219, the g loss is: 3087.5, the ae loss is: 0.0066119363, the jacobian loss is:0.16769008\n",
            "This is the iter 7027, the d1 loss is: 2945.6406, the d2 loss is: -3205.1719, the g loss is: 3162.914, the ae loss is: 0.006200064, the jacobian loss is:0.10388652\n",
            "This is the iter 7028, the d1 loss is: 2917.75, the d2 loss is: -3158.4844, the g loss is: 3182.625, the ae loss is: 0.0063741757, the jacobian loss is:0.13706517\n",
            "This is the iter 7029, the d1 loss is: 2966.5781, the d2 loss is: -3174.461, the g loss is: 3167.6484, the ae loss is: 0.0060811266, the jacobian loss is:0.11429674\n",
            "This is the iter 7030, the d1 loss is: 2732.5312, the d2 loss is: -2954.2656, the g loss is: 2996.586, the ae loss is: 0.007011966, the jacobian loss is:0.1142951\n",
            "This is the iter 7031, the d1 loss is: 2893.539, the d2 loss is: -3142.4297, the g loss is: 3145.4219, the ae loss is: 0.0072940104, the jacobian loss is:0.17286737\n",
            "This is the iter 7032, the d1 loss is: 2936.9297, the d2 loss is: -3144.4844, the g loss is: 3149.6562, the ae loss is: 0.0069315163, the jacobian loss is:0.18983689\n",
            "This is the iter 7033, the d1 loss is: 2987.3047, the d2 loss is: -3190.0469, the g loss is: 3148.5625, the ae loss is: 0.006279065, the jacobian loss is:0.12465357\n",
            "This is the iter 7034, the d1 loss is: 3212.8984, the d2 loss is: -3434.6875, the g loss is: 3454.789, the ae loss is: 0.0050398796, the jacobian loss is:0.13644117\n",
            "This is the iter 7035, the d1 loss is: 2920.125, the d2 loss is: -3136.0312, the g loss is: 3158.8125, the ae loss is: 0.007816736, the jacobian loss is:0.12174297\n",
            "This is the iter 7036, the d1 loss is: 2951.9531, the d2 loss is: -3181.1875, the g loss is: 3110.3047, the ae loss is: 0.0071957405, the jacobian loss is:0.23639067\n",
            "This is the iter 7037, the d1 loss is: 2913.8516, the d2 loss is: -3156.3594, the g loss is: 3171.414, the ae loss is: 0.0070412643, the jacobian loss is:0.14821297\n",
            "This is the iter 7038, the d1 loss is: 2988.1719, the d2 loss is: -3254.7812, the g loss is: 3182.1406, the ae loss is: 0.0062451335, the jacobian loss is:0.17586136\n",
            "This is the iter 7039, the d1 loss is: 2936.3906, the d2 loss is: -3162.3203, the g loss is: 3155.4531, the ae loss is: 0.006873385, the jacobian loss is:0.13938098\n",
            "This is the iter 7040, the d1 loss is: 3052.1719, the d2 loss is: -3250.664, the g loss is: 3201.4375, the ae loss is: 0.006698558, the jacobian loss is:0.109643646\n",
            "This is the iter 7041, the d1 loss is: 3109.711, the d2 loss is: -3362.3906, the g loss is: 3364.6562, the ae loss is: 0.008881014, the jacobian loss is:0.13217948\n",
            "This is the iter 7042, the d1 loss is: 2915.9219, the d2 loss is: -3164.8438, the g loss is: 3204.711, the ae loss is: 0.0069505, the jacobian loss is:0.121826045\n",
            "This is the iter 7043, the d1 loss is: 2885.7422, the d2 loss is: -3079.4453, the g loss is: 3135.539, the ae loss is: 0.0061316127, the jacobian loss is:0.14022404\n",
            "This is the iter 7044, the d1 loss is: 2926.875, the d2 loss is: -3142.0625, the g loss is: 3171.7266, the ae loss is: 0.0047722477, the jacobian loss is:0.112854734\n",
            "This is the iter 7045, the d1 loss is: 2975.8828, the d2 loss is: -3198.6094, the g loss is: 3169.4531, the ae loss is: 0.007818054, the jacobian loss is:0.11346397\n",
            "This is the iter 7046, the d1 loss is: 3156.1328, the d2 loss is: -3393.4688, the g loss is: 3417.2578, the ae loss is: 0.006379945, the jacobian loss is:0.18074006\n",
            "This is the iter 7047, the d1 loss is: 3138.5469, the d2 loss is: -3399.4297, the g loss is: 3389.4766, the ae loss is: 0.007423741, the jacobian loss is:0.15113427\n",
            "This is the iter 7048, the d1 loss is: 3056.5, the d2 loss is: -3295.9375, the g loss is: 3307.5, the ae loss is: 0.0065423013, the jacobian loss is:0.1694382\n",
            "This is the iter 7049, the d1 loss is: 2771.6797, the d2 loss is: -3036.4375, the g loss is: 3030.625, the ae loss is: 0.005913484, the jacobian loss is:0.10979029\n",
            "This is the iter 7050, the d1 loss is: 3037.4531, the d2 loss is: -3266.4297, the g loss is: 3222.8125, the ae loss is: 0.005274413, the jacobian loss is:0.12358766\n",
            "This is the iter 7051, the d1 loss is: 2942.0703, the d2 loss is: -3176.1719, the g loss is: 3198.8047, the ae loss is: 0.0076933573, the jacobian loss is:0.11997423\n",
            "This is the iter 7052, the d1 loss is: 3012.7656, the d2 loss is: -3234.4375, the g loss is: 3213.6094, the ae loss is: 0.008256348, the jacobian loss is:0.117731825\n",
            "This is the iter 7053, the d1 loss is: 2966.6797, the d2 loss is: -3199.5781, the g loss is: 3203.4062, the ae loss is: 0.006260695, the jacobian loss is:0.12578648\n",
            "This is the iter 7054, the d1 loss is: 2997.1406, the d2 loss is: -3229.4688, the g loss is: 3190.4219, the ae loss is: 0.0097124055, the jacobian loss is:0.11300648\n",
            "This is the iter 7055, the d1 loss is: 3179.6406, the d2 loss is: -3425.3438, the g loss is: 3462.4531, the ae loss is: 0.006876488, the jacobian loss is:0.1596743\n",
            "This is the iter 7056, the d1 loss is: 2938.6328, the d2 loss is: -3164.9453, the g loss is: 3161.2031, the ae loss is: 0.005158944, the jacobian loss is:0.16343255\n",
            "This is the iter 7057, the d1 loss is: 3147.0781, the d2 loss is: -3355.7031, the g loss is: 3408.2656, the ae loss is: 0.006190572, the jacobian loss is:0.100652196\n",
            "This is the iter 7058, the d1 loss is: 3062.7578, the d2 loss is: -3286.9297, the g loss is: 3219.5078, the ae loss is: 0.0084491735, the jacobian loss is:0.14524129\n",
            "This is the iter 7059, the d1 loss is: 3064.875, the d2 loss is: -3287.3828, the g loss is: 3306.5156, the ae loss is: 0.005833026, the jacobian loss is:0.15430751\n",
            "This is the iter 7060, the d1 loss is: 3041.039, the d2 loss is: -3258.539, the g loss is: 3245.3906, the ae loss is: 0.008160794, the jacobian loss is:0.13041498\n",
            "This is the iter 7061, the d1 loss is: 3102.5781, the d2 loss is: -3331.6719, the g loss is: 3323.3203, the ae loss is: 0.0072925827, the jacobian loss is:0.1234646\n",
            "This is the iter 7062, the d1 loss is: 3319.1094, the d2 loss is: -3568.5156, the g loss is: 3595.125, the ae loss is: 0.007783965, the jacobian loss is:0.17988987\n",
            "This is the iter 7063, the d1 loss is: 3029.2344, the d2 loss is: -3258.4453, the g loss is: 3253.0781, the ae loss is: 0.005297193, the jacobian loss is:0.13510443\n",
            "This is the iter 7064, the d1 loss is: 2930.0469, the d2 loss is: -3165.0625, the g loss is: 3238.4531, the ae loss is: 0.005856325, the jacobian loss is:0.12155829\n",
            "This is the iter 7065, the d1 loss is: 2745.6953, the d2 loss is: -3002.5, the g loss is: 3016.4688, the ae loss is: 0.00837894, the jacobian loss is:0.14088054\n",
            "This is the iter 7066, the d1 loss is: 3040.0469, the d2 loss is: -3275.8438, the g loss is: 3280.8438, the ae loss is: 0.009455366, the jacobian loss is:0.08879707\n",
            "This is the iter 7067, the d1 loss is: 3040.7344, the d2 loss is: -3258.375, the g loss is: 3273.0703, the ae loss is: 0.0068878327, the jacobian loss is:0.12544379\n",
            "This is the iter 7068, the d1 loss is: 2634.0469, the d2 loss is: -2843.125, the g loss is: 2977.6953, the ae loss is: 0.0069362726, the jacobian loss is:0.09501441\n",
            "This is the iter 7069, the d1 loss is: 2969.625, the d2 loss is: -3189.8438, the g loss is: 3171.2188, the ae loss is: 0.0041877595, the jacobian loss is:0.16037352\n",
            "This is the iter 7070, the d1 loss is: 3052.7188, the d2 loss is: -3298.461, the g loss is: 3264.6719, the ae loss is: 0.00820586, the jacobian loss is:0.14929613\n",
            "This is the iter 7071, the d1 loss is: 2957.0547, the d2 loss is: -3201.1875, the g loss is: 3119.8984, the ae loss is: 0.010145138, the jacobian loss is:0.14016913\n",
            "This is the iter 7072, the d1 loss is: 2976.1875, the d2 loss is: -3227.875, the g loss is: 3214.5312, the ae loss is: 0.006630423, the jacobian loss is:0.17136967\n",
            "This is the iter 7073, the d1 loss is: 2885.9062, the d2 loss is: -3103.414, the g loss is: 3094.914, the ae loss is: 0.0073378393, the jacobian loss is:0.12747048\n",
            "This is the iter 7074, the d1 loss is: 3016.4297, the d2 loss is: -3238.9219, the g loss is: 3223.3047, the ae loss is: 0.0062746704, the jacobian loss is:0.1738292\n",
            "This is the iter 7075, the d1 loss is: 2753.9219, the d2 loss is: -3002.7812, the g loss is: 3004.1562, the ae loss is: 0.006389585, the jacobian loss is:0.14715591\n",
            "This is the iter 7076, the d1 loss is: 2951.664, the d2 loss is: -3193.8906, the g loss is: 3179.961, the ae loss is: 0.008981511, the jacobian loss is:0.13223326\n",
            "This is the iter 7077, the d1 loss is: 2751.7969, the d2 loss is: -3009.6953, the g loss is: 3008.7969, the ae loss is: 0.0061898218, the jacobian loss is:0.1058552\n",
            "This is the iter 7078, the d1 loss is: 2793.289, the d2 loss is: -3031.0625, the g loss is: 3052.2812, the ae loss is: 0.005416442, the jacobian loss is:0.15703781\n",
            "This is the iter 7079, the d1 loss is: 2882.7344, the d2 loss is: -3086.1875, the g loss is: 3169.0469, the ae loss is: 0.0057320744, the jacobian loss is:0.1707068\n",
            "This is the iter 7080, the d1 loss is: 3227.1719, the d2 loss is: -3439.8906, the g loss is: 3486.0312, the ae loss is: 0.0070303148, the jacobian loss is:0.19663502\n",
            "This is the iter 7081, the d1 loss is: 2944.8203, the d2 loss is: -3162.0625, the g loss is: 3211.3594, the ae loss is: 0.007449299, the jacobian loss is:0.120160714\n",
            "This is the iter 7082, the d1 loss is: 3146.5234, the d2 loss is: -3401.914, the g loss is: 3396.664, the ae loss is: 0.0070624975, the jacobian loss is:0.122721635\n",
            "This is the iter 7083, the d1 loss is: 3085.789, the d2 loss is: -3306.1094, the g loss is: 3353.3984, the ae loss is: 0.007211297, the jacobian loss is:0.1304288\n",
            "This is the iter 7084, the d1 loss is: 2918.5234, the d2 loss is: -3171.8047, the g loss is: 3198.2344, the ae loss is: 0.0047696624, the jacobian loss is:0.14553742\n",
            "This is the iter 7085, the d1 loss is: 2915.5625, the d2 loss is: -3140.7734, the g loss is: 3186.1172, the ae loss is: 0.0060817543, the jacobian loss is:0.10563632\n",
            "This is the iter 7086, the d1 loss is: 2921.0547, the d2 loss is: -3147.961, the g loss is: 3178.0078, the ae loss is: 0.0059169587, the jacobian loss is:0.12865855\n",
            "This is the iter 7087, the d1 loss is: 2975.3672, the d2 loss is: -3219.6875, the g loss is: 3196.0547, the ae loss is: 0.005592652, the jacobian loss is:0.11638583\n",
            "This is the iter 7088, the d1 loss is: 2965.086, the d2 loss is: -3212.7656, the g loss is: 3213.9297, the ae loss is: 0.008083005, the jacobian loss is:0.13046512\n",
            "This is the iter 7089, the d1 loss is: 2935.7812, the d2 loss is: -3166.039, the g loss is: 3194.375, the ae loss is: 0.0077766404, the jacobian loss is:0.12975547\n",
            "This is the iter 7090, the d1 loss is: 2980.6328, the d2 loss is: -3218.0625, the g loss is: 3171.8047, the ae loss is: 0.0068946653, the jacobian loss is:0.13463445\n",
            "This is the iter 7091, the d1 loss is: 2951.5625, the d2 loss is: -3181.7422, the g loss is: 3182.9688, the ae loss is: 0.008307114, the jacobian loss is:0.14063643\n",
            "This is the iter 7092, the d1 loss is: 3033.6953, the d2 loss is: -3284.3984, the g loss is: 3220.8984, the ae loss is: 0.0066943616, the jacobian loss is:0.15157036\n",
            "This is the iter 7093, the d1 loss is: 2911.336, the d2 loss is: -3162.2812, the g loss is: 3176.1094, the ae loss is: 0.0078003206, the jacobian loss is:0.1923434\n",
            "This is the iter 7094, the d1 loss is: 2975.2031, the d2 loss is: -3202.914, the g loss is: 3169.3203, the ae loss is: 0.00528106, the jacobian loss is:0.16779272\n",
            "This is the iter 7095, the d1 loss is: 3023.0938, the d2 loss is: -3218.5, the g loss is: 3212.1172, the ae loss is: 0.006660424, the jacobian loss is:0.14330155\n",
            "This is the iter 7096, the d1 loss is: 2989.6172, the d2 loss is: -3240.6719, the g loss is: 3231.1406, the ae loss is: 0.008329565, the jacobian loss is:0.11904453\n",
            "This is the iter 7097, the d1 loss is: 2753.6328, the d2 loss is: -2987.8047, the g loss is: 2992.0469, the ae loss is: 0.0071625635, the jacobian loss is:0.14198184\n",
            "This is the iter 7098, the d1 loss is: 2978.3438, the d2 loss is: -3196.0938, the g loss is: 3208.3281, the ae loss is: 0.005398874, the jacobian loss is:0.13626745\n",
            "This is the iter 7099, the d1 loss is: 2695.664, the d2 loss is: -2950.4688, the g loss is: 2956.3125, the ae loss is: 0.006470113, the jacobian loss is:0.13427132\n",
            "This is the iter 7100, the d1 loss is: 3022.7188, the d2 loss is: -3258.25, the g loss is: 3217.1719, the ae loss is: 0.006712503, the jacobian loss is:0.15317409\n",
            "0.23602015\n",
            "0.9148335\n",
            "This is the iter 7101, the d1 loss is: 2778.3516, the d2 loss is: -3010.5938, the g loss is: 3023.3438, the ae loss is: 0.007831686, the jacobian loss is:0.17096022\n",
            "This is the iter 7102, the d1 loss is: 2912.7188, the d2 loss is: -3159.2969, the g loss is: 3145.4297, the ae loss is: 0.00889739, the jacobian loss is:0.17310643\n",
            "This is the iter 7103, the d1 loss is: 2839.4453, the d2 loss is: -3093.336, the g loss is: 3101.875, the ae loss is: 0.010288496, the jacobian loss is:0.13813893\n",
            "This is the iter 7104, the d1 loss is: 2887.164, the d2 loss is: -3141.6719, the g loss is: 3140.3281, the ae loss is: 0.0054705795, the jacobian loss is:0.14471297\n",
            "This is the iter 7105, the d1 loss is: 2989.4375, the d2 loss is: -3211.9062, the g loss is: 3222.4531, the ae loss is: 0.0068886336, the jacobian loss is:0.10197829\n",
            "This is the iter 7106, the d1 loss is: 3151.1875, the d2 loss is: -3368.664, the g loss is: 3400.3203, the ae loss is: 0.005482934, the jacobian loss is:0.121918276\n",
            "This is the iter 7107, the d1 loss is: 2957.4375, the d2 loss is: -3197.6719, the g loss is: 3224.3047, the ae loss is: 0.0073728706, the jacobian loss is:0.110826366\n",
            "This is the iter 7108, the d1 loss is: 2824.9297, the d2 loss is: -3038.6719, the g loss is: 2970.3984, the ae loss is: 0.0051186625, the jacobian loss is:0.19963746\n",
            "This is the iter 7109, the d1 loss is: 2900.6562, the d2 loss is: -3136.6172, the g loss is: 3108.711, the ae loss is: 0.0051857447, the jacobian loss is:0.13650867\n",
            "This is the iter 7110, the d1 loss is: 3018.6406, the d2 loss is: -3271.414, the g loss is: 3219.6875, the ae loss is: 0.0046692207, the jacobian loss is:0.12023698\n",
            "This is the iter 7111, the d1 loss is: 2920.9375, the d2 loss is: -3156.1875, the g loss is: 3132.586, the ae loss is: 0.0068408, the jacobian loss is:0.13574089\n",
            "This is the iter 7112, the d1 loss is: 3012.8828, the d2 loss is: -3261.2266, the g loss is: 3269.4531, the ae loss is: 0.008195131, the jacobian loss is:0.14101194\n",
            "This is the iter 7113, the d1 loss is: 2885.0703, the d2 loss is: -3113.0, the g loss is: 3111.7266, the ae loss is: 0.0057702195, the jacobian loss is:0.14166354\n",
            "This is the iter 7114, the d1 loss is: 2626.0469, the d2 loss is: -2908.3594, the g loss is: 2842.9375, the ae loss is: 0.009096713, the jacobian loss is:0.17711206\n",
            "This is the iter 7115, the d1 loss is: 2911.2422, the d2 loss is: -3137.2812, the g loss is: 3139.4766, the ae loss is: 0.008471483, the jacobian loss is:0.17239425\n",
            "This is the iter 7116, the d1 loss is: 3001.8828, the d2 loss is: -3235.2266, the g loss is: 3210.8125, the ae loss is: 0.005726905, the jacobian loss is:0.1442548\n",
            "This is the iter 7117, the d1 loss is: 2949.1328, the d2 loss is: -3181.0078, the g loss is: 3173.1016, the ae loss is: 0.007562719, the jacobian loss is:0.15601215\n",
            "This is the iter 7118, the d1 loss is: 2919.539, the d2 loss is: -3149.7344, the g loss is: 3153.6172, the ae loss is: 0.0077390396, the jacobian loss is:0.16968517\n",
            "This is the iter 7119, the d1 loss is: 2957.6719, the d2 loss is: -3198.5469, the g loss is: 3167.2734, the ae loss is: 0.0065817595, the jacobian loss is:0.15922734\n",
            "This is the iter 7120, the d1 loss is: 2899.6875, the d2 loss is: -3127.6094, the g loss is: 3155.1094, the ae loss is: 0.0063122706, the jacobian loss is:0.11684411\n",
            "This is the iter 7121, the d1 loss is: 2836.2656, the d2 loss is: -3074.2656, the g loss is: 3015.4766, the ae loss is: 0.009240592, the jacobian loss is:0.1771098\n",
            "This is the iter 7122, the d1 loss is: 2835.8828, the d2 loss is: -3088.5078, the g loss is: 3066.961, the ae loss is: 0.008656932, the jacobian loss is:0.10524725\n",
            "This is the iter 7123, the d1 loss is: 2876.5625, the d2 loss is: -3114.1719, the g loss is: 3076.8281, the ae loss is: 0.009609006, the jacobian loss is:0.178064\n",
            "This is the iter 7124, the d1 loss is: 2884.8281, the d2 loss is: -3069.4531, the g loss is: 3160.0703, the ae loss is: 0.004711856, the jacobian loss is:0.11975\n",
            "This is the iter 7125, the d1 loss is: 2901.8984, the d2 loss is: -3116.5, the g loss is: 3198.4531, the ae loss is: 0.0076319044, the jacobian loss is:0.15530704\n",
            "This is the iter 7126, the d1 loss is: 2849.6562, the d2 loss is: -3114.3594, the g loss is: 3114.6094, the ae loss is: 0.0076730773, the jacobian loss is:0.15180828\n",
            "This is the iter 7127, the d1 loss is: 2896.5, the d2 loss is: -3135.6016, the g loss is: 3133.9062, the ae loss is: 0.009363705, the jacobian loss is:0.14191721\n",
            "This is the iter 7128, the d1 loss is: 2884.9531, the d2 loss is: -3132.9844, the g loss is: 3127.836, the ae loss is: 0.0062374137, the jacobian loss is:0.16329934\n",
            "This is the iter 7129, the d1 loss is: 2956.5625, the d2 loss is: -3180.2812, the g loss is: 3174.664, the ae loss is: 0.007126746, the jacobian loss is:0.11288694\n",
            "This is the iter 7130, the d1 loss is: 2936.2188, the d2 loss is: -3162.9219, the g loss is: 3171.2969, the ae loss is: 0.006166067, the jacobian loss is:0.12621303\n",
            "This is the iter 7131, the d1 loss is: 2739.6562, the d2 loss is: -2948.5547, the g loss is: 2934.3203, the ae loss is: 0.008697971, the jacobian loss is:0.13682559\n",
            "This is the iter 7132, the d1 loss is: 2894.2188, the d2 loss is: -3097.7969, the g loss is: 3127.6172, the ae loss is: 0.0071485597, the jacobian loss is:0.18763667\n",
            "This is the iter 7133, the d1 loss is: 3162.1016, the d2 loss is: -3389.7344, the g loss is: 3371.2344, the ae loss is: 0.006722568, the jacobian loss is:0.19545294\n",
            "This is the iter 7134, the d1 loss is: 2950.2344, the d2 loss is: -3190.4297, the g loss is: 3264.2656, the ae loss is: 0.006620855, the jacobian loss is:0.12982547\n",
            "This is the iter 7135, the d1 loss is: 2872.8906, the d2 loss is: -3143.8906, the g loss is: 3049.461, the ae loss is: 0.009239468, the jacobian loss is:0.29569238\n",
            "This is the iter 7136, the d1 loss is: 2904.8438, the d2 loss is: -3112.4453, the g loss is: 3115.5312, the ae loss is: 0.00597785, the jacobian loss is:0.13021213\n",
            "This is the iter 7137, the d1 loss is: 3219.3984, the d2 loss is: -3434.336, the g loss is: 3455.3438, the ae loss is: 0.0066986233, the jacobian loss is:0.13257693\n",
            "This is the iter 7138, the d1 loss is: 2633.7812, the d2 loss is: -2882.0156, the g loss is: 2899.1562, the ae loss is: 0.007652616, the jacobian loss is:0.146823\n",
            "This is the iter 7139, the d1 loss is: 2960.4531, the d2 loss is: -3162.5, the g loss is: 3129.6797, the ae loss is: 0.0069644065, the jacobian loss is:0.14626497\n",
            "This is the iter 7140, the d1 loss is: 3304.8438, the d2 loss is: -3514.8047, the g loss is: 3489.6328, the ae loss is: 0.00557859, the jacobian loss is:0.11821186\n",
            "This is the iter 7141, the d1 loss is: 2710.5547, the d2 loss is: -2936.2422, the g loss is: 2902.586, the ae loss is: 0.0075662266, the jacobian loss is:0.1451398\n",
            "This is the iter 7142, the d1 loss is: 2884.875, the d2 loss is: -3128.3203, the g loss is: 3148.336, the ae loss is: 0.007780852, the jacobian loss is:0.11819938\n",
            "This is the iter 7143, the d1 loss is: 2939.875, the d2 loss is: -3150.4375, the g loss is: 3166.3047, the ae loss is: 0.008383709, the jacobian loss is:0.21652417\n",
            "This is the iter 7144, the d1 loss is: 3050.0, the d2 loss is: -3288.8047, the g loss is: 3278.5781, the ae loss is: 0.0052766125, the jacobian loss is:0.1394897\n",
            "This is the iter 7145, the d1 loss is: 2937.1016, the d2 loss is: -3130.9062, the g loss is: 3152.336, the ae loss is: 0.006595147, the jacobian loss is:0.16627736\n",
            "This is the iter 7146, the d1 loss is: 2950.0625, the d2 loss is: -3177.9766, the g loss is: 3195.5078, the ae loss is: 0.006057335, the jacobian loss is:0.094788246\n",
            "This is the iter 7147, the d1 loss is: 2943.3672, the d2 loss is: -3197.3438, the g loss is: 3192.1406, the ae loss is: 0.007845405, the jacobian loss is:0.14039612\n",
            "This is the iter 7148, the d1 loss is: 2972.1016, the d2 loss is: -3216.914, the g loss is: 3243.1719, the ae loss is: 0.0073109064, the jacobian loss is:0.14788532\n",
            "This is the iter 7149, the d1 loss is: 2951.1016, the d2 loss is: -3172.5078, the g loss is: 3190.3906, the ae loss is: 0.005367983, the jacobian loss is:0.14066483\n",
            "This is the iter 7150, the d1 loss is: 2905.7344, the d2 loss is: -3176.9453, the g loss is: 3141.5234, the ae loss is: 0.00625412, the jacobian loss is:0.1580892\n",
            "This is the iter 7151, the d1 loss is: 3176.664, the d2 loss is: -3410.2578, the g loss is: 3372.5156, the ae loss is: 0.0065473737, the jacobian loss is:0.13107504\n",
            "This is the iter 7152, the d1 loss is: 2999.8281, the d2 loss is: -3237.0, the g loss is: 3234.0, the ae loss is: 0.0074990653, the jacobian loss is:0.1227255\n",
            "This is the iter 7153, the d1 loss is: 2943.7266, the d2 loss is: -3177.2344, the g loss is: 3229.5781, the ae loss is: 0.007560175, the jacobian loss is:0.15960978\n",
            "This is the iter 7154, the d1 loss is: 2843.3906, the d2 loss is: -3050.0781, the g loss is: 3027.6016, the ae loss is: 0.007408607, the jacobian loss is:0.15755601\n",
            "This is the iter 7155, the d1 loss is: 2990.875, the d2 loss is: -3215.6562, the g loss is: 3233.2188, the ae loss is: 0.00778493, the jacobian loss is:0.1413129\n",
            "This is the iter 7156, the d1 loss is: 2666.039, the d2 loss is: -2894.9922, the g loss is: 2904.9297, the ae loss is: 0.0053652506, the jacobian loss is:0.15947507\n",
            "This is the iter 7157, the d1 loss is: 2981.7188, the d2 loss is: -3215.0938, the g loss is: 3166.836, the ae loss is: 0.006484133, the jacobian loss is:0.1536831\n",
            "This is the iter 7158, the d1 loss is: 2941.7578, the d2 loss is: -3173.0078, the g loss is: 3175.4766, the ae loss is: 0.0048810747, the jacobian loss is:0.11077028\n",
            "This is the iter 7159, the d1 loss is: 3114.0547, the d2 loss is: -3349.6094, the g loss is: 3413.6016, the ae loss is: 0.0053195353, the jacobian loss is:0.13563554\n",
            "This is the iter 7160, the d1 loss is: 3035.4766, the d2 loss is: -3246.414, the g loss is: 3251.3281, the ae loss is: 0.007374198, the jacobian loss is:0.15862475\n",
            "This is the iter 7161, the d1 loss is: 2829.9766, the d2 loss is: -3032.6406, the g loss is: 3031.7969, the ae loss is: 0.007979818, the jacobian loss is:0.15852906\n",
            "This is the iter 7162, the d1 loss is: 2920.875, the d2 loss is: -3136.4688, the g loss is: 3204.8594, the ae loss is: 0.0076211053, the jacobian loss is:0.11337482\n",
            "This is the iter 7163, the d1 loss is: 3025.2266, the d2 loss is: -3267.1719, the g loss is: 3335.25, the ae loss is: 0.00922229, the jacobian loss is:0.1952842\n",
            "This is the iter 7164, the d1 loss is: 2949.7188, the d2 loss is: -3181.6406, the g loss is: 3163.6094, the ae loss is: 0.0062591718, the jacobian loss is:0.11982158\n",
            "This is the iter 7165, the d1 loss is: 3027.375, the d2 loss is: -3255.4297, the g loss is: 3217.8516, the ae loss is: 0.00823778, the jacobian loss is:0.13561274\n",
            "This is the iter 7166, the d1 loss is: 2960.2656, the d2 loss is: -3202.5078, the g loss is: 3180.4844, the ae loss is: 0.0047219824, the jacobian loss is:0.12282836\n",
            "This is the iter 7167, the d1 loss is: 3030.9688, the d2 loss is: -3266.0938, the g loss is: 3246.3516, the ae loss is: 0.009160053, the jacobian loss is:0.17659934\n",
            "This is the iter 7168, the d1 loss is: 2947.0312, the d2 loss is: -3181.3438, the g loss is: 3165.5312, the ae loss is: 0.007929636, the jacobian loss is:0.18047673\n",
            "This is the iter 7169, the d1 loss is: 3074.7734, the d2 loss is: -3275.6719, the g loss is: 3255.8828, the ae loss is: 0.006755381, the jacobian loss is:0.12179973\n",
            "This is the iter 7170, the d1 loss is: 3043.5547, the d2 loss is: -3252.789, the g loss is: 3236.9453, the ae loss is: 0.0054389695, the jacobian loss is:0.16949059\n",
            "This is the iter 7171, the d1 loss is: 2989.0156, the d2 loss is: -3231.3281, the g loss is: 3242.8281, the ae loss is: 0.006367854, the jacobian loss is:0.11081879\n",
            "This is the iter 7172, the d1 loss is: 2997.2812, the d2 loss is: -3237.2656, the g loss is: 3205.4297, the ae loss is: 0.005650102, the jacobian loss is:0.11213434\n",
            "This is the iter 7173, the d1 loss is: 2935.8203, the d2 loss is: -3176.6484, the g loss is: 3178.9062, the ae loss is: 0.008021434, the jacobian loss is:0.12036574\n",
            "This is the iter 7174, the d1 loss is: 2952.0781, the d2 loss is: -3188.8281, the g loss is: 3140.6172, the ae loss is: 0.005610306, the jacobian loss is:0.14772058\n",
            "This is the iter 7175, the d1 loss is: 2976.961, the d2 loss is: -3221.0469, the g loss is: 3254.4062, the ae loss is: 0.0060950224, the jacobian loss is:0.13208479\n",
            "This is the iter 7176, the d1 loss is: 2989.9844, the d2 loss is: -3192.3047, the g loss is: 3183.8594, the ae loss is: 0.005457487, the jacobian loss is:0.119150214\n",
            "This is the iter 7177, the d1 loss is: 2988.2656, the d2 loss is: -3192.25, the g loss is: 3196.0469, the ae loss is: 0.005566365, the jacobian loss is:0.1320473\n",
            "This is the iter 7178, the d1 loss is: 3006.9531, the d2 loss is: -3209.1016, the g loss is: 3278.5781, the ae loss is: 0.0059079845, the jacobian loss is:0.11496128\n",
            "This is the iter 7179, the d1 loss is: 2982.7734, the d2 loss is: -3205.25, the g loss is: 3240.2656, the ae loss is: 0.0060436805, the jacobian loss is:0.16890454\n",
            "This is the iter 7180, the d1 loss is: 3033.4922, the d2 loss is: -3219.5, the g loss is: 3219.2344, the ae loss is: 0.006343726, the jacobian loss is:0.1406524\n",
            "This is the iter 7181, the d1 loss is: 2969.3203, the d2 loss is: -3187.9922, the g loss is: 3168.1719, the ae loss is: 0.0051641, the jacobian loss is:0.13010065\n",
            "This is the iter 7182, the d1 loss is: 2991.3672, the d2 loss is: -3242.0625, the g loss is: 3283.039, the ae loss is: 0.0060094185, the jacobian loss is:0.18329011\n",
            "This is the iter 7183, the d1 loss is: 2968.414, the d2 loss is: -3217.6719, the g loss is: 3213.164, the ae loss is: 0.005404607, the jacobian loss is:0.13166362\n",
            "This is the iter 7184, the d1 loss is: 2935.7266, the d2 loss is: -3130.9219, the g loss is: 3065.9688, the ae loss is: 0.0077682, the jacobian loss is:0.19968869\n",
            "This is the iter 7185, the d1 loss is: 2976.336, the d2 loss is: -3198.0156, the g loss is: 3214.4922, the ae loss is: 0.006665958, the jacobian loss is:0.119450815\n",
            "This is the iter 7186, the d1 loss is: 2712.9219, the d2 loss is: -2973.9766, the g loss is: 2916.9922, the ae loss is: 0.007756336, the jacobian loss is:0.16881832\n",
            "This is the iter 7187, the d1 loss is: 3085.3984, the d2 loss is: -3306.5312, the g loss is: 3293.0, the ae loss is: 0.008993409, the jacobian loss is:0.15416655\n",
            "This is the iter 7188, the d1 loss is: 3040.8203, the d2 loss is: -3244.9375, the g loss is: 3282.164, the ae loss is: 0.006842302, the jacobian loss is:0.1103366\n",
            "This is the iter 7189, the d1 loss is: 2905.5625, the d2 loss is: -3125.5469, the g loss is: 3184.4062, the ae loss is: 0.0047854236, the jacobian loss is:0.11423404\n",
            "This is the iter 7190, the d1 loss is: 3003.2422, the d2 loss is: -3201.8438, the g loss is: 3161.211, the ae loss is: 0.006306732, the jacobian loss is:0.11944535\n",
            "This is the iter 7191, the d1 loss is: 2988.6016, the d2 loss is: -3194.6328, the g loss is: 3236.1328, the ae loss is: 0.0063917083, the jacobian loss is:0.17809637\n",
            "This is the iter 7192, the d1 loss is: 2990.0625, the d2 loss is: -3228.086, the g loss is: 3258.4219, the ae loss is: 0.0061397385, the jacobian loss is:0.12961768\n",
            "This is the iter 7193, the d1 loss is: 2990.2578, the d2 loss is: -3200.5625, the g loss is: 3233.7344, the ae loss is: 0.0051751314, the jacobian loss is:0.15454137\n",
            "This is the iter 7194, the d1 loss is: 2898.4922, the d2 loss is: -3146.6094, the g loss is: 3188.7656, the ae loss is: 0.008037766, the jacobian loss is:0.14647739\n",
            "This is the iter 7195, the d1 loss is: 3041.8594, the d2 loss is: -3235.9375, the g loss is: 3209.875, the ae loss is: 0.0068948492, the jacobian loss is:0.15789545\n",
            "This is the iter 7196, the d1 loss is: 2910.836, the d2 loss is: -3128.2656, the g loss is: 3135.2578, the ae loss is: 0.005771215, the jacobian loss is:0.11865935\n",
            "This is the iter 7197, the d1 loss is: 2982.4375, the d2 loss is: -3217.4453, the g loss is: 3163.3516, the ae loss is: 0.009624999, the jacobian loss is:0.1465533\n",
            "This is the iter 7198, the d1 loss is: 2940.875, the d2 loss is: -3182.2734, the g loss is: 3157.7031, the ae loss is: 0.0047147814, the jacobian loss is:0.13679945\n",
            "This is the iter 7199, the d1 loss is: 2996.625, the d2 loss is: -3217.6094, the g loss is: 3228.5156, the ae loss is: 0.00503844, the jacobian loss is:0.124458544\n",
            "This is the iter 7200, the d1 loss is: 2924.0625, the d2 loss is: -3133.0781, the g loss is: 3153.086, the ae loss is: 0.0068788985, the jacobian loss is:0.14633884\n",
            "0.24446695\n",
            "0.9606737\n",
            "This is the iter 7201, the d1 loss is: 2954.0625, the d2 loss is: -3176.664, the g loss is: 3213.836, the ae loss is: 0.0065068337, the jacobian loss is:0.14297691\n",
            "This is the iter 7202, the d1 loss is: 2992.2031, the d2 loss is: -3187.4844, the g loss is: 3161.625, the ae loss is: 0.008461284, the jacobian loss is:0.19479293\n",
            "This is the iter 7203, the d1 loss is: 2995.8906, the d2 loss is: -3224.1953, the g loss is: 3247.1953, the ae loss is: 0.006017787, the jacobian loss is:0.12368127\n",
            "This is the iter 7204, the d1 loss is: 2935.4219, the d2 loss is: -3185.5547, the g loss is: 3134.9922, the ae loss is: 0.0073869424, the jacobian loss is:0.123668976\n",
            "This is the iter 7205, the d1 loss is: 3255.5547, the d2 loss is: -3450.2266, the g loss is: 3419.3594, the ae loss is: 0.0050505316, the jacobian loss is:0.12664917\n",
            "This is the iter 7206, the d1 loss is: 2910.3906, the d2 loss is: -3146.6484, the g loss is: 3160.0469, the ae loss is: 0.00477725, the jacobian loss is:0.1228314\n",
            "This is the iter 7207, the d1 loss is: 3020.2969, the d2 loss is: -3237.2656, the g loss is: 3301.8672, the ae loss is: 0.006393983, the jacobian loss is:0.13285878\n",
            "This is the iter 7208, the d1 loss is: 3003.4922, the d2 loss is: -3219.2734, the g loss is: 3173.875, the ae loss is: 0.0067855134, the jacobian loss is:0.1541552\n",
            "This is the iter 7209, the d1 loss is: 2931.0781, the d2 loss is: -3173.0469, the g loss is: 3173.7812, the ae loss is: 0.006214953, the jacobian loss is:0.15680481\n",
            "This is the iter 7210, the d1 loss is: 2983.5625, the d2 loss is: -3197.4844, the g loss is: 3195.4688, the ae loss is: 0.0059113423, the jacobian loss is:0.1650558\n",
            "This is the iter 7211, the d1 loss is: 2975.4766, the d2 loss is: -3187.4688, the g loss is: 3225.0156, the ae loss is: 0.0057756626, the jacobian loss is:0.17361686\n",
            "This is the iter 7212, the d1 loss is: 2976.9688, the d2 loss is: -3230.2031, the g loss is: 3234.5469, the ae loss is: 0.008246219, the jacobian loss is:0.157507\n",
            "This is the iter 7213, the d1 loss is: 2967.8672, the d2 loss is: -3187.3438, the g loss is: 3158.6797, the ae loss is: 0.0046312376, the jacobian loss is:0.14764275\n",
            "This is the iter 7214, the d1 loss is: 2937.9844, the d2 loss is: -3163.2031, the g loss is: 3183.2266, the ae loss is: 0.0066677667, the jacobian loss is:0.10763938\n",
            "This is the iter 7215, the d1 loss is: 2972.0938, the d2 loss is: -3168.539, the g loss is: 3177.8594, the ae loss is: 0.0072240853, the jacobian loss is:0.14387396\n",
            "This is the iter 7216, the d1 loss is: 2978.7266, the d2 loss is: -3202.0547, the g loss is: 3181.7812, the ae loss is: 0.006071348, the jacobian loss is:0.12598401\n",
            "This is the iter 7217, the d1 loss is: 2991.7656, the d2 loss is: -3241.7031, the g loss is: 3207.9922, the ae loss is: 0.0049137697, the jacobian loss is:0.1526384\n",
            "This is the iter 7218, the d1 loss is: 2777.0938, the d2 loss is: -3024.7969, the g loss is: 3008.8906, the ae loss is: 0.005946016, the jacobian loss is:0.09103943\n",
            "This is the iter 7219, the d1 loss is: 2926.8438, the d2 loss is: -3154.875, the g loss is: 3153.7812, the ae loss is: 0.0043667722, the jacobian loss is:0.13082446\n",
            "This is the iter 7220, the d1 loss is: 2908.5234, the d2 loss is: -3114.7656, the g loss is: 3146.8047, the ae loss is: 0.005916237, the jacobian loss is:0.13953495\n",
            "This is the iter 7221, the d1 loss is: 3050.375, the d2 loss is: -3279.3672, the g loss is: 3238.4688, the ae loss is: 0.0043468, the jacobian loss is:0.14168184\n",
            "This is the iter 7222, the d1 loss is: 2995.7031, the d2 loss is: -3179.0156, the g loss is: 3172.0938, the ae loss is: 0.008256141, the jacobian loss is:0.121111\n",
            "This is the iter 7223, the d1 loss is: 2950.039, the d2 loss is: -3170.875, the g loss is: 3173.8594, the ae loss is: 0.0053893356, the jacobian loss is:0.13231108\n",
            "This is the iter 7224, the d1 loss is: 2984.5156, the d2 loss is: -3230.0781, the g loss is: 3204.8281, the ae loss is: 0.008233024, the jacobian loss is:0.12876195\n",
            "This is the iter 7225, the d1 loss is: 3153.7812, the d2 loss is: -3339.6875, the g loss is: 3367.0, the ae loss is: 0.006225973, the jacobian loss is:0.14070752\n",
            "This is the iter 7226, the d1 loss is: 2985.2969, the d2 loss is: -3217.7031, the g loss is: 3191.4688, the ae loss is: 0.0066034724, the jacobian loss is:0.13764577\n",
            "This is the iter 7227, the d1 loss is: 3003.2266, the d2 loss is: -3219.625, the g loss is: 3190.8672, the ae loss is: 0.005765935, the jacobian loss is:0.15890141\n",
            "This is the iter 7228, the d1 loss is: 3010.8125, the d2 loss is: -3231.4062, the g loss is: 3159.4766, the ae loss is: 0.0064318245, the jacobian loss is:0.13804908\n",
            "This is the iter 7229, the d1 loss is: 2985.5781, the d2 loss is: -3202.7422, the g loss is: 3243.461, the ae loss is: 0.0050474047, the jacobian loss is:0.11859219\n",
            "This is the iter 7230, the d1 loss is: 2994.3906, the d2 loss is: -3206.1875, the g loss is: 3203.875, the ae loss is: 0.004277579, the jacobian loss is:0.15262039\n",
            "This is the iter 7231, the d1 loss is: 2957.7734, the d2 loss is: -3139.1953, the g loss is: 3202.0469, the ae loss is: 0.003971851, the jacobian loss is:0.13602719\n",
            "This is the iter 7232, the d1 loss is: 2972.5156, the d2 loss is: -3201.7656, the g loss is: 3176.5, the ae loss is: 0.0066490727, the jacobian loss is:0.12356278\n",
            "This is the iter 7233, the d1 loss is: 2983.9219, the d2 loss is: -3194.9453, the g loss is: 3161.8516, the ae loss is: 0.00506943, the jacobian loss is:0.14118838\n",
            "This is the iter 7234, the d1 loss is: 2982.75, the d2 loss is: -3190.8516, the g loss is: 3133.5781, the ae loss is: 0.0077506052, the jacobian loss is:0.1885166\n",
            "This is the iter 7235, the d1 loss is: 2974.2266, the d2 loss is: -3209.039, the g loss is: 3205.7812, the ae loss is: 0.0076136384, the jacobian loss is:0.15624751\n",
            "This is the iter 7236, the d1 loss is: 2945.0625, the d2 loss is: -3127.7188, the g loss is: 3194.461, the ae loss is: 0.0072674598, the jacobian loss is:0.10276898\n",
            "This is the iter 7237, the d1 loss is: 2974.6797, the d2 loss is: -3187.5234, the g loss is: 3170.375, the ae loss is: 0.005633602, the jacobian loss is:0.113537386\n",
            "This is the iter 7238, the d1 loss is: 3091.7344, the d2 loss is: -3299.5312, the g loss is: 3285.9062, the ae loss is: 0.008204254, the jacobian loss is:0.14298557\n",
            "This is the iter 7239, the d1 loss is: 2887.8594, the d2 loss is: -3117.2031, the g loss is: 3133.4531, the ae loss is: 0.0051507913, the jacobian loss is:0.13591187\n",
            "This is the iter 7240, the d1 loss is: 2940.9766, the d2 loss is: -3185.6562, the g loss is: 3177.6562, the ae loss is: 0.0056163417, the jacobian loss is:0.13316715\n",
            "This is the iter 7241, the d1 loss is: 3039.625, the d2 loss is: -3261.9844, the g loss is: 3278.7188, the ae loss is: 0.0061221253, the jacobian loss is:0.12874863\n",
            "This is the iter 7242, the d1 loss is: 2972.289, the d2 loss is: -3196.164, the g loss is: 3238.7656, the ae loss is: 0.007084944, the jacobian loss is:0.090446964\n",
            "This is the iter 7243, the d1 loss is: 2988.2344, the d2 loss is: -3210.6094, the g loss is: 3209.1484, the ae loss is: 0.006654216, the jacobian loss is:0.15750283\n",
            "This is the iter 7244, the d1 loss is: 2964.375, the d2 loss is: -3208.3594, the g loss is: 3240.5156, the ae loss is: 0.006713173, the jacobian loss is:0.12535805\n",
            "This is the iter 7245, the d1 loss is: 2643.5547, the d2 loss is: -2860.4297, the g loss is: 2866.1406, the ae loss is: 0.00571978, the jacobian loss is:0.11478815\n",
            "This is the iter 7246, the d1 loss is: 2936.836, the d2 loss is: -3161.6562, the g loss is: 3166.3594, the ae loss is: 0.0087081045, the jacobian loss is:0.123282366\n",
            "This is the iter 7247, the d1 loss is: 2944.3047, the d2 loss is: -3148.4531, the g loss is: 3130.4844, the ae loss is: 0.00558728, the jacobian loss is:0.12775582\n",
            "This is the iter 7248, the d1 loss is: 2885.0156, the d2 loss is: -3133.3516, the g loss is: 3147.586, the ae loss is: 0.00809374, the jacobian loss is:0.14835015\n",
            "This is the iter 7249, the d1 loss is: 3180.1016, the d2 loss is: -3406.1172, the g loss is: 3449.7812, the ae loss is: 0.0067119244, the jacobian loss is:0.12774442\n",
            "This is the iter 7250, the d1 loss is: 2832.0625, the d2 loss is: -3061.5078, the g loss is: 3140.5938, the ae loss is: 0.0084938025, the jacobian loss is:0.124543734\n",
            "This is the iter 7251, the d1 loss is: 2990.8594, the d2 loss is: -3210.4297, the g loss is: 3165.5547, the ae loss is: 0.0071478067, the jacobian loss is:0.17199536\n",
            "This is the iter 7252, the d1 loss is: 2998.3672, the d2 loss is: -3211.914, the g loss is: 3171.2578, the ae loss is: 0.0039570713, the jacobian loss is:0.13694663\n",
            "This is the iter 7253, the d1 loss is: 3025.8594, the d2 loss is: -3262.6875, the g loss is: 3218.7812, the ae loss is: 0.0047998996, the jacobian loss is:0.1314266\n",
            "This is the iter 7254, the d1 loss is: 2961.1562, the d2 loss is: -3183.3438, the g loss is: 3170.4922, the ae loss is: 0.006281396, the jacobian loss is:0.11658652\n",
            "This is the iter 7255, the d1 loss is: 2922.3906, the d2 loss is: -3176.0469, the g loss is: 3145.6875, the ae loss is: 0.00751181, the jacobian loss is:0.13894983\n",
            "This is the iter 7256, the d1 loss is: 2849.586, the d2 loss is: -3067.9766, the g loss is: 3048.0625, the ae loss is: 0.0048314445, the jacobian loss is:0.11768595\n",
            "This is the iter 7257, the d1 loss is: 2895.5625, the d2 loss is: -3108.2266, the g loss is: 3107.5781, the ae loss is: 0.0069372403, the jacobian loss is:0.12821098\n",
            "This is the iter 7258, the d1 loss is: 2963.2812, the d2 loss is: -3173.5781, the g loss is: 3114.1875, the ae loss is: 0.009315487, the jacobian loss is:0.12450107\n",
            "This is the iter 7259, the d1 loss is: 2803.2969, the d2 loss is: -3008.625, the g loss is: 3017.2969, the ae loss is: 0.0050091418, the jacobian loss is:0.099515416\n",
            "This is the iter 7260, the d1 loss is: 2954.7266, the d2 loss is: -3187.125, the g loss is: 3188.039, the ae loss is: 0.005714586, the jacobian loss is:0.115159705\n",
            "This is the iter 7261, the d1 loss is: 3144.1562, the d2 loss is: -3346.1406, the g loss is: 3303.5703, the ae loss is: 0.0068657435, the jacobian loss is:0.18745276\n",
            "This is the iter 7262, the d1 loss is: 2936.0312, the d2 loss is: -3190.5938, the g loss is: 3162.25, the ae loss is: 0.005885171, the jacobian loss is:0.14086865\n",
            "This is the iter 7263, the d1 loss is: 2943.5625, the d2 loss is: -3169.6719, the g loss is: 3170.289, the ae loss is: 0.0074082855, the jacobian loss is:0.12618169\n",
            "This is the iter 7264, the d1 loss is: 2899.0703, the d2 loss is: -3143.086, the g loss is: 3139.6953, the ae loss is: 0.008428889, the jacobian loss is:0.12473717\n",
            "This is the iter 7265, the d1 loss is: 2972.0312, the d2 loss is: -3174.5156, the g loss is: 3177.039, the ae loss is: 0.007106358, the jacobian loss is:0.13792598\n",
            "This is the iter 7266, the d1 loss is: 2894.3125, the d2 loss is: -3130.0781, the g loss is: 3132.375, the ae loss is: 0.0085051805, the jacobian loss is:0.10781704\n",
            "This is the iter 7267, the d1 loss is: 2947.8281, the d2 loss is: -3139.7031, the g loss is: 3150.8828, the ae loss is: 0.006777258, the jacobian loss is:0.13164173\n",
            "This is the iter 7268, the d1 loss is: 2884.4844, the d2 loss is: -3130.8828, the g loss is: 3128.0547, the ae loss is: 0.0071478942, the jacobian loss is:0.118712716\n",
            "This is the iter 7269, the d1 loss is: 2921.9531, the d2 loss is: -3169.9375, the g loss is: 3152.8594, the ae loss is: 0.0051035527, the jacobian loss is:0.13547905\n",
            "This is the iter 7270, the d1 loss is: 2974.3047, the d2 loss is: -3193.3516, the g loss is: 3188.0312, the ae loss is: 0.00625103, the jacobian loss is:0.12686741\n",
            "This is the iter 7271, the d1 loss is: 2921.9531, the d2 loss is: -3153.7031, the g loss is: 3190.0156, the ae loss is: 0.0046398183, the jacobian loss is:0.11697163\n",
            "This is the iter 7272, the d1 loss is: 2859.3438, the d2 loss is: -3089.0938, the g loss is: 3043.6797, the ae loss is: 0.009471687, the jacobian loss is:0.18997635\n",
            "This is the iter 7273, the d1 loss is: 2944.7969, the d2 loss is: -3176.0234, the g loss is: 3154.6953, the ae loss is: 0.006650294, the jacobian loss is:0.093651146\n",
            "This is the iter 7274, the d1 loss is: 2995.7734, the d2 loss is: -3205.3594, the g loss is: 3206.7578, the ae loss is: 0.0073424038, the jacobian loss is:0.13633822\n",
            "This is the iter 7275, the d1 loss is: 2714.2578, the d2 loss is: -2919.0469, the g loss is: 2895.4688, the ae loss is: 0.0058046347, the jacobian loss is:0.124849066\n",
            "This is the iter 7276, the d1 loss is: 2887.5156, the d2 loss is: -3162.9531, the g loss is: 3090.4219, the ae loss is: 0.005501935, the jacobian loss is:0.107662015\n",
            "This is the iter 7277, the d1 loss is: 2953.5938, the d2 loss is: -3152.5938, the g loss is: 3078.3594, the ae loss is: 0.005878305, the jacobian loss is:0.11721079\n",
            "This is the iter 7278, the d1 loss is: 2989.6484, the d2 loss is: -3205.4453, the g loss is: 3148.1328, the ae loss is: 0.0075244154, the jacobian loss is:0.1275323\n",
            "This is the iter 7279, the d1 loss is: 2873.9062, the d2 loss is: -3123.7578, the g loss is: 3095.5, the ae loss is: 0.0055407337, the jacobian loss is:0.10793378\n",
            "This is the iter 7280, the d1 loss is: 2895.6797, the d2 loss is: -3138.086, the g loss is: 3143.6172, the ae loss is: 0.0069279373, the jacobian loss is:0.13229485\n",
            "This is the iter 7281, the d1 loss is: 2783.0938, the d2 loss is: -2999.0, the g loss is: 3000.414, the ae loss is: 0.007245044, the jacobian loss is:0.120227925\n",
            "This is the iter 7282, the d1 loss is: 2935.164, the d2 loss is: -3159.7812, the g loss is: 3120.3125, the ae loss is: 0.006274195, the jacobian loss is:0.07843416\n",
            "This is the iter 7283, the d1 loss is: 2945.3047, the d2 loss is: -3156.8203, the g loss is: 3134.4531, the ae loss is: 0.006764, the jacobian loss is:0.13193038\n",
            "This is the iter 7284, the d1 loss is: 2900.25, the d2 loss is: -3139.25, the g loss is: 3127.7422, the ae loss is: 0.005765828, the jacobian loss is:0.12320087\n",
            "This is the iter 7285, the d1 loss is: 2955.3906, the d2 loss is: -3124.1406, the g loss is: 3198.4688, the ae loss is: 0.0061104177, the jacobian loss is:0.12247776\n",
            "This is the iter 7286, the d1 loss is: 2910.625, the d2 loss is: -3137.5938, the g loss is: 3079.0469, the ae loss is: 0.006038254, the jacobian loss is:0.11541042\n",
            "This is the iter 7287, the d1 loss is: 2990.086, the d2 loss is: -3219.6406, the g loss is: 3170.8984, the ae loss is: 0.0058685653, the jacobian loss is:0.09780856\n",
            "This is the iter 7288, the d1 loss is: 2954.8047, the d2 loss is: -3167.0156, the g loss is: 3174.5312, the ae loss is: 0.006182991, the jacobian loss is:0.12595011\n",
            "This is the iter 7289, the d1 loss is: 2991.2188, the d2 loss is: -3140.9688, the g loss is: 3147.0625, the ae loss is: 0.006872511, the jacobian loss is:0.12619786\n",
            "This is the iter 7290, the d1 loss is: 2938.3203, the d2 loss is: -3180.6016, the g loss is: 3160.5625, the ae loss is: 0.006773699, the jacobian loss is:0.10840958\n",
            "This is the iter 7291, the d1 loss is: 2947.0469, the d2 loss is: -3146.2188, the g loss is: 3105.1328, the ae loss is: 0.006542935, the jacobian loss is:0.120812915\n",
            "This is the iter 7292, the d1 loss is: 2993.3906, the d2 loss is: -3195.3281, the g loss is: 3202.3281, the ae loss is: 0.008315364, the jacobian loss is:0.11575571\n",
            "This is the iter 7293, the d1 loss is: 2920.9219, the d2 loss is: -3129.0156, the g loss is: 3137.6094, the ae loss is: 0.004689263, the jacobian loss is:0.14528401\n",
            "This is the iter 7294, the d1 loss is: 2928.961, the d2 loss is: -3097.2188, the g loss is: 3061.0469, the ae loss is: 0.007941559, the jacobian loss is:0.12505218\n",
            "This is the iter 7295, the d1 loss is: 2942.8984, the d2 loss is: -3140.6484, the g loss is: 3138.3281, the ae loss is: 0.0055580344, the jacobian loss is:0.12404734\n",
            "This is the iter 7296, the d1 loss is: 2980.4531, the d2 loss is: -3203.8594, the g loss is: 3210.7188, the ae loss is: 0.0062233713, the jacobian loss is:0.11120079\n",
            "This is the iter 7297, the d1 loss is: 2904.836, the d2 loss is: -3101.875, the g loss is: 3122.9688, the ae loss is: 0.0055638677, the jacobian loss is:0.1311777\n",
            "This is the iter 7298, the d1 loss is: 2903.625, the d2 loss is: -3099.7031, the g loss is: 3147.875, the ae loss is: 0.0069051352, the jacobian loss is:0.15577982\n",
            "This is the iter 7299, the d1 loss is: 3118.1562, the d2 loss is: -3343.8594, the g loss is: 3406.8594, the ae loss is: 0.0067272037, the jacobian loss is:0.11377462\n",
            "This is the iter 7300, the d1 loss is: 2920.4375, the d2 loss is: -3156.0703, the g loss is: 3117.7422, the ae loss is: 0.0061433134, the jacobian loss is:0.10456077\n",
            "0.2519139\n",
            "0.9901533\n",
            "This is the iter 7301, the d1 loss is: 2838.6094, the d2 loss is: -3064.789, the g loss is: 3112.039, the ae loss is: 0.006696702, the jacobian loss is:0.13292794\n",
            "This is the iter 7302, the d1 loss is: 3076.0547, the d2 loss is: -3278.0469, the g loss is: 3221.8281, the ae loss is: 0.007030802, the jacobian loss is:0.11663572\n",
            "This is the iter 7303, the d1 loss is: 3007.0781, the d2 loss is: -3207.2344, the g loss is: 3213.6719, the ae loss is: 0.0066860365, the jacobian loss is:0.11841528\n",
            "This is the iter 7304, the d1 loss is: 2731.3203, the d2 loss is: -2914.3281, the g loss is: 2877.6719, the ae loss is: 0.005633276, the jacobian loss is:0.108384974\n",
            "This is the iter 7305, the d1 loss is: 2959.1094, the d2 loss is: -3184.4922, the g loss is: 3159.3438, the ae loss is: 0.006770501, the jacobian loss is:0.10481793\n",
            "This is the iter 7306, the d1 loss is: 2950.375, the d2 loss is: -3154.5781, the g loss is: 3132.3594, the ae loss is: 0.008184807, the jacobian loss is:0.13288353\n",
            "This is the iter 7307, the d1 loss is: 2884.0, the d2 loss is: -3122.9688, the g loss is: 3151.2266, the ae loss is: 0.004978299, the jacobian loss is:0.13083884\n",
            "This is the iter 7308, the d1 loss is: 3040.6797, the d2 loss is: -3246.789, the g loss is: 3197.1484, the ae loss is: 0.008784896, the jacobian loss is:0.12140243\n",
            "This is the iter 7309, the d1 loss is: 2892.4375, the d2 loss is: -3112.289, the g loss is: 3122.7969, the ae loss is: 0.008251293, the jacobian loss is:0.105686285\n",
            "This is the iter 7310, the d1 loss is: 2940.789, the d2 loss is: -3143.4766, the g loss is: 3143.6562, the ae loss is: 0.008405741, the jacobian loss is:0.10325724\n",
            "This is the iter 7311, the d1 loss is: 2686.039, the d2 loss is: -2927.7344, the g loss is: 2955.6875, the ae loss is: 0.0061859423, the jacobian loss is:0.13453697\n",
            "This is the iter 7312, the d1 loss is: 3094.7188, the d2 loss is: -3305.75, the g loss is: 3289.5078, the ae loss is: 0.007554693, the jacobian loss is:0.124762096\n",
            "This is the iter 7313, the d1 loss is: 3160.0547, the d2 loss is: -3353.5703, the g loss is: 3338.7344, the ae loss is: 0.007527792, the jacobian loss is:0.19018792\n",
            "This is the iter 7314, the d1 loss is: 2912.0547, the d2 loss is: -3148.586, the g loss is: 3119.1719, the ae loss is: 0.006568931, the jacobian loss is:0.11480291\n",
            "This is the iter 7315, the d1 loss is: 2788.9219, the d2 loss is: -3001.2266, the g loss is: 2965.039, the ae loss is: 0.0052672196, the jacobian loss is:0.117259234\n",
            "This is the iter 7316, the d1 loss is: 2949.6094, the d2 loss is: -3156.3828, the g loss is: 3164.7188, the ae loss is: 0.006988872, the jacobian loss is:0.19943956\n",
            "This is the iter 7317, the d1 loss is: 3006.8203, the d2 loss is: -3210.6406, the g loss is: 3137.1406, the ae loss is: 0.005313645, the jacobian loss is:0.11614269\n",
            "This is the iter 7318, the d1 loss is: 2989.0547, the d2 loss is: -3187.7812, the g loss is: 3144.375, the ae loss is: 0.0076906374, the jacobian loss is:0.14849553\n",
            "This is the iter 7319, the d1 loss is: 3020.9922, the d2 loss is: -3211.7422, the g loss is: 3216.4922, the ae loss is: 0.0045959414, the jacobian loss is:0.12189034\n",
            "This is the iter 7320, the d1 loss is: 2992.9062, the d2 loss is: -3198.6719, the g loss is: 3139.5938, the ae loss is: 0.0068411594, the jacobian loss is:0.101889506\n",
            "This is the iter 7321, the d1 loss is: 2910.4375, the d2 loss is: -3125.1797, the g loss is: 3163.0312, the ae loss is: 0.006272303, the jacobian loss is:0.12752306\n",
            "This is the iter 7322, the d1 loss is: 2901.1016, the d2 loss is: -3154.8828, the g loss is: 3146.7031, the ae loss is: 0.007025034, the jacobian loss is:0.10565943\n",
            "This is the iter 7323, the d1 loss is: 2980.0312, the d2 loss is: -3195.8984, the g loss is: 3163.8672, the ae loss is: 0.0074177473, the jacobian loss is:0.1292403\n",
            "This is the iter 7324, the d1 loss is: 3020.8672, the d2 loss is: -3233.25, the g loss is: 3218.6797, the ae loss is: 0.0071637603, the jacobian loss is:0.12543803\n",
            "This is the iter 7325, the d1 loss is: 2776.2031, the d2 loss is: -2986.5781, the g loss is: 3022.1562, the ae loss is: 0.007093707, the jacobian loss is:0.12722252\n",
            "This is the iter 7326, the d1 loss is: 2971.5, the d2 loss is: -3226.414, the g loss is: 3199.3906, the ae loss is: 0.007893374, the jacobian loss is:0.12215733\n",
            "This is the iter 7327, the d1 loss is: 2937.2031, the d2 loss is: -3155.961, the g loss is: 3137.664, the ae loss is: 0.0050468016, the jacobian loss is:0.111472666\n",
            "This is the iter 7328, the d1 loss is: 2857.3281, the d2 loss is: -3072.8281, the g loss is: 3153.2734, the ae loss is: 0.0041806973, the jacobian loss is:0.15635988\n",
            "This is the iter 7329, the d1 loss is: 2946.3281, the d2 loss is: -3150.9766, the g loss is: 3186.75, the ae loss is: 0.0056535574, the jacobian loss is:0.108427696\n",
            "This is the iter 7330, the d1 loss is: 2971.0, the d2 loss is: -3187.9219, the g loss is: 3159.2266, the ae loss is: 0.0053505716, the jacobian loss is:0.11064539\n",
            "This is the iter 7331, the d1 loss is: 3029.9375, the d2 loss is: -3242.4688, the g loss is: 3202.5938, the ae loss is: 0.005087586, the jacobian loss is:0.12536304\n",
            "This is the iter 7332, the d1 loss is: 2925.6953, the d2 loss is: -3164.7422, the g loss is: 3170.1016, the ae loss is: 0.005083289, the jacobian loss is:0.13672872\n",
            "This is the iter 7333, the d1 loss is: 3052.664, the d2 loss is: -3248.25, the g loss is: 3201.6484, the ae loss is: 0.0051320754, the jacobian loss is:0.12616241\n",
            "This is the iter 7334, the d1 loss is: 2960.7344, the d2 loss is: -3204.6953, the g loss is: 3200.1016, the ae loss is: 0.005314675, the jacobian loss is:0.12789594\n",
            "This is the iter 7335, the d1 loss is: 2975.4766, the d2 loss is: -3179.8516, the g loss is: 3224.5938, the ae loss is: 0.00448039, the jacobian loss is:0.10392731\n",
            "This is the iter 7336, the d1 loss is: 2962.7266, the d2 loss is: -3171.461, the g loss is: 3193.2812, the ae loss is: 0.0061344774, the jacobian loss is:0.11627613\n",
            "This is the iter 7337, the d1 loss is: 2655.2656, the d2 loss is: -2900.8906, the g loss is: 2853.2031, the ae loss is: 0.006388841, the jacobian loss is:0.113404185\n",
            "This is the iter 7338, the d1 loss is: 3077.2266, the d2 loss is: -3285.8203, the g loss is: 3247.7656, the ae loss is: 0.0078547085, the jacobian loss is:0.14154483\n",
            "This is the iter 7339, the d1 loss is: 2941.0156, the d2 loss is: -3132.875, the g loss is: 3161.3438, the ae loss is: 0.0046432302, the jacobian loss is:0.104814835\n",
            "This is the iter 7340, the d1 loss is: 2962.6172, the d2 loss is: -3199.3438, the g loss is: 3213.1562, the ae loss is: 0.007984907, the jacobian loss is:0.09590606\n",
            "This is the iter 7341, the d1 loss is: 2885.586, the d2 loss is: -3159.5938, the g loss is: 3122.6406, the ae loss is: 0.0067177108, the jacobian loss is:0.13918515\n",
            "This is the iter 7342, the d1 loss is: 2997.0781, the d2 loss is: -3187.9219, the g loss is: 3206.3438, the ae loss is: 0.0057972083, the jacobian loss is:0.20223123\n",
            "This is the iter 7343, the d1 loss is: 3004.9531, the d2 loss is: -3204.164, the g loss is: 3196.9844, the ae loss is: 0.0053235814, the jacobian loss is:0.11888183\n",
            "This is the iter 7344, the d1 loss is: 2753.461, the d2 loss is: -2999.5625, the g loss is: 2954.7656, the ae loss is: 0.0063307467, the jacobian loss is:0.112794556\n",
            "This is the iter 7345, the d1 loss is: 2934.4922, the d2 loss is: -3133.414, the g loss is: 3154.1172, the ae loss is: 0.008087037, the jacobian loss is:0.103249826\n",
            "This is the iter 7346, the d1 loss is: 2712.7344, the d2 loss is: -2903.8672, the g loss is: 2956.5469, the ae loss is: 0.007817758, the jacobian loss is:0.12850204\n",
            "This is the iter 7347, the d1 loss is: 2963.3594, the d2 loss is: -3182.2031, the g loss is: 3163.9375, the ae loss is: 0.0077295713, the jacobian loss is:0.1129379\n",
            "This is the iter 7348, the d1 loss is: 3036.7188, the d2 loss is: -3221.789, the g loss is: 3205.0156, the ae loss is: 0.0066453507, the jacobian loss is:0.12033527\n",
            "This is the iter 7349, the d1 loss is: 2960.4219, the d2 loss is: -3161.6953, the g loss is: 3175.3438, the ae loss is: 0.006247407, the jacobian loss is:0.10678662\n",
            "This is the iter 7350, the d1 loss is: 2969.3984, the d2 loss is: -3190.1484, the g loss is: 3155.9297, the ae loss is: 0.0059111333, the jacobian loss is:0.14102969\n",
            "This is the iter 7351, the d1 loss is: 3003.6172, the d2 loss is: -3188.3672, the g loss is: 3222.2969, the ae loss is: 0.00863548, the jacobian loss is:0.12068938\n",
            "This is the iter 7352, the d1 loss is: 2963.5, the d2 loss is: -3205.7188, the g loss is: 3171.0781, the ae loss is: 0.008535803, the jacobian loss is:0.1209932\n",
            "This is the iter 7353, the d1 loss is: 2973.6562, the d2 loss is: -3171.5469, the g loss is: 3159.2266, the ae loss is: 0.0072991904, the jacobian loss is:0.123607665\n",
            "This is the iter 7354, the d1 loss is: 2978.914, the d2 loss is: -3185.0938, the g loss is: 3225.7344, the ae loss is: 0.0070588198, the jacobian loss is:0.13262284\n",
            "This is the iter 7355, the d1 loss is: 3129.1719, the d2 loss is: -3317.2734, the g loss is: 3339.4375, the ae loss is: 0.008073455, the jacobian loss is:0.15432146\n",
            "This is the iter 7356, the d1 loss is: 2954.3594, the d2 loss is: -3168.5469, the g loss is: 3097.3125, the ae loss is: 0.005374755, the jacobian loss is:0.11189883\n",
            "This is the iter 7357, the d1 loss is: 2954.7266, the d2 loss is: -3154.414, the g loss is: 3185.2344, the ae loss is: 0.0067850333, the jacobian loss is:0.1171026\n",
            "This is the iter 7358, the d1 loss is: 2911.8828, the d2 loss is: -3099.3438, the g loss is: 3148.3828, the ae loss is: 0.006647948, the jacobian loss is:0.1496575\n",
            "This is the iter 7359, the d1 loss is: 2983.5469, the d2 loss is: -3200.7656, the g loss is: 3217.0938, the ae loss is: 0.00462595, the jacobian loss is:0.11775444\n",
            "This is the iter 7360, the d1 loss is: 2921.4922, the d2 loss is: -3127.2734, the g loss is: 3178.125, the ae loss is: 0.0057876385, the jacobian loss is:0.107946075\n",
            "This is the iter 7361, the d1 loss is: 3042.7969, the d2 loss is: -3223.3672, the g loss is: 3200.1875, the ae loss is: 0.006531945, the jacobian loss is:0.11742378\n",
            "This is the iter 7362, the d1 loss is: 2991.4922, the d2 loss is: -3220.4375, the g loss is: 3236.5469, the ae loss is: 0.0069361236, the jacobian loss is:0.1092146\n",
            "This is the iter 7363, the d1 loss is: 2884.2578, the d2 loss is: -3118.6406, the g loss is: 3109.2969, the ae loss is: 0.0076528396, the jacobian loss is:0.11621385\n",
            "This is the iter 7364, the d1 loss is: 3215.961, the d2 loss is: -3433.9062, the g loss is: 3462.6719, the ae loss is: 0.009533129, the jacobian loss is:0.09964379\n",
            "This is the iter 7365, the d1 loss is: 2988.2969, the d2 loss is: -3181.5469, the g loss is: 3213.4375, the ae loss is: 0.006073953, the jacobian loss is:0.1082526\n",
            "This is the iter 7366, the d1 loss is: 2789.4453, the d2 loss is: -3017.3281, the g loss is: 3066.75, the ae loss is: 0.006386612, the jacobian loss is:0.13294874\n",
            "This is the iter 7367, the d1 loss is: 2879.9219, the d2 loss is: -3079.7344, the g loss is: 3066.7188, the ae loss is: 0.008378153, the jacobian loss is:0.12776591\n",
            "This is the iter 7368, the d1 loss is: 3077.7656, the d2 loss is: -3261.6953, the g loss is: 3270.9453, the ae loss is: 0.00828632, the jacobian loss is:0.15090846\n",
            "This is the iter 7369, the d1 loss is: 2986.8516, the d2 loss is: -3191.0938, the g loss is: 3206.6562, the ae loss is: 0.0047534793, the jacobian loss is:0.11463785\n",
            "This is the iter 7370, the d1 loss is: 3235.664, the d2 loss is: -3433.8984, the g loss is: 3470.1172, the ae loss is: 0.0072643156, the jacobian loss is:0.13855098\n",
            "This is the iter 7371, the d1 loss is: 2900.4531, the d2 loss is: -3128.3906, the g loss is: 3138.625, the ae loss is: 0.0058059487, the jacobian loss is:0.12829722\n",
            "This is the iter 7372, the d1 loss is: 2991.8047, the d2 loss is: -3195.8047, the g loss is: 3185.8594, the ae loss is: 0.0061513023, the jacobian loss is:0.13907993\n",
            "This is the iter 7373, the d1 loss is: 2895.7031, the d2 loss is: -3079.3906, the g loss is: 3034.1875, the ae loss is: 0.0055450993, the jacobian loss is:0.14924356\n",
            "This is the iter 7374, the d1 loss is: 3171.9844, the d2 loss is: -3361.9844, the g loss is: 3406.9531, the ae loss is: 0.0061841486, the jacobian loss is:0.117251724\n",
            "This is the iter 7375, the d1 loss is: 2825.7812, the d2 loss is: -3053.9453, the g loss is: 3093.9766, the ae loss is: 0.0071432586, the jacobian loss is:0.121197574\n",
            "This is the iter 7376, the d1 loss is: 2959.7422, the d2 loss is: -3191.664, the g loss is: 3137.289, the ae loss is: 0.0061260574, the jacobian loss is:0.14300679\n",
            "This is the iter 7377, the d1 loss is: 3047.3281, the d2 loss is: -3244.3281, the g loss is: 3246.9297, the ae loss is: 0.008152643, the jacobian loss is:0.107438944\n",
            "This is the iter 7378, the d1 loss is: 2944.3594, the d2 loss is: -3165.7812, the g loss is: 3103.3047, the ae loss is: 0.009669811, the jacobian loss is:0.13961914\n",
            "This is the iter 7379, the d1 loss is: 3266.539, the d2 loss is: -3487.9844, the g loss is: 3474.8281, the ae loss is: 0.007931083, the jacobian loss is:0.11436333\n",
            "This is the iter 7380, the d1 loss is: 3006.3594, the d2 loss is: -3235.7578, the g loss is: 3220.539, the ae loss is: 0.0075106113, the jacobian loss is:0.15169604\n",
            "This is the iter 7381, the d1 loss is: 3008.1797, the d2 loss is: -3221.9062, the g loss is: 3187.289, the ae loss is: 0.007788222, the jacobian loss is:0.120264634\n",
            "This is the iter 7382, the d1 loss is: 2998.7656, the d2 loss is: -3202.0312, the g loss is: 3188.3125, the ae loss is: 0.00551653, the jacobian loss is:0.12136978\n",
            "This is the iter 7383, the d1 loss is: 3009.836, the d2 loss is: -3228.8672, the g loss is: 3251.3438, the ae loss is: 0.00575826, the jacobian loss is:0.10814911\n",
            "This is the iter 7384, the d1 loss is: 2994.7422, the d2 loss is: -3202.2969, the g loss is: 3229.3516, the ae loss is: 0.0037469896, the jacobian loss is:0.13502775\n",
            "This is the iter 7385, the d1 loss is: 2940.289, the d2 loss is: -3183.6328, the g loss is: 3171.4922, the ae loss is: 0.0056410027, the jacobian loss is:0.10100539\n",
            "This is the iter 7386, the d1 loss is: 2998.1797, the d2 loss is: -3207.1719, the g loss is: 3167.7812, the ae loss is: 0.0060733305, the jacobian loss is:0.13590756\n",
            "This is the iter 7387, the d1 loss is: 3329.3672, the d2 loss is: -3520.7656, the g loss is: 3560.8125, the ae loss is: 0.00705088, the jacobian loss is:0.117939115\n",
            "This is the iter 7388, the d1 loss is: 3288.5469, the d2 loss is: -3497.7969, the g loss is: 3482.3984, the ae loss is: 0.009245687, the jacobian loss is:0.14158013\n",
            "This is the iter 7389, the d1 loss is: 2996.039, the d2 loss is: -3183.2812, the g loss is: 3181.4297, the ae loss is: 0.008832287, the jacobian loss is:0.111773945\n",
            "This is the iter 7390, the d1 loss is: 3042.0078, the d2 loss is: -3251.8672, the g loss is: 3241.0625, the ae loss is: 0.0064649177, the jacobian loss is:0.15081345\n",
            "This is the iter 7391, the d1 loss is: 3104.5156, the d2 loss is: -3321.3281, the g loss is: 3264.961, the ae loss is: 0.0073297964, the jacobian loss is:0.13561782\n",
            "This is the iter 7392, the d1 loss is: 3032.0, the d2 loss is: -3206.6094, the g loss is: 3207.7969, the ae loss is: 0.005042581, the jacobian loss is:0.09791625\n",
            "This is the iter 7393, the d1 loss is: 3007.3125, the d2 loss is: -3211.6719, the g loss is: 3209.0703, the ae loss is: 0.005309964, the jacobian loss is:0.11671723\n",
            "This is the iter 7394, the d1 loss is: 3009.4922, the d2 loss is: -3248.6328, the g loss is: 3216.539, the ae loss is: 0.0055530174, the jacobian loss is:0.115883924\n",
            "This is the iter 7395, the d1 loss is: 3004.9688, the d2 loss is: -3227.414, the g loss is: 3189.3438, the ae loss is: 0.008244064, the jacobian loss is:0.102283806\n",
            "This is the iter 7396, the d1 loss is: 3342.25, the d2 loss is: -3557.7734, the g loss is: 3561.2656, the ae loss is: 0.007476694, the jacobian loss is:0.1320069\n",
            "This is the iter 7397, the d1 loss is: 3001.539, the d2 loss is: -3228.7812, the g loss is: 3279.4531, the ae loss is: 0.006575971, the jacobian loss is:0.11927519\n",
            "This is the iter 7398, the d1 loss is: 3023.9766, the d2 loss is: -3259.4531, the g loss is: 3275.5703, the ae loss is: 0.004402132, the jacobian loss is:0.1100685\n",
            "This is the iter 7399, the d1 loss is: 3265.3672, the d2 loss is: -3460.5469, the g loss is: 3486.7969, the ae loss is: 0.0052106082, the jacobian loss is:0.1147631\n",
            "This is the iter 7400, the d1 loss is: 3055.1875, the d2 loss is: -3259.9297, the g loss is: 3272.3125, the ae loss is: 0.008479318, the jacobian loss is:0.11413396\n",
            "0.2542241\n",
            "0.99429536\n",
            "This is the iter 7401, the d1 loss is: 3039.1094, the d2 loss is: -3235.5078, the g loss is: 3262.9688, the ae loss is: 0.007252536, the jacobian loss is:0.11847171\n",
            "This is the iter 7402, the d1 loss is: 3132.7969, the d2 loss is: -3315.3281, the g loss is: 3295.8906, the ae loss is: 0.0082024345, the jacobian loss is:0.10890208\n",
            "This is the iter 7403, the d1 loss is: 2983.9062, the d2 loss is: -3182.4922, the g loss is: 3229.3594, the ae loss is: 0.007734506, the jacobian loss is:0.12747136\n",
            "This is the iter 7404, the d1 loss is: 3077.625, the d2 loss is: -3259.1562, the g loss is: 3305.3828, the ae loss is: 0.009579722, the jacobian loss is:0.09264622\n",
            "This is the iter 7405, the d1 loss is: 2724.2266, the d2 loss is: -2931.6406, the g loss is: 2962.125, the ae loss is: 0.006699856, the jacobian loss is:0.13995291\n",
            "This is the iter 7406, the d1 loss is: 3105.4062, the d2 loss is: -3314.1016, the g loss is: 3352.7344, the ae loss is: 0.006160355, the jacobian loss is:0.09828069\n",
            "This is the iter 7407, the d1 loss is: 3170.1094, the d2 loss is: -3372.5938, the g loss is: 3333.4219, the ae loss is: 0.0051857806, the jacobian loss is:0.11126596\n",
            "This is the iter 7408, the d1 loss is: 2870.9219, the d2 loss is: -3083.6797, the g loss is: 3049.9531, the ae loss is: 0.008196263, the jacobian loss is:0.12784177\n",
            "This is the iter 7409, the d1 loss is: 2983.1172, the d2 loss is: -3190.2188, the g loss is: 3154.4531, the ae loss is: 0.0061163683, the jacobian loss is:0.11153977\n",
            "This is the iter 7410, the d1 loss is: 2998.336, the d2 loss is: -3191.3984, the g loss is: 3231.7578, the ae loss is: 0.0073868716, the jacobian loss is:0.12774016\n",
            "This is the iter 7411, the d1 loss is: 3049.4453, the d2 loss is: -3252.2656, the g loss is: 3234.5156, the ae loss is: 0.006944193, the jacobian loss is:0.09746083\n",
            "This is the iter 7412, the d1 loss is: 3125.2812, the d2 loss is: -3312.1484, the g loss is: 3343.5312, the ae loss is: 0.0070552444, the jacobian loss is:0.09490436\n",
            "This is the iter 7413, the d1 loss is: 3066.2422, the d2 loss is: -3281.0156, the g loss is: 3246.7969, the ae loss is: 0.006599415, the jacobian loss is:0.11639927\n",
            "This is the iter 7414, the d1 loss is: 3056.8906, the d2 loss is: -3236.7422, the g loss is: 3271.4766, the ae loss is: 0.004692555, the jacobian loss is:0.14115742\n",
            "This is the iter 7415, the d1 loss is: 3022.125, the d2 loss is: -3234.9297, the g loss is: 3282.4375, the ae loss is: 0.009032394, the jacobian loss is:0.13131386\n",
            "This is the iter 7416, the d1 loss is: 2956.8438, the d2 loss is: -3169.25, the g loss is: 3162.5312, the ae loss is: 0.007317809, the jacobian loss is:0.12212563\n",
            "This is the iter 7417, the d1 loss is: 2958.6875, the d2 loss is: -3128.3672, the g loss is: 3149.625, the ae loss is: 0.0067719705, the jacobian loss is:0.11187088\n",
            "This is the iter 7418, the d1 loss is: 2929.414, the d2 loss is: -3098.6328, the g loss is: 3184.4766, the ae loss is: 0.0071462253, the jacobian loss is:0.10294428\n",
            "This is the iter 7419, the d1 loss is: 2879.3672, the d2 loss is: -3079.1875, the g loss is: 3051.6719, the ae loss is: 0.0065847556, the jacobian loss is:0.123134725\n",
            "This is the iter 7420, the d1 loss is: 3060.5, the d2 loss is: -3245.6172, the g loss is: 3234.7656, the ae loss is: 0.007588187, the jacobian loss is:0.12529045\n",
            "This is the iter 7421, the d1 loss is: 3212.0, the d2 loss is: -3412.0703, the g loss is: 3431.8125, the ae loss is: 0.0055735223, the jacobian loss is:0.11669007\n",
            "This is the iter 7422, the d1 loss is: 3084.5781, the d2 loss is: -3272.125, the g loss is: 3253.1484, the ae loss is: 0.0050648656, the jacobian loss is:0.12054466\n",
            "This is the iter 7423, the d1 loss is: 3068.4844, the d2 loss is: -3249.5625, the g loss is: 3254.0781, the ae loss is: 0.005973612, the jacobian loss is:0.1304909\n",
            "This is the iter 7424, the d1 loss is: 3052.4531, the d2 loss is: -3272.9844, the g loss is: 3228.086, the ae loss is: 0.0073068645, the jacobian loss is:0.12457844\n",
            "This is the iter 7425, the d1 loss is: 3076.7734, the d2 loss is: -3261.2969, the g loss is: 3265.6172, the ae loss is: 0.0069384426, the jacobian loss is:0.101933286\n",
            "This is the iter 7426, the d1 loss is: 3089.1094, the d2 loss is: -3309.2344, the g loss is: 3302.7734, the ae loss is: 0.0077157957, the jacobian loss is:0.12544489\n",
            "This is the iter 7427, the d1 loss is: 3085.2422, the d2 loss is: -3292.5, the g loss is: 3266.336, the ae loss is: 0.005195637, the jacobian loss is:0.11671655\n",
            "This is the iter 7428, the d1 loss is: 3128.4219, the d2 loss is: -3316.7734, the g loss is: 3292.5, the ae loss is: 0.008850552, the jacobian loss is:0.14929545\n",
            "This is the iter 7429, the d1 loss is: 3067.2656, the d2 loss is: -3251.5234, the g loss is: 3280.3906, the ae loss is: 0.0045570843, the jacobian loss is:0.12638506\n",
            "This is the iter 7430, the d1 loss is: 2867.9219, the d2 loss is: -3065.539, the g loss is: 3122.7422, the ae loss is: 0.0070633674, the jacobian loss is:0.10814582\n",
            "This is the iter 7431, the d1 loss is: 2870.8516, the d2 loss is: -3084.0625, the g loss is: 3101.1328, the ae loss is: 0.006740297, the jacobian loss is:0.11199282\n",
            "This is the iter 7432, the d1 loss is: 3037.3516, the d2 loss is: -3240.6562, the g loss is: 3244.8984, the ae loss is: 0.006040762, the jacobian loss is:0.14223959\n",
            "This is the iter 7433, the d1 loss is: 3068.3984, the d2 loss is: -3283.2344, the g loss is: 3300.5156, the ae loss is: 0.005776194, the jacobian loss is:0.120901026\n",
            "This is the iter 7434, the d1 loss is: 3036.0078, the d2 loss is: -3259.1953, the g loss is: 3298.1094, the ae loss is: 0.0061469562, the jacobian loss is:0.12783135\n",
            "This is the iter 7435, the d1 loss is: 3056.3281, the d2 loss is: -3263.3672, the g loss is: 3261.125, the ae loss is: 0.007290231, the jacobian loss is:0.16852692\n",
            "This is the iter 7436, the d1 loss is: 3068.5156, the d2 loss is: -3251.4531, the g loss is: 3278.5625, the ae loss is: 0.006882851, the jacobian loss is:0.12706833\n",
            "This is the iter 7437, the d1 loss is: 2992.6562, the d2 loss is: -3211.8828, the g loss is: 3219.039, the ae loss is: 0.007389213, the jacobian loss is:0.16130787\n",
            "This is the iter 7438, the d1 loss is: 2950.8516, the d2 loss is: -3139.125, the g loss is: 3064.8828, the ae loss is: 0.011591027, the jacobian loss is:0.14722241\n",
            "This is the iter 7439, the d1 loss is: 3027.0938, the d2 loss is: -3246.9766, the g loss is: 3234.8281, the ae loss is: 0.0077557787, the jacobian loss is:0.11964868\n",
            "This is the iter 7440, the d1 loss is: 3108.3281, the d2 loss is: -3339.9766, the g loss is: 3298.3438, the ae loss is: 0.0066518416, the jacobian loss is:0.099017404\n",
            "This is the iter 7441, the d1 loss is: 3019.9219, the d2 loss is: -3229.9688, the g loss is: 3199.9922, the ae loss is: 0.008152444, the jacobian loss is:0.09165614\n",
            "This is the iter 7442, the d1 loss is: 2998.8516, the d2 loss is: -3194.8828, the g loss is: 3221.0, the ae loss is: 0.0068669515, the jacobian loss is:0.096301235\n",
            "This is the iter 7443, the d1 loss is: 3000.6016, the d2 loss is: -3180.461, the g loss is: 3212.5156, the ae loss is: 0.006930148, the jacobian loss is:0.11206066\n",
            "This is the iter 7444, the d1 loss is: 3272.8594, the d2 loss is: -3497.5, the g loss is: 3480.625, the ae loss is: 0.0063021737, the jacobian loss is:0.110673256\n",
            "This is the iter 7445, the d1 loss is: 2946.5469, the d2 loss is: -3206.9922, the g loss is: 3187.8281, the ae loss is: 0.008087268, the jacobian loss is:0.11446477\n",
            "This is the iter 7446, the d1 loss is: 3040.2266, the d2 loss is: -3241.6562, the g loss is: 3205.336, the ae loss is: 0.009151745, the jacobian loss is:0.21937232\n",
            "This is the iter 7447, the d1 loss is: 3105.4297, the d2 loss is: -3304.836, the g loss is: 3312.6797, the ae loss is: 0.0065916004, the jacobian loss is:0.11488244\n",
            "This is the iter 7448, the d1 loss is: 3085.1406, the d2 loss is: -3298.9844, the g loss is: 3291.789, the ae loss is: 0.007925638, the jacobian loss is:0.12097454\n",
            "This is the iter 7449, the d1 loss is: 3114.6562, the d2 loss is: -3291.6562, the g loss is: 3237.0, the ae loss is: 0.0060616955, the jacobian loss is:0.14778583\n",
            "This is the iter 7450, the d1 loss is: 3055.8125, the d2 loss is: -3281.5781, the g loss is: 3204.164, the ae loss is: 0.0078121526, the jacobian loss is:0.11645267\n",
            "This is the iter 7451, the d1 loss is: 2931.8125, the d2 loss is: -3114.5625, the g loss is: 3163.7266, the ae loss is: 0.0075572133, the jacobian loss is:0.10779919\n",
            "This is the iter 7452, the d1 loss is: 3253.2422, the d2 loss is: -3456.7969, the g loss is: 3529.086, the ae loss is: 0.0054183966, the jacobian loss is:0.104446486\n",
            "This is the iter 7453, the d1 loss is: 3047.375, the d2 loss is: -3220.1484, the g loss is: 3182.7344, the ae loss is: 0.006467693, the jacobian loss is:0.14800803\n",
            "This is the iter 7454, the d1 loss is: 3044.9922, the d2 loss is: -3254.289, the g loss is: 3305.6875, the ae loss is: 0.00653924, the jacobian loss is:0.10195974\n",
            "This is the iter 7455, the d1 loss is: 3283.4062, the d2 loss is: -3489.8125, the g loss is: 3430.0156, the ae loss is: 0.009112131, the jacobian loss is:0.11966032\n",
            "This is the iter 7456, the d1 loss is: 3294.4531, the d2 loss is: -3474.9375, the g loss is: 3462.7578, the ae loss is: 0.005406621, the jacobian loss is:0.12533182\n",
            "This is the iter 7457, the d1 loss is: 3096.1016, the d2 loss is: -3295.2422, the g loss is: 3316.9062, the ae loss is: 0.00759867, the jacobian loss is:0.11196754\n",
            "This is the iter 7458, the d1 loss is: 3023.8047, the d2 loss is: -3229.1406, the g loss is: 3327.8438, the ae loss is: 0.007222371, the jacobian loss is:0.1183231\n",
            "This is the iter 7459, the d1 loss is: 3068.4375, the d2 loss is: -3247.164, the g loss is: 3236.9297, the ae loss is: 0.0075151855, the jacobian loss is:0.12569769\n",
            "This is the iter 7460, the d1 loss is: 3088.2344, the d2 loss is: -3327.3906, the g loss is: 3335.1406, the ae loss is: 0.005735891, the jacobian loss is:0.11915478\n",
            "This is the iter 7461, the d1 loss is: 3062.875, the d2 loss is: -3282.375, the g loss is: 3259.6719, the ae loss is: 0.006469323, the jacobian loss is:0.10911655\n",
            "This is the iter 7462, the d1 loss is: 3145.2344, the d2 loss is: -3359.6484, the g loss is: 3375.2266, the ae loss is: 0.005211883, the jacobian loss is:0.102557346\n",
            "This is the iter 7463, the d1 loss is: 3039.836, the d2 loss is: -3214.1562, the g loss is: 3260.4766, the ae loss is: 0.008214481, the jacobian loss is:0.108153455\n",
            "This is the iter 7464, the d1 loss is: 3029.1953, the d2 loss is: -3260.3594, the g loss is: 3272.8125, the ae loss is: 0.004726784, the jacobian loss is:0.15216456\n",
            "This is the iter 7465, the d1 loss is: 3034.9453, the d2 loss is: -3255.5938, the g loss is: 3230.5781, the ae loss is: 0.0064959563, the jacobian loss is:0.13046233\n",
            "This is the iter 7466, the d1 loss is: 3065.6875, the d2 loss is: -3262.1406, the g loss is: 3247.3672, the ae loss is: 0.0068205227, the jacobian loss is:0.10025327\n",
            "This is the iter 7467, the d1 loss is: 3060.0547, the d2 loss is: -3282.0781, the g loss is: 3282.414, the ae loss is: 0.007092707, the jacobian loss is:0.109539665\n",
            "This is the iter 7468, the d1 loss is: 3091.2656, the d2 loss is: -3287.9219, the g loss is: 3254.7266, the ae loss is: 0.006991556, the jacobian loss is:0.19417484\n",
            "This is the iter 7469, the d1 loss is: 3081.5, the d2 loss is: -3280.7031, the g loss is: 3297.0547, the ae loss is: 0.00549454, the jacobian loss is:0.105896205\n",
            "This is the iter 7470, the d1 loss is: 3109.6406, the d2 loss is: -3288.5469, the g loss is: 3264.4688, the ae loss is: 0.008895524, the jacobian loss is:0.13558179\n",
            "This is the iter 7471, the d1 loss is: 3287.789, the d2 loss is: -3526.414, the g loss is: 3493.1406, the ae loss is: 0.008420681, the jacobian loss is:0.12722935\n",
            "This is the iter 7472, the d1 loss is: 3131.3125, the d2 loss is: -3320.2188, the g loss is: 3307.9219, the ae loss is: 0.0052582636, the jacobian loss is:0.10801608\n",
            "This is the iter 7473, the d1 loss is: 2968.1016, the d2 loss is: -3193.7031, the g loss is: 3257.086, the ae loss is: 0.005189068, the jacobian loss is:0.104573056\n",
            "This is the iter 7474, the d1 loss is: 3032.8125, the d2 loss is: -3245.461, the g loss is: 3325.3125, the ae loss is: 0.0056030466, the jacobian loss is:0.11752348\n",
            "This is the iter 7475, the d1 loss is: 3060.8594, the d2 loss is: -3283.586, the g loss is: 3259.9297, the ae loss is: 0.00619618, the jacobian loss is:0.12851407\n",
            "This is the iter 7476, the d1 loss is: 3120.1875, the d2 loss is: -3300.5625, the g loss is: 3321.1406, the ae loss is: 0.0050167674, the jacobian loss is:0.12836902\n",
            "This is the iter 7477, the d1 loss is: 3194.9219, the d2 loss is: -3411.3828, the g loss is: 3437.0, the ae loss is: 0.005591801, the jacobian loss is:0.096845984\n",
            "This is the iter 7478, the d1 loss is: 3104.3125, the d2 loss is: -3319.3516, the g loss is: 3286.9453, the ae loss is: 0.0068244147, the jacobian loss is:0.12040952\n",
            "This is the iter 7479, the d1 loss is: 2948.6719, the d2 loss is: -3129.0312, the g loss is: 3115.8594, the ae loss is: 0.0072894413, the jacobian loss is:0.11815718\n",
            "This is the iter 7480, the d1 loss is: 3144.2656, the d2 loss is: -3325.9375, the g loss is: 3325.5781, the ae loss is: 0.0060216333, the jacobian loss is:0.14683798\n",
            "This is the iter 7481, the d1 loss is: 3309.25, the d2 loss is: -3517.711, the g loss is: 3514.1562, the ae loss is: 0.0083388155, the jacobian loss is:0.11903858\n",
            "This is the iter 7482, the d1 loss is: 3133.6875, the d2 loss is: -3358.0312, the g loss is: 3340.2031, the ae loss is: 0.0045776875, the jacobian loss is:0.100937665\n",
            "This is the iter 7483, the d1 loss is: 3043.1406, the d2 loss is: -3272.8828, the g loss is: 3318.4219, the ae loss is: 0.005759849, the jacobian loss is:0.11983973\n",
            "This is the iter 7484, the d1 loss is: 3033.6328, the d2 loss is: -3247.8828, the g loss is: 3279.9688, the ae loss is: 0.0064058984, the jacobian loss is:0.1323044\n",
            "This is the iter 7485, the d1 loss is: 3041.3125, the d2 loss is: -3243.1562, the g loss is: 3285.8672, the ae loss is: 0.0072697527, the jacobian loss is:0.10157799\n",
            "This is the iter 7486, the d1 loss is: 3103.8281, the d2 loss is: -3300.8594, the g loss is: 3316.7188, the ae loss is: 0.008278192, the jacobian loss is:0.14926952\n",
            "This is the iter 7487, the d1 loss is: 3132.3984, the d2 loss is: -3329.0312, the g loss is: 3318.0547, the ae loss is: 0.0077699847, the jacobian loss is:0.12353681\n",
            "This is the iter 7488, the d1 loss is: 3024.2422, the d2 loss is: -3200.2344, the g loss is: 3197.8203, the ae loss is: 0.0044040373, the jacobian loss is:0.1289663\n",
            "This is the iter 7489, the d1 loss is: 3024.4766, the d2 loss is: -3237.5469, the g loss is: 3229.5234, the ae loss is: 0.007480283, the jacobian loss is:0.108183734\n",
            "This is the iter 7490, the d1 loss is: 3164.0156, the d2 loss is: -3348.836, the g loss is: 3361.3438, the ae loss is: 0.006862818, the jacobian loss is:0.10752507\n",
            "This is the iter 7491, the d1 loss is: 3112.336, the d2 loss is: -3302.289, the g loss is: 3313.7578, the ae loss is: 0.0065446687, the jacobian loss is:0.12700027\n",
            "This is the iter 7492, the d1 loss is: 3232.3516, the d2 loss is: -3423.4219, the g loss is: 3407.6875, the ae loss is: 0.0077066873, the jacobian loss is:0.10604181\n",
            "This is the iter 7493, the d1 loss is: 3103.5156, the d2 loss is: -3311.7266, the g loss is: 3310.4844, the ae loss is: 0.005566806, the jacobian loss is:0.110386044\n",
            "This is the iter 7494, the d1 loss is: 3119.3984, the d2 loss is: -3331.1562, the g loss is: 3363.3281, the ae loss is: 0.00681014, the jacobian loss is:0.0996567\n",
            "This is the iter 7495, the d1 loss is: 2805.5156, the d2 loss is: -2956.8594, the g loss is: 2949.9219, the ae loss is: 0.008505258, the jacobian loss is:0.1171383\n",
            "This is the iter 7496, the d1 loss is: 3012.7266, the d2 loss is: -3201.6016, the g loss is: 3212.7188, the ae loss is: 0.0052172076, the jacobian loss is:0.11622479\n",
            "This is the iter 7497, the d1 loss is: 3079.6328, the d2 loss is: -3283.8281, the g loss is: 3281.2422, the ae loss is: 0.0059620887, the jacobian loss is:0.11748978\n",
            "This is the iter 7498, the d1 loss is: 3028.9453, the d2 loss is: -3240.3203, the g loss is: 3226.25, the ae loss is: 0.005602263, the jacobian loss is:0.09698611\n",
            "This is the iter 7499, the d1 loss is: 3010.4844, the d2 loss is: -3188.836, the g loss is: 3166.8984, the ae loss is: 0.006401946, the jacobian loss is:0.11471636\n",
            "This is the iter 7500, the d1 loss is: 3004.1562, the d2 loss is: -3227.8203, the g loss is: 3262.914, the ae loss is: 0.0047144787, the jacobian loss is:0.13622037\n",
            "0.25567043\n",
            "1.003914\n",
            "This is the iter 7501, the d1 loss is: 3003.7422, the d2 loss is: -3167.9219, the g loss is: 3175.4531, the ae loss is: 0.005074274, the jacobian loss is:0.12538902\n",
            "This is the iter 7502, the d1 loss is: 2972.6094, the d2 loss is: -3177.5547, the g loss is: 3104.789, the ae loss is: 0.006649155, the jacobian loss is:0.1141468\n",
            "This is the iter 7503, the d1 loss is: 3026.1094, the d2 loss is: -3251.3984, the g loss is: 3182.1719, the ae loss is: 0.007721167, the jacobian loss is:0.112401366\n",
            "This is the iter 7504, the d1 loss is: 3148.1016, the d2 loss is: -3342.6719, the g loss is: 3319.6172, the ae loss is: 0.0049790954, the jacobian loss is:0.12440953\n",
            "This is the iter 7505, the d1 loss is: 3041.9375, the d2 loss is: -3204.7188, the g loss is: 3211.9688, the ae loss is: 0.0070147133, the jacobian loss is:0.11452333\n",
            "This is the iter 7506, the d1 loss is: 3031.2656, the d2 loss is: -3218.3984, the g loss is: 3240.7969, the ae loss is: 0.0068310136, the jacobian loss is:0.13651995\n",
            "This is the iter 7507, the d1 loss is: 2991.0469, the d2 loss is: -3177.8438, the g loss is: 3227.1953, the ae loss is: 0.007386608, the jacobian loss is:0.105472244\n",
            "This is the iter 7508, the d1 loss is: 3109.5312, the d2 loss is: -3291.1406, the g loss is: 3316.7578, the ae loss is: 0.0065746806, the jacobian loss is:0.09804376\n",
            "This is the iter 7509, the d1 loss is: 3237.1328, the d2 loss is: -3412.9062, the g loss is: 3383.3516, the ae loss is: 0.0057262555, the jacobian loss is:0.10075004\n",
            "This is the iter 7510, the d1 loss is: 3017.8125, the d2 loss is: -3211.4844, the g loss is: 3187.8516, the ae loss is: 0.0047136038, the jacobian loss is:0.14720966\n",
            "This is the iter 7511, the d1 loss is: 3088.7969, the d2 loss is: -3292.4062, the g loss is: 3270.4688, the ae loss is: 0.0064480267, the jacobian loss is:0.10762433\n",
            "This is the iter 7512, the d1 loss is: 3053.8594, the d2 loss is: -3235.1875, the g loss is: 3248.8047, the ae loss is: 0.0071291327, the jacobian loss is:0.09149782\n",
            "This is the iter 7513, the d1 loss is: 2901.3594, the d2 loss is: -3082.375, the g loss is: 3147.8281, the ae loss is: 0.0078120995, the jacobian loss is:0.12622316\n",
            "This is the iter 7514, the d1 loss is: 3098.8281, the d2 loss is: -3278.5469, the g loss is: 3293.6562, the ae loss is: 0.0057426924, the jacobian loss is:0.098542824\n",
            "This is the iter 7515, the d1 loss is: 2717.961, the d2 loss is: -2928.4844, the g loss is: 2929.6875, the ae loss is: 0.005424809, the jacobian loss is:0.098301105\n",
            "This is the iter 7516, the d1 loss is: 2859.1719, the d2 loss is: -3052.3125, the g loss is: 3044.0547, the ae loss is: 0.0067142155, the jacobian loss is:0.09456627\n",
            "This is the iter 7517, the d1 loss is: 3027.8594, the d2 loss is: -3198.336, the g loss is: 3168.9219, the ae loss is: 0.007396622, the jacobian loss is:0.13854353\n",
            "This is the iter 7518, the d1 loss is: 2997.3125, the d2 loss is: -3196.961, the g loss is: 3198.789, the ae loss is: 0.009807656, the jacobian loss is:0.10985135\n",
            "This is the iter 7519, the d1 loss is: 3360.6406, the d2 loss is: -3565.5547, the g loss is: 3557.5625, the ae loss is: 0.009927385, the jacobian loss is:0.15043034\n",
            "This is the iter 7520, the d1 loss is: 2877.625, the d2 loss is: -3076.125, the g loss is: 3065.1562, the ae loss is: 0.0069439467, the jacobian loss is:0.098883815\n",
            "This is the iter 7521, the d1 loss is: 2922.3594, the d2 loss is: -3110.5312, the g loss is: 3153.3203, the ae loss is: 0.0046021673, the jacobian loss is:0.10524751\n",
            "This is the iter 7522, the d1 loss is: 2770.3906, the d2 loss is: -2969.875, the g loss is: 2955.4922, the ae loss is: 0.005408003, the jacobian loss is:0.10779801\n",
            "This is the iter 7523, the d1 loss is: 2996.2422, the d2 loss is: -3195.0, the g loss is: 3144.4844, the ae loss is: 0.006293253, the jacobian loss is:0.16491707\n",
            "This is the iter 7524, the d1 loss is: 3002.6016, the d2 loss is: -3186.5312, the g loss is: 3179.4062, the ae loss is: 0.0053559104, the jacobian loss is:0.09587579\n",
            "This is the iter 7525, the d1 loss is: 3016.539, the d2 loss is: -3195.3906, the g loss is: 3241.1875, the ae loss is: 0.007319161, the jacobian loss is:0.14019963\n",
            "This is the iter 7526, the d1 loss is: 2887.3516, the d2 loss is: -3078.0312, the g loss is: 3193.7188, the ae loss is: 0.008829191, the jacobian loss is:0.10830812\n",
            "This is the iter 7527, the d1 loss is: 2958.4922, the d2 loss is: -3162.375, the g loss is: 3182.0781, the ae loss is: 0.0072882986, the jacobian loss is:0.1205809\n",
            "This is the iter 7528, the d1 loss is: 3031.0234, the d2 loss is: -3221.2031, the g loss is: 3187.6875, the ae loss is: 0.006795374, the jacobian loss is:0.120805375\n",
            "This is the iter 7529, the d1 loss is: 2971.6719, the d2 loss is: -3145.4688, the g loss is: 3225.4922, the ae loss is: 0.006871635, the jacobian loss is:0.12325974\n",
            "This is the iter 7530, the d1 loss is: 2981.7344, the d2 loss is: -3143.9219, the g loss is: 3275.7266, the ae loss is: 0.005781895, the jacobian loss is:0.107386015\n",
            "This is the iter 7531, the d1 loss is: 2919.4688, the d2 loss is: -3132.4062, the g loss is: 3143.8203, the ae loss is: 0.0064967237, the jacobian loss is:0.10252825\n",
            "This is the iter 7532, the d1 loss is: 2967.1016, the d2 loss is: -3149.3594, the g loss is: 3174.1094, the ae loss is: 0.00762254, the jacobian loss is:0.1263084\n",
            "This is the iter 7533, the d1 loss is: 2955.3125, the d2 loss is: -3155.8125, the g loss is: 3136.6562, the ae loss is: 0.004001789, the jacobian loss is:0.10949344\n",
            "This is the iter 7534, the d1 loss is: 3078.2812, the d2 loss is: -3290.3281, the g loss is: 3279.1328, the ae loss is: 0.006633424, the jacobian loss is:0.10751184\n",
            "This is the iter 7535, the d1 loss is: 2852.8984, the d2 loss is: -3046.4688, the g loss is: 3050.3828, the ae loss is: 0.005004242, the jacobian loss is:0.10382311\n",
            "This is the iter 7536, the d1 loss is: 2997.6562, the d2 loss is: -3206.9844, the g loss is: 3151.9844, the ae loss is: 0.0070743826, the jacobian loss is:0.15903321\n",
            "This is the iter 7537, the d1 loss is: 3054.2969, the d2 loss is: -3247.8438, the g loss is: 3194.7812, the ae loss is: 0.0055888025, the jacobian loss is:0.14815976\n",
            "This is the iter 7538, the d1 loss is: 3020.6797, the d2 loss is: -3205.8594, the g loss is: 3196.4375, the ae loss is: 0.0065586017, the jacobian loss is:0.09891954\n",
            "This is the iter 7539, the d1 loss is: 3218.6094, the d2 loss is: -3396.0234, the g loss is: 3421.625, the ae loss is: 0.007532654, the jacobian loss is:0.10519161\n",
            "This is the iter 7540, the d1 loss is: 3283.0469, the d2 loss is: -3491.4219, the g loss is: 3479.25, the ae loss is: 0.0063412488, the jacobian loss is:0.102835625\n",
            "This is the iter 7541, the d1 loss is: 2852.9844, the d2 loss is: -3026.7734, the g loss is: 3022.2344, the ae loss is: 0.0059548765, the jacobian loss is:0.1107401\n",
            "This is the iter 7542, the d1 loss is: 3182.2031, the d2 loss is: -3367.4531, the g loss is: 3417.336, the ae loss is: 0.0064685037, the jacobian loss is:0.10558981\n",
            "This is the iter 7543, the d1 loss is: 2950.2656, the d2 loss is: -3153.7266, the g loss is: 3183.7422, the ae loss is: 0.0079194475, the jacobian loss is:0.123488024\n",
            "This is the iter 7544, the d1 loss is: 2994.3438, the d2 loss is: -3192.6172, the g loss is: 3195.1562, the ae loss is: 0.0066870824, the jacobian loss is:0.1262338\n",
            "This is the iter 7545, the d1 loss is: 3034.8047, the d2 loss is: -3222.5, the g loss is: 3222.1797, the ae loss is: 0.0061434973, the jacobian loss is:0.10482898\n",
            "This is the iter 7546, the d1 loss is: 3047.6562, the d2 loss is: -3219.461, the g loss is: 3235.9688, the ae loss is: 0.006355513, the jacobian loss is:0.10321443\n",
            "This is the iter 7547, the d1 loss is: 2994.4531, the d2 loss is: -3200.4844, the g loss is: 3197.2188, the ae loss is: 0.008455831, the jacobian loss is:0.10111043\n",
            "This is the iter 7548, the d1 loss is: 3151.664, the d2 loss is: -3386.414, the g loss is: 3399.8906, the ae loss is: 0.005885109, the jacobian loss is:0.09758417\n",
            "This is the iter 7549, the d1 loss is: 3022.875, the d2 loss is: -3223.4062, the g loss is: 3201.9375, the ae loss is: 0.0074066846, the jacobian loss is:0.11044132\n",
            "This is the iter 7550, the d1 loss is: 3109.7656, the d2 loss is: -3294.3984, the g loss is: 3219.7578, the ae loss is: 0.00626776, the jacobian loss is:0.11976894\n",
            "This is the iter 7551, the d1 loss is: 2973.0781, the d2 loss is: -3182.9219, the g loss is: 3201.625, the ae loss is: 0.0055843266, the jacobian loss is:0.109657414\n",
            "This is the iter 7552, the d1 loss is: 3074.0156, the d2 loss is: -3248.7656, the g loss is: 3216.211, the ae loss is: 0.007620953, the jacobian loss is:0.10250025\n",
            "This is the iter 7553, the d1 loss is: 3003.9688, the d2 loss is: -3214.1016, the g loss is: 3175.1328, the ae loss is: 0.0066627823, the jacobian loss is:0.10287364\n",
            "This is the iter 7554, the d1 loss is: 3048.8438, the d2 loss is: -3270.7734, the g loss is: 3282.2812, the ae loss is: 0.005557479, the jacobian loss is:0.12816527\n",
            "This is the iter 7555, the d1 loss is: 3042.8281, the d2 loss is: -3243.586, the g loss is: 3285.8984, the ae loss is: 0.007614335, the jacobian loss is:0.10635466\n",
            "This is the iter 7556, the d1 loss is: 3117.8047, the d2 loss is: -3289.7188, the g loss is: 3277.3672, the ae loss is: 0.008322544, the jacobian loss is:0.122892685\n",
            "This is the iter 7557, the d1 loss is: 3424.1406, the d2 loss is: -3604.6406, the g loss is: 3589.1953, the ae loss is: 0.0048495447, the jacobian loss is:0.13672115\n",
            "This is the iter 7558, the d1 loss is: 3052.2812, the d2 loss is: -3255.8281, the g loss is: 3219.7578, the ae loss is: 0.010371586, the jacobian loss is:0.119173184\n",
            "This is the iter 7559, the d1 loss is: 3041.6016, the d2 loss is: -3218.961, the g loss is: 3205.9922, the ae loss is: 0.0060615055, the jacobian loss is:0.107566364\n",
            "This is the iter 7560, the d1 loss is: 3094.5156, the d2 loss is: -3281.0703, the g loss is: 3242.3047, the ae loss is: 0.0078045446, the jacobian loss is:0.12455725\n",
            "This is the iter 7561, the d1 loss is: 3030.1797, the d2 loss is: -3232.9766, the g loss is: 3212.5703, the ae loss is: 0.0065158317, the jacobian loss is:0.09765269\n",
            "This is the iter 7562, the d1 loss is: 3084.4844, the d2 loss is: -3259.789, the g loss is: 3209.5312, the ae loss is: 0.006991534, the jacobian loss is:0.12649624\n",
            "This is the iter 7563, the d1 loss is: 3086.7422, the d2 loss is: -3249.5469, the g loss is: 3236.3594, the ae loss is: 0.006193386, the jacobian loss is:0.11077138\n",
            "This is the iter 7564, the d1 loss is: 3039.9844, the d2 loss is: -3234.7656, the g loss is: 3217.2812, the ae loss is: 0.006129219, the jacobian loss is:0.1443205\n",
            "This is the iter 7565, the d1 loss is: 3072.8594, the d2 loss is: -3249.3906, the g loss is: 3214.1797, the ae loss is: 0.0077766543, the jacobian loss is:0.15297195\n",
            "This is the iter 7566, the d1 loss is: 2950.7812, the d2 loss is: -3131.6562, the g loss is: 3181.1875, the ae loss is: 0.006081952, the jacobian loss is:0.13083625\n",
            "This is the iter 7567, the d1 loss is: 3153.5312, the d2 loss is: -3351.2578, the g loss is: 3377.1562, the ae loss is: 0.0069538793, the jacobian loss is:0.101271026\n",
            "This is the iter 7568, the d1 loss is: 3020.2578, the d2 loss is: -3230.5234, the g loss is: 3225.3047, the ae loss is: 0.006579014, the jacobian loss is:0.10685922\n",
            "This is the iter 7569, the d1 loss is: 3263.3438, the d2 loss is: -3472.9219, the g loss is: 3470.6953, the ae loss is: 0.0055553243, the jacobian loss is:0.10546665\n",
            "This is the iter 7570, the d1 loss is: 2983.8047, the d2 loss is: -3172.414, the g loss is: 3212.7969, the ae loss is: 0.0069383997, the jacobian loss is:0.10063045\n",
            "This is the iter 7571, the d1 loss is: 3104.8047, the d2 loss is: -3302.4297, the g loss is: 3302.375, the ae loss is: 0.005568411, the jacobian loss is:0.07843978\n",
            "This is the iter 7572, the d1 loss is: 2948.5, the d2 loss is: -3119.6484, the g loss is: 3126.7656, the ae loss is: 0.0059339767, the jacobian loss is:0.11065148\n",
            "This is the iter 7573, the d1 loss is: 3077.9219, the d2 loss is: -3265.1562, the g loss is: 3251.5625, the ae loss is: 0.005143552, the jacobian loss is:0.10118707\n",
            "This is the iter 7574, the d1 loss is: 3130.1094, the d2 loss is: -3308.1562, the g loss is: 3293.8125, the ae loss is: 0.0062642205, the jacobian loss is:0.1261751\n",
            "This is the iter 7575, the d1 loss is: 3110.2266, the d2 loss is: -3291.9844, the g loss is: 3280.1562, the ae loss is: 0.0070961337, the jacobian loss is:0.11132618\n",
            "This is the iter 7576, the d1 loss is: 2788.4531, the d2 loss is: -3033.25, the g loss is: 3003.4844, the ae loss is: 0.007649772, the jacobian loss is:0.10882015\n",
            "This is the iter 7577, the d1 loss is: 3120.4844, the d2 loss is: -3292.4844, the g loss is: 3341.1406, the ae loss is: 0.008214597, the jacobian loss is:0.1122801\n",
            "This is the iter 7578, the d1 loss is: 3147.7344, the d2 loss is: -3353.3125, the g loss is: 3329.6016, the ae loss is: 0.0056288973, the jacobian loss is:0.08966815\n",
            "This is the iter 7579, the d1 loss is: 3060.0547, the d2 loss is: -3268.0547, the g loss is: 3244.9766, the ae loss is: 0.004779416, the jacobian loss is:0.101134494\n",
            "This is the iter 7580, the d1 loss is: 2905.0156, the d2 loss is: -3105.414, the g loss is: 3100.7188, the ae loss is: 0.0045676753, the jacobian loss is:0.11238445\n",
            "This is the iter 7581, the d1 loss is: 2788.5, the d2 loss is: -2983.9531, the g loss is: 3017.086, the ae loss is: 0.004743401, the jacobian loss is:0.09666501\n",
            "This is the iter 7582, the d1 loss is: 2985.4453, the d2 loss is: -3188.3828, the g loss is: 3202.1484, the ae loss is: 0.008289405, the jacobian loss is:0.13185097\n",
            "This is the iter 7583, the d1 loss is: 3020.0312, the d2 loss is: -3240.2969, the g loss is: 3263.3906, the ae loss is: 0.006402009, the jacobian loss is:0.094044834\n",
            "This is the iter 7584, the d1 loss is: 3260.664, the d2 loss is: -3469.8828, the g loss is: 3446.6719, the ae loss is: 0.00794606, the jacobian loss is:0.120978825\n",
            "This is the iter 7585, the d1 loss is: 3011.1172, the d2 loss is: -3227.8594, the g loss is: 3218.914, the ae loss is: 0.005555195, the jacobian loss is:0.08764131\n",
            "This is the iter 7586, the d1 loss is: 2884.8828, the d2 loss is: -3098.164, the g loss is: 3064.289, the ae loss is: 0.0064682625, the jacobian loss is:0.13991345\n",
            "This is the iter 7587, the d1 loss is: 3129.8828, the d2 loss is: -3283.5, the g loss is: 3287.8047, the ae loss is: 0.007035783, the jacobian loss is:0.101037785\n",
            "This is the iter 7588, the d1 loss is: 3292.539, the d2 loss is: -3510.5625, the g loss is: 3522.8672, the ae loss is: 0.010417053, the jacobian loss is:0.12648897\n",
            "This is the iter 7589, the d1 loss is: 2932.7031, the d2 loss is: -3153.836, the g loss is: 3149.3438, the ae loss is: 0.005002684, the jacobian loss is:0.081048146\n",
            "This is the iter 7590, the d1 loss is: 3311.3047, the d2 loss is: -3489.711, the g loss is: 3470.8984, the ae loss is: 0.007539057, the jacobian loss is:0.10087594\n",
            "This is the iter 7591, the d1 loss is: 3009.0156, the d2 loss is: -3191.961, the g loss is: 3219.3828, the ae loss is: 0.006859583, the jacobian loss is:0.11983277\n",
            "This is the iter 7592, the d1 loss is: 3041.7344, the d2 loss is: -3241.7344, the g loss is: 3223.664, the ae loss is: 0.007154299, the jacobian loss is:0.07837674\n",
            "This is the iter 7593, the d1 loss is: 3170.2656, the d2 loss is: -3343.2969, the g loss is: 3368.7734, the ae loss is: 0.007916852, the jacobian loss is:0.0827463\n",
            "This is the iter 7594, the d1 loss is: 3054.3984, the d2 loss is: -3282.211, the g loss is: 3290.5469, the ae loss is: 0.006495614, the jacobian loss is:0.097914666\n",
            "This is the iter 7595, the d1 loss is: 3102.8984, the d2 loss is: -3302.1094, the g loss is: 3283.0938, the ae loss is: 0.00554268, the jacobian loss is:0.08827909\n",
            "This is the iter 7596, the d1 loss is: 3009.3438, the d2 loss is: -3215.1016, the g loss is: 3167.0625, the ae loss is: 0.0062672305, the jacobian loss is:0.1308357\n",
            "This is the iter 7597, the d1 loss is: 3038.8984, the d2 loss is: -3246.6172, the g loss is: 3221.5469, the ae loss is: 0.0066563245, the jacobian loss is:0.09259034\n",
            "This is the iter 7598, the d1 loss is: 3248.289, the d2 loss is: -3425.3984, the g loss is: 3475.9297, the ae loss is: 0.0047473274, the jacobian loss is:0.10583065\n",
            "This is the iter 7599, the d1 loss is: 3123.5938, the d2 loss is: -3298.2734, the g loss is: 3325.5469, the ae loss is: 0.006083639, the jacobian loss is:0.09309172\n",
            "This is the iter 7600, the d1 loss is: 3126.2812, the d2 loss is: -3287.5234, the g loss is: 3287.9453, the ae loss is: 0.009050547, the jacobian loss is:0.13433269\n",
            "0.25797105\n",
            "1.0078417\n",
            "This is the iter 7601, the d1 loss is: 3297.1016, the d2 loss is: -3520.1484, the g loss is: 3469.0312, the ae loss is: 0.006595608, the jacobian loss is:0.12302313\n",
            "This is the iter 7602, the d1 loss is: 3067.1484, the d2 loss is: -3284.4844, the g loss is: 3265.1016, the ae loss is: 0.006983292, the jacobian loss is:0.13010257\n",
            "This is the iter 7603, the d1 loss is: 2892.0234, the d2 loss is: -3059.3984, the g loss is: 3014.8672, the ae loss is: 0.00801661, the jacobian loss is:0.110352516\n",
            "This is the iter 7604, the d1 loss is: 3288.5, the d2 loss is: -3459.4766, the g loss is: 3525.7188, the ae loss is: 0.0075301174, the jacobian loss is:0.13258205\n",
            "This is the iter 7605, the d1 loss is: 3329.5078, the d2 loss is: -3529.3516, the g loss is: 3493.0703, the ae loss is: 0.006491483, the jacobian loss is:0.114532486\n",
            "This is the iter 7606, the d1 loss is: 2766.375, the d2 loss is: -2952.7734, the g loss is: 2929.3594, the ae loss is: 0.006557024, the jacobian loss is:0.10351547\n",
            "This is the iter 7607, the d1 loss is: 3120.3516, the d2 loss is: -3316.789, the g loss is: 3302.7578, the ae loss is: 0.0060338713, the jacobian loss is:0.0818142\n",
            "This is the iter 7608, the d1 loss is: 3404.2422, the d2 loss is: -3565.8672, the g loss is: 3483.6875, the ae loss is: 0.009215764, the jacobian loss is:0.1186746\n",
            "This is the iter 7609, the d1 loss is: 3022.5938, the d2 loss is: -3253.9922, the g loss is: 3275.9688, the ae loss is: 0.0059559452, the jacobian loss is:0.119299926\n",
            "This is the iter 7610, the d1 loss is: 2894.039, the d2 loss is: -3074.461, the g loss is: 3083.9375, the ae loss is: 0.0063681696, the jacobian loss is:0.10253349\n",
            "This is the iter 7611, the d1 loss is: 3185.6094, the d2 loss is: -3348.9766, the g loss is: 3313.586, the ae loss is: 0.00806479, the jacobian loss is:0.10673038\n",
            "This is the iter 7612, the d1 loss is: 3005.9062, the d2 loss is: -3190.5781, the g loss is: 3199.3125, the ae loss is: 0.0050910446, the jacobian loss is:0.1063802\n",
            "This is the iter 7613, the d1 loss is: 3313.039, the d2 loss is: -3501.664, the g loss is: 3480.3516, the ae loss is: 0.009136347, the jacobian loss is:0.13107318\n",
            "This is the iter 7614, the d1 loss is: 2998.9844, the d2 loss is: -3169.0469, the g loss is: 3186.164, the ae loss is: 0.008803121, the jacobian loss is:0.15417886\n",
            "This is the iter 7615, the d1 loss is: 3078.3281, the d2 loss is: -3302.211, the g loss is: 3292.3672, the ae loss is: 0.0071218703, the jacobian loss is:0.11402459\n",
            "This is the iter 7616, the d1 loss is: 3105.3438, the d2 loss is: -3294.4375, the g loss is: 3282.7266, the ae loss is: 0.007898392, the jacobian loss is:0.10259085\n",
            "This is the iter 7617, the d1 loss is: 3113.4219, the d2 loss is: -3314.3281, the g loss is: 3267.8438, the ae loss is: 0.009110867, the jacobian loss is:0.09599332\n",
            "This is the iter 7618, the d1 loss is: 3155.3125, the d2 loss is: -3320.2188, the g loss is: 3290.0781, the ae loss is: 0.0050365636, the jacobian loss is:0.10720649\n",
            "This is the iter 7619, the d1 loss is: 3113.8125, the d2 loss is: -3288.3672, the g loss is: 3291.2969, the ae loss is: 0.0058083446, the jacobian loss is:0.10266367\n",
            "This is the iter 7620, the d1 loss is: 3351.6328, the d2 loss is: -3507.4688, the g loss is: 3473.5625, the ae loss is: 0.0066305255, the jacobian loss is:0.09225543\n",
            "This is the iter 7621, the d1 loss is: 3301.7031, the d2 loss is: -3505.5625, the g loss is: 3433.5469, the ae loss is: 0.0073547685, the jacobian loss is:0.11193429\n",
            "This is the iter 7622, the d1 loss is: 3065.9453, the d2 loss is: -3290.9766, the g loss is: 3298.4766, the ae loss is: 0.005981641, the jacobian loss is:0.09729305\n",
            "This is the iter 7623, the d1 loss is: 2914.3828, the d2 loss is: -3106.8828, the g loss is: 3074.9531, the ae loss is: 0.0047624432, the jacobian loss is:0.0998058\n",
            "This is the iter 7624, the d1 loss is: 3086.125, the d2 loss is: -3304.5625, the g loss is: 3272.6406, the ae loss is: 0.0074846526, the jacobian loss is:0.08991254\n",
            "This is the iter 7625, the d1 loss is: 3005.461, the d2 loss is: -3190.8672, the g loss is: 3216.8438, the ae loss is: 0.007120087, the jacobian loss is:0.09218172\n",
            "This is the iter 7626, the d1 loss is: 2960.7031, the d2 loss is: -3126.6484, the g loss is: 3213.1406, the ae loss is: 0.009371557, the jacobian loss is:0.09169892\n",
            "This is the iter 7627, the d1 loss is: 3217.6094, the d2 loss is: -3389.7266, the g loss is: 3371.0156, the ae loss is: 0.006532656, the jacobian loss is:0.11331845\n",
            "This is the iter 7628, the d1 loss is: 3027.3203, the d2 loss is: -3245.0234, the g loss is: 3278.0156, the ae loss is: 0.0071230573, the jacobian loss is:0.09138364\n",
            "This is the iter 7629, the d1 loss is: 2986.7812, the d2 loss is: -3135.6094, the g loss is: 3116.0781, the ae loss is: 0.004524828, the jacobian loss is:0.089913085\n",
            "This is the iter 7630, the d1 loss is: 3397.3125, the d2 loss is: -3579.0469, the g loss is: 3538.7578, the ae loss is: 0.007741214, the jacobian loss is:0.08926433\n",
            "This is the iter 7631, the d1 loss is: 3185.6172, the d2 loss is: -3363.2344, the g loss is: 3404.9062, the ae loss is: 0.007292999, the jacobian loss is:0.169618\n",
            "This is the iter 7632, the d1 loss is: 3113.0469, the d2 loss is: -3326.9297, the g loss is: 3327.4766, the ae loss is: 0.004963972, the jacobian loss is:0.105266795\n",
            "This is the iter 7633, the d1 loss is: 3115.0938, the d2 loss is: -3315.5078, the g loss is: 3337.7969, the ae loss is: 0.00842617, the jacobian loss is:0.0929563\n",
            "This is the iter 7634, the d1 loss is: 3109.7656, the d2 loss is: -3269.6797, the g loss is: 3292.1562, the ae loss is: 0.007885646, the jacobian loss is:0.08728822\n",
            "This is the iter 7635, the d1 loss is: 3388.5234, the d2 loss is: -3573.125, the g loss is: 3597.789, the ae loss is: 0.0060234983, the jacobian loss is:0.09326295\n",
            "This is the iter 7636, the d1 loss is: 3223.3047, the d2 loss is: -3403.8438, the g loss is: 3383.2344, the ae loss is: 0.0072742742, the jacobian loss is:0.10144622\n",
            "This is the iter 7637, the d1 loss is: 3185.086, the d2 loss is: -3341.5156, the g loss is: 3342.3203, the ae loss is: 0.0048090597, the jacobian loss is:0.11382583\n",
            "This is the iter 7638, the d1 loss is: 3129.8203, the d2 loss is: -3327.6953, the g loss is: 3361.75, the ae loss is: 0.005837663, the jacobian loss is:0.13000472\n",
            "This is the iter 7639, the d1 loss is: 3069.7188, the d2 loss is: -3254.0234, the g loss is: 3289.9531, the ae loss is: 0.005489868, the jacobian loss is:0.09872564\n",
            "This is the iter 7640, the d1 loss is: 3041.9297, the d2 loss is: -3208.4062, the g loss is: 3310.0156, the ae loss is: 0.006584268, the jacobian loss is:0.11367213\n",
            "This is the iter 7641, the d1 loss is: 3037.5, the d2 loss is: -3250.9453, the g loss is: 3287.2969, the ae loss is: 0.006521761, the jacobian loss is:0.12821847\n",
            "This is the iter 7642, the d1 loss is: 2975.2578, the d2 loss is: -3151.4453, the g loss is: 3167.3125, the ae loss is: 0.0071506836, the jacobian loss is:0.098494366\n",
            "This is the iter 7643, the d1 loss is: 2944.5547, the d2 loss is: -3154.1328, the g loss is: 3190.3281, the ae loss is: 0.0056525357, the jacobian loss is:0.11124889\n",
            "This is the iter 7644, the d1 loss is: 3075.375, the d2 loss is: -3223.1328, the g loss is: 3264.0, the ae loss is: 0.0058059157, the jacobian loss is:0.100442335\n",
            "This is the iter 7645, the d1 loss is: 3118.7188, the d2 loss is: -3291.0625, the g loss is: 3296.6953, the ae loss is: 0.006795447, the jacobian loss is:0.12050721\n",
            "This is the iter 7646, the d1 loss is: 2998.6719, the d2 loss is: -3178.1172, the g loss is: 3178.5703, the ae loss is: 0.0075937505, the jacobian loss is:0.090911366\n",
            "This is the iter 7647, the d1 loss is: 3100.9922, the d2 loss is: -3296.0156, the g loss is: 3260.9531, the ae loss is: 0.005940479, the jacobian loss is:0.09416155\n",
            "This is the iter 7648, the d1 loss is: 3121.0625, the d2 loss is: -3315.1406, the g loss is: 3298.6016, the ae loss is: 0.007700039, the jacobian loss is:0.09505152\n",
            "This is the iter 7649, the d1 loss is: 3105.6797, the d2 loss is: -3286.461, the g loss is: 3269.0312, the ae loss is: 0.007357843, the jacobian loss is:0.11153585\n",
            "This is the iter 7650, the d1 loss is: 3134.2734, the d2 loss is: -3324.7969, the g loss is: 3270.8516, the ae loss is: 0.010031286, the jacobian loss is:0.10283741\n",
            "This is the iter 7651, the d1 loss is: 3103.1719, the d2 loss is: -3288.5938, the g loss is: 3206.8828, the ae loss is: 0.008022104, the jacobian loss is:0.15566674\n",
            "This is the iter 7652, the d1 loss is: 3077.0781, the d2 loss is: -3250.4297, the g loss is: 3273.6406, the ae loss is: 0.00567774, the jacobian loss is:0.09204434\n",
            "This is the iter 7653, the d1 loss is: 3068.2188, the d2 loss is: -3233.8594, the g loss is: 3206.8906, the ae loss is: 0.0056209164, the jacobian loss is:0.096539125\n",
            "This is the iter 7654, the d1 loss is: 2998.7812, the d2 loss is: -3175.3438, the g loss is: 3246.461, the ae loss is: 0.0048531396, the jacobian loss is:0.0726866\n",
            "This is the iter 7655, the d1 loss is: 3161.4375, the d2 loss is: -3347.3594, the g loss is: 3315.1719, the ae loss is: 0.008494998, the jacobian loss is:0.0980527\n",
            "This is the iter 7656, the d1 loss is: 3143.3906, the d2 loss is: -3308.7344, the g loss is: 3249.625, the ae loss is: 0.006814274, the jacobian loss is:0.1334763\n",
            "This is the iter 7657, the d1 loss is: 2846.4375, the d2 loss is: -3019.375, the g loss is: 3083.5469, the ae loss is: 0.0062453533, the jacobian loss is:0.12535729\n",
            "This is the iter 7658, the d1 loss is: 2993.7031, the d2 loss is: -3173.4688, the g loss is: 3164.1016, the ae loss is: 0.0071366653, the jacobian loss is:0.094810165\n",
            "This is the iter 7659, the d1 loss is: 3384.5703, the d2 loss is: -3539.7031, the g loss is: 3518.7969, the ae loss is: 0.007189086, the jacobian loss is:0.09547272\n",
            "This is the iter 7660, the d1 loss is: 3191.1328, the d2 loss is: -3374.4922, the g loss is: 3376.5234, the ae loss is: 0.006502255, the jacobian loss is:0.081479706\n",
            "This is the iter 7661, the d1 loss is: 3383.9219, the d2 loss is: -3572.7812, the g loss is: 3555.375, the ae loss is: 0.0059398394, the jacobian loss is:0.10003987\n",
            "This is the iter 7662, the d1 loss is: 3240.7422, the d2 loss is: -3392.1016, the g loss is: 3338.6172, the ae loss is: 0.010899445, the jacobian loss is:0.14703941\n",
            "This is the iter 7663, the d1 loss is: 3229.4062, the d2 loss is: -3377.9531, the g loss is: 3453.3906, the ae loss is: 0.0069668773, the jacobian loss is:0.09609395\n",
            "This is the iter 7664, the d1 loss is: 2945.2656, the d2 loss is: -3136.3594, the g loss is: 3103.2812, the ae loss is: 0.007823988, the jacobian loss is:0.10385124\n",
            "This is the iter 7665, the d1 loss is: 3113.2188, the d2 loss is: -3301.625, the g loss is: 3280.4922, the ae loss is: 0.005916533, the jacobian loss is:0.10055479\n",
            "This is the iter 7666, the d1 loss is: 3140.5625, the d2 loss is: -3307.289, the g loss is: 3292.664, the ae loss is: 0.00928685, the jacobian loss is:0.111916415\n",
            "This is the iter 7667, the d1 loss is: 3102.414, the d2 loss is: -3286.7734, the g loss is: 3270.5625, the ae loss is: 0.0061007086, the jacobian loss is:0.10196069\n",
            "This is the iter 7668, the d1 loss is: 3116.1328, the d2 loss is: -3299.9375, the g loss is: 3293.4844, the ae loss is: 0.0074500595, the jacobian loss is:0.11245257\n",
            "This is the iter 7669, the d1 loss is: 3070.875, the d2 loss is: -3286.4219, the g loss is: 3318.6797, the ae loss is: 0.0071123997, the jacobian loss is:0.11468097\n",
            "This is the iter 7670, the d1 loss is: 3200.914, the d2 loss is: -3398.914, the g loss is: 3345.9766, the ae loss is: 0.0058357557, the jacobian loss is:0.09999109\n",
            "This is the iter 7671, the d1 loss is: 3123.7266, the d2 loss is: -3337.3203, the g loss is: 3289.875, the ae loss is: 0.0057163527, the jacobian loss is:0.08159045\n",
            "This is the iter 7672, the d1 loss is: 3228.0547, the d2 loss is: -3408.961, the g loss is: 3407.8438, the ae loss is: 0.006844818, the jacobian loss is:0.104994096\n",
            "This is the iter 7673, the d1 loss is: 3047.25, the d2 loss is: -3244.8438, the g loss is: 3213.1797, the ae loss is: 0.0068008113, the jacobian loss is:0.123301454\n",
            "This is the iter 7674, the d1 loss is: 3340.8906, the d2 loss is: -3542.5156, the g loss is: 3565.211, the ae loss is: 0.0068904376, the jacobian loss is:0.10922948\n",
            "This is the iter 7675, the d1 loss is: 2878.1719, the d2 loss is: -3049.8281, the g loss is: 3019.711, the ae loss is: 0.0053842207, the jacobian loss is:0.19218783\n",
            "This is the iter 7676, the d1 loss is: 3115.711, the d2 loss is: -3324.625, the g loss is: 3258.2188, the ae loss is: 0.008227863, the jacobian loss is:0.08980259\n",
            "This is the iter 7677, the d1 loss is: 3136.625, the d2 loss is: -3324.7031, the g loss is: 3246.8438, the ae loss is: 0.0071033994, the jacobian loss is:0.14989148\n",
            "This is the iter 7678, the d1 loss is: 3115.164, the d2 loss is: -3306.0703, the g loss is: 3310.914, the ae loss is: 0.007632607, the jacobian loss is:0.0991552\n",
            "This is the iter 7679, the d1 loss is: 3377.9531, the d2 loss is: -3533.6875, the g loss is: 3523.1719, the ae loss is: 0.0068193525, the jacobian loss is:0.1361314\n",
            "This is the iter 7680, the d1 loss is: 3191.6875, the d2 loss is: -3382.3281, the g loss is: 3342.1406, the ae loss is: 0.0063409586, the jacobian loss is:0.16938837\n",
            "This is the iter 7681, the d1 loss is: 2871.711, the d2 loss is: -3071.6094, the g loss is: 3106.8125, the ae loss is: 0.008654892, the jacobian loss is:0.109366305\n",
            "This is the iter 7682, the d1 loss is: 3171.1406, the d2 loss is: -3349.5234, the g loss is: 3344.1172, the ae loss is: 0.009481807, the jacobian loss is:0.13990708\n",
            "This is the iter 7683, the d1 loss is: 2999.836, the d2 loss is: -3159.4531, the g loss is: 3109.4219, the ae loss is: 0.005525239, the jacobian loss is:0.10899106\n",
            "This is the iter 7684, the d1 loss is: 3120.7734, the d2 loss is: -3307.7031, the g loss is: 3316.1172, the ae loss is: 0.0065634623, the jacobian loss is:0.14056447\n",
            "This is the iter 7685, the d1 loss is: 3102.0625, the d2 loss is: -3289.0625, the g loss is: 3257.4844, the ae loss is: 0.006723199, the jacobian loss is:0.16355494\n",
            "This is the iter 7686, the d1 loss is: 3068.1094, the d2 loss is: -3234.0703, the g loss is: 3342.2188, the ae loss is: 0.0067753196, the jacobian loss is:0.13291547\n",
            "This is the iter 7687, the d1 loss is: 3138.1875, the d2 loss is: -3305.8125, the g loss is: 3300.0234, the ae loss is: 0.007982692, the jacobian loss is:0.12819616\n",
            "This is the iter 7688, the d1 loss is: 3057.914, the d2 loss is: -3259.125, the g loss is: 3199.625, the ae loss is: 0.010087293, the jacobian loss is:0.13596985\n",
            "This is the iter 7689, the d1 loss is: 3095.5, the d2 loss is: -3282.289, the g loss is: 3286.1953, the ae loss is: 0.008329813, the jacobian loss is:0.12468185\n",
            "This is the iter 7690, the d1 loss is: 3179.625, the d2 loss is: -3366.6016, the g loss is: 3366.0625, the ae loss is: 0.009128638, the jacobian loss is:0.103968136\n",
            "This is the iter 7691, the d1 loss is: 3128.7578, the d2 loss is: -3320.0234, the g loss is: 3295.6328, the ae loss is: 0.00807279, the jacobian loss is:0.113630764\n",
            "This is the iter 7692, the d1 loss is: 3159.1406, the d2 loss is: -3351.8672, the g loss is: 3326.6484, the ae loss is: 0.0058179153, the jacobian loss is:0.12608607\n",
            "This is the iter 7693, the d1 loss is: 3054.289, the d2 loss is: -3248.75, the g loss is: 3259.8047, the ae loss is: 0.007844401, the jacobian loss is:0.099348955\n",
            "This is the iter 7694, the d1 loss is: 3224.625, the d2 loss is: -3372.9062, the g loss is: 3304.4375, the ae loss is: 0.008314104, the jacobian loss is:0.098564446\n",
            "This is the iter 7695, the d1 loss is: 3125.336, the d2 loss is: -3303.0469, the g loss is: 3294.25, the ae loss is: 0.006059712, the jacobian loss is:0.09821407\n",
            "This is the iter 7696, the d1 loss is: 3159.0625, the d2 loss is: -3333.2578, the g loss is: 3335.0469, the ae loss is: 0.0072904094, the jacobian loss is:0.1080159\n",
            "This is the iter 7697, the d1 loss is: 2817.4531, the d2 loss is: -3007.336, the g loss is: 2989.8828, the ae loss is: 0.0050323876, the jacobian loss is:0.11058559\n",
            "This is the iter 7698, the d1 loss is: 3173.6094, the d2 loss is: -3340.9531, the g loss is: 3380.0703, the ae loss is: 0.004785897, the jacobian loss is:0.12980507\n",
            "This is the iter 7699, the d1 loss is: 3077.5234, the d2 loss is: -3290.2344, the g loss is: 3299.625, the ae loss is: 0.0064970404, the jacobian loss is:0.10668082\n",
            "This is the iter 7700, the d1 loss is: 3179.1328, the d2 loss is: -3342.6094, the g loss is: 3356.9766, the ae loss is: 0.0053878976, the jacobian loss is:0.096185505\n",
            "0.25583825\n",
            "0.9941726\n",
            "This is the iter 7701, the d1 loss is: 3096.1875, the d2 loss is: -3267.1875, the g loss is: 3296.5781, the ae loss is: 0.0038855486, the jacobian loss is:0.13460709\n",
            "This is the iter 7702, the d1 loss is: 3095.7812, the d2 loss is: -3297.9453, the g loss is: 3228.6484, the ae loss is: 0.005471903, the jacobian loss is:0.10171054\n",
            "This is the iter 7703, the d1 loss is: 2747.75, the d2 loss is: -2959.5, the g loss is: 2931.836, the ae loss is: 0.007940253, the jacobian loss is:0.1135599\n",
            "This is the iter 7704, the d1 loss is: 2979.0312, the d2 loss is: -3136.5156, the g loss is: 3175.6094, the ae loss is: 0.0073499894, the jacobian loss is:0.13893102\n",
            "This is the iter 7705, the d1 loss is: 2948.9297, the d2 loss is: -3163.3438, the g loss is: 3152.1328, the ae loss is: 0.006727453, the jacobian loss is:0.11156928\n",
            "This is the iter 7706, the d1 loss is: 3102.1797, the d2 loss is: -3274.125, the g loss is: 3254.7656, the ae loss is: 0.0071654385, the jacobian loss is:0.13176644\n",
            "This is the iter 7707, the d1 loss is: 3033.1875, the d2 loss is: -3210.3203, the g loss is: 3222.5938, the ae loss is: 0.007497645, the jacobian loss is:0.10302414\n",
            "This is the iter 7708, the d1 loss is: 3012.7344, the d2 loss is: -3199.9375, the g loss is: 3172.8438, the ae loss is: 0.009113686, the jacobian loss is:0.13715369\n",
            "This is the iter 7709, the d1 loss is: 2981.461, the d2 loss is: -3178.1562, the g loss is: 3166.461, the ae loss is: 0.011460857, the jacobian loss is:0.10696245\n",
            "This is the iter 7710, the d1 loss is: 3103.25, the d2 loss is: -3271.0547, the g loss is: 3240.2031, the ae loss is: 0.007662177, the jacobian loss is:0.12269591\n",
            "This is the iter 7711, the d1 loss is: 2971.4844, the d2 loss is: -3148.9219, the g loss is: 3167.9531, the ae loss is: 0.0074023837, the jacobian loss is:0.1002656\n",
            "This is the iter 7712, the d1 loss is: 3020.7734, the d2 loss is: -3236.7578, the g loss is: 3210.4375, the ae loss is: 0.005762158, the jacobian loss is:0.09917811\n",
            "This is the iter 7713, the d1 loss is: 3193.1562, the d2 loss is: -3392.75, the g loss is: 3369.4062, the ae loss is: 0.006987529, the jacobian loss is:0.10509032\n",
            "This is the iter 7714, the d1 loss is: 3029.6875, the d2 loss is: -3225.8984, the g loss is: 3194.4453, the ae loss is: 0.008426449, the jacobian loss is:0.11239856\n",
            "This is the iter 7715, the d1 loss is: 3016.5312, the d2 loss is: -3208.8906, the g loss is: 3230.1484, the ae loss is: 0.0056395447, the jacobian loss is:0.07894639\n",
            "This is the iter 7716, the d1 loss is: 3057.539, the d2 loss is: -3240.2266, the g loss is: 3233.5547, the ae loss is: 0.006630269, the jacobian loss is:0.10149675\n",
            "This is the iter 7717, the d1 loss is: 3033.7812, the d2 loss is: -3229.3047, the g loss is: 3243.1875, the ae loss is: 0.0075532505, the jacobian loss is:0.09427105\n",
            "This is the iter 7718, the d1 loss is: 2971.3828, the d2 loss is: -3166.5625, the g loss is: 3135.0312, the ae loss is: 0.0060058986, the jacobian loss is:0.085601516\n",
            "This is the iter 7719, the d1 loss is: 3084.289, the d2 loss is: -3263.3438, the g loss is: 3265.0625, the ae loss is: 0.005379694, the jacobian loss is:0.1348765\n",
            "This is the iter 7720, the d1 loss is: 3221.0, the d2 loss is: -3395.4453, the g loss is: 3395.3594, the ae loss is: 0.009216389, the jacobian loss is:0.1097099\n",
            "This is the iter 7721, the d1 loss is: 3081.3672, the d2 loss is: -3258.6172, the g loss is: 3244.5234, the ae loss is: 0.00435824, the jacobian loss is:0.12528709\n",
            "This is the iter 7722, the d1 loss is: 3014.6875, the d2 loss is: -3223.9844, the g loss is: 3230.6406, the ae loss is: 0.0046052914, the jacobian loss is:0.10757987\n",
            "This is the iter 7723, the d1 loss is: 3137.625, the d2 loss is: -3318.4062, the g loss is: 3288.6328, the ae loss is: 0.0067181615, the jacobian loss is:0.10053984\n",
            "This is the iter 7724, the d1 loss is: 3151.1406, the d2 loss is: -3359.8828, the g loss is: 3309.0625, the ae loss is: 0.0088360235, the jacobian loss is:0.107628435\n",
            "This is the iter 7725, the d1 loss is: 2871.4766, the d2 loss is: -3046.5781, the g loss is: 3032.0625, the ae loss is: 0.0034114476, the jacobian loss is:0.07418909\n",
            "This is the iter 7726, the d1 loss is: 3055.9688, the d2 loss is: -3237.2031, the g loss is: 3254.2188, the ae loss is: 0.004965659, the jacobian loss is:0.10252479\n",
            "This is the iter 7727, the d1 loss is: 3018.2812, the d2 loss is: -3186.7031, the g loss is: 3146.3281, the ae loss is: 0.0074131917, the jacobian loss is:0.10676359\n",
            "This is the iter 7728, the d1 loss is: 3045.0547, the d2 loss is: -3259.625, the g loss is: 3250.5781, the ae loss is: 0.005912102, the jacobian loss is:0.113247775\n",
            "This is the iter 7729, the d1 loss is: 3091.2656, the d2 loss is: -3253.8281, the g loss is: 3210.0078, the ae loss is: 0.007797733, the jacobian loss is:0.101177596\n",
            "This is the iter 7730, the d1 loss is: 3018.9375, the d2 loss is: -3170.1406, the g loss is: 3201.7812, the ae loss is: 0.0068216827, the jacobian loss is:0.114011444\n",
            "This is the iter 7731, the d1 loss is: 3044.0938, the d2 loss is: -3220.836, the g loss is: 3212.3438, the ae loss is: 0.0057453685, the jacobian loss is:0.09218733\n",
            "This is the iter 7732, the d1 loss is: 3133.7344, the d2 loss is: -3320.6797, the g loss is: 3312.7188, the ae loss is: 0.006708227, the jacobian loss is:0.1745624\n",
            "This is the iter 7733, the d1 loss is: 2952.4453, the d2 loss is: -3158.5469, the g loss is: 3161.375, the ae loss is: 0.008110486, the jacobian loss is:0.09429098\n",
            "This is the iter 7734, the d1 loss is: 2974.164, the d2 loss is: -3156.6172, the g loss is: 3163.8047, the ae loss is: 0.011075981, the jacobian loss is:0.08561164\n",
            "This is the iter 7735, the d1 loss is: 3021.8984, the d2 loss is: -3214.8828, the g loss is: 3204.6328, the ae loss is: 0.0056682117, the jacobian loss is:0.100033924\n",
            "This is the iter 7736, the d1 loss is: 3100.5078, the d2 loss is: -3277.0781, the g loss is: 3252.3125, the ae loss is: 0.009260155, the jacobian loss is:0.12308788\n",
            "This is the iter 7737, the d1 loss is: 3041.7422, the d2 loss is: -3213.6172, the g loss is: 3198.0156, the ae loss is: 0.007200245, the jacobian loss is:0.09510463\n",
            "This is the iter 7738, the d1 loss is: 3019.0625, the d2 loss is: -3187.3906, the g loss is: 3227.0078, the ae loss is: 0.006024545, the jacobian loss is:0.076614834\n",
            "This is the iter 7739, the d1 loss is: 3020.125, the d2 loss is: -3175.2656, the g loss is: 3242.1719, the ae loss is: 0.007861442, the jacobian loss is:0.11317054\n",
            "This is the iter 7740, the d1 loss is: 3025.414, the d2 loss is: -3228.4766, the g loss is: 3203.2578, the ae loss is: 0.0066601215, the jacobian loss is:0.08704404\n",
            "This is the iter 7741, the d1 loss is: 2934.4062, the d2 loss is: -3158.6953, the g loss is: 3221.6562, the ae loss is: 0.0065699196, the jacobian loss is:0.08994906\n",
            "This is the iter 7742, the d1 loss is: 3205.6953, the d2 loss is: -3351.2344, the g loss is: 3379.7344, the ae loss is: 0.009168854, the jacobian loss is:0.09833672\n",
            "This is the iter 7743, the d1 loss is: 2950.0938, the d2 loss is: -3102.5234, the g loss is: 3124.9297, the ae loss is: 0.0075029884, the jacobian loss is:0.09061244\n",
            "This is the iter 7744, the d1 loss is: 3050.9844, the d2 loss is: -3235.8594, the g loss is: 3230.3594, the ae loss is: 0.009710605, the jacobian loss is:0.084794834\n",
            "This is the iter 7745, the d1 loss is: 2949.0156, the d2 loss is: -3104.0469, the g loss is: 3175.789, the ae loss is: 0.0049663885, the jacobian loss is:0.08672923\n",
            "This is the iter 7746, the d1 loss is: 3209.789, the d2 loss is: -3395.1016, the g loss is: 3403.1484, the ae loss is: 0.006348454, the jacobian loss is:0.07997201\n",
            "This is the iter 7747, the d1 loss is: 2992.625, the d2 loss is: -3115.8516, the g loss is: 3142.1953, the ae loss is: 0.0104295, the jacobian loss is:0.082804084\n",
            "This is the iter 7748, the d1 loss is: 2825.539, the d2 loss is: -3008.2734, the g loss is: 3038.75, the ae loss is: 0.0070774257, the jacobian loss is:0.08505726\n",
            "This is the iter 7749, the d1 loss is: 2932.2734, the d2 loss is: -3124.3438, the g loss is: 3097.836, the ae loss is: 0.008158152, the jacobian loss is:0.08469181\n",
            "This is the iter 7750, the d1 loss is: 2931.5703, the d2 loss is: -3142.6172, the g loss is: 3163.7734, the ae loss is: 0.005240776, the jacobian loss is:0.08014697\n",
            "This is the iter 7751, the d1 loss is: 3066.961, the d2 loss is: -3230.9219, the g loss is: 3290.4922, the ae loss is: 0.0042752423, the jacobian loss is:0.21506508\n",
            "This is the iter 7752, the d1 loss is: 3010.9062, the d2 loss is: -3192.1953, the g loss is: 3172.289, the ae loss is: 0.007548713, the jacobian loss is:0.07639304\n",
            "This is the iter 7753, the d1 loss is: 2966.9844, the d2 loss is: -3138.0703, the g loss is: 3101.4453, the ae loss is: 0.006123637, the jacobian loss is:0.10986727\n",
            "This is the iter 7754, the d1 loss is: 3132.5938, the d2 loss is: -3296.7578, the g loss is: 3276.6797, the ae loss is: 0.006329138, the jacobian loss is:0.10787764\n",
            "This is the iter 7755, the d1 loss is: 2986.4219, the d2 loss is: -3168.5312, the g loss is: 3116.4219, the ae loss is: 0.0074417153, the jacobian loss is:0.09609891\n",
            "This is the iter 7756, the d1 loss is: 3007.2422, the d2 loss is: -3175.9453, the g loss is: 3187.2031, the ae loss is: 0.005969111, the jacobian loss is:0.11430729\n",
            "This is the iter 7757, the d1 loss is: 2977.0938, the d2 loss is: -3169.461, the g loss is: 3167.8828, the ae loss is: 0.007971997, the jacobian loss is:0.094799764\n",
            "This is the iter 7758, the d1 loss is: 3177.336, the d2 loss is: -3367.7344, the g loss is: 3313.5312, the ae loss is: 0.0051795384, the jacobian loss is:0.10914858\n",
            "This is the iter 7759, the d1 loss is: 3055.3125, the d2 loss is: -3233.6719, the g loss is: 3259.3672, the ae loss is: 0.0057473825, the jacobian loss is:0.09282423\n",
            "This is the iter 7760, the d1 loss is: 3079.3906, the d2 loss is: -3265.1797, the g loss is: 3242.4062, the ae loss is: 0.006541921, the jacobian loss is:0.08548905\n",
            "This is the iter 7761, the d1 loss is: 3004.1719, the d2 loss is: -3166.039, the g loss is: 3173.1094, the ae loss is: 0.007230779, the jacobian loss is:0.10425508\n",
            "This is the iter 7762, the d1 loss is: 3018.2031, the d2 loss is: -3192.289, the g loss is: 3201.6484, the ae loss is: 0.005431364, the jacobian loss is:0.073881164\n",
            "This is the iter 7763, the d1 loss is: 3394.8828, the d2 loss is: -3574.9375, the g loss is: 3561.3125, the ae loss is: 0.0069264374, the jacobian loss is:0.07769119\n",
            "This is the iter 7764, the d1 loss is: 2900.4062, the d2 loss is: -3087.2188, the g loss is: 3142.0, the ae loss is: 0.007061742, the jacobian loss is:0.10663208\n",
            "This is the iter 7765, the d1 loss is: 3255.3281, the d2 loss is: -3457.4531, the g loss is: 3348.0469, the ae loss is: 0.0070619863, the jacobian loss is:0.09970353\n",
            "This is the iter 7766, the d1 loss is: 3096.3516, the d2 loss is: -3271.5703, the g loss is: 3223.914, the ae loss is: 0.006787013, the jacobian loss is:0.116485156\n",
            "This is the iter 7767, the d1 loss is: 3046.6562, the d2 loss is: -3244.6875, the g loss is: 3233.7188, the ae loss is: 0.0070472765, the jacobian loss is:0.107247196\n",
            "This is the iter 7768, the d1 loss is: 3039.9375, the d2 loss is: -3222.625, the g loss is: 3214.375, the ae loss is: 0.0067270314, the jacobian loss is:0.12248563\n",
            "This is the iter 7769, the d1 loss is: 2998.0156, the d2 loss is: -3157.711, the g loss is: 3205.7266, the ae loss is: 0.004991182, the jacobian loss is:0.09538885\n",
            "This is the iter 7770, the d1 loss is: 3294.0312, the d2 loss is: -3509.4062, the g loss is: 3515.2266, the ae loss is: 0.0060062553, the jacobian loss is:0.11814906\n",
            "This is the iter 7771, the d1 loss is: 3216.539, the d2 loss is: -3383.8516, the g loss is: 3346.6484, the ae loss is: 0.0068090972, the jacobian loss is:0.09043128\n",
            "This is the iter 7772, the d1 loss is: 3129.7031, the d2 loss is: -3319.8984, the g loss is: 3256.6875, the ae loss is: 0.0064647384, the jacobian loss is:0.14731587\n",
            "This is the iter 7773, the d1 loss is: 3240.711, the d2 loss is: -3419.5938, the g loss is: 3436.4688, the ae loss is: 0.006220091, the jacobian loss is:0.09181251\n",
            "This is the iter 7774, the d1 loss is: 3030.3047, the d2 loss is: -3214.1484, the g loss is: 3235.3594, the ae loss is: 0.006646237, the jacobian loss is:0.11802266\n",
            "This is the iter 7775, the d1 loss is: 3046.4062, the d2 loss is: -3232.1016, the g loss is: 3235.0469, the ae loss is: 0.006964593, the jacobian loss is:0.09514261\n",
            "This is the iter 7776, the d1 loss is: 3018.789, the d2 loss is: -3174.8672, the g loss is: 3176.039, the ae loss is: 0.0065807374, the jacobian loss is:0.093622416\n",
            "This is the iter 7777, the d1 loss is: 3027.4219, the d2 loss is: -3212.3438, the g loss is: 3242.039, the ae loss is: 0.007020387, the jacobian loss is:0.10660503\n",
            "This is the iter 7778, the d1 loss is: 3139.5781, the d2 loss is: -3304.8672, the g loss is: 3326.6094, the ae loss is: 0.0049148463, the jacobian loss is:0.16483752\n",
            "This is the iter 7779, the d1 loss is: 3001.7344, the d2 loss is: -3172.9375, the g loss is: 3258.1172, the ae loss is: 0.006681146, the jacobian loss is:0.10499176\n",
            "This is the iter 7780, the d1 loss is: 3092.125, the d2 loss is: -3292.3906, the g loss is: 3316.5469, the ae loss is: 0.005146305, the jacobian loss is:0.092240356\n",
            "This is the iter 7781, the d1 loss is: 3147.4844, the d2 loss is: -3302.625, the g loss is: 3316.6406, the ae loss is: 0.006967568, the jacobian loss is:0.09488874\n",
            "This is the iter 7782, the d1 loss is: 3062.5781, the d2 loss is: -3233.625, the g loss is: 3330.1875, the ae loss is: 0.005758996, the jacobian loss is:0.08199067\n",
            "This is the iter 7783, the d1 loss is: 3028.2188, the d2 loss is: -3212.836, the g loss is: 3185.7188, the ae loss is: 0.006600115, the jacobian loss is:0.12104993\n",
            "This is the iter 7784, the d1 loss is: 3320.836, the d2 loss is: -3502.4531, the g loss is: 3455.5078, the ae loss is: 0.006008289, the jacobian loss is:0.11457635\n",
            "This is the iter 7785, the d1 loss is: 2824.9375, the d2 loss is: -3010.711, the g loss is: 2967.4766, the ae loss is: 0.008634331, the jacobian loss is:0.09899825\n",
            "This is the iter 7786, the d1 loss is: 3148.289, the d2 loss is: -3311.5078, the g loss is: 3313.164, the ae loss is: 0.0073750108, the jacobian loss is:0.08668772\n",
            "This is the iter 7787, the d1 loss is: 3079.7734, the d2 loss is: -3249.414, the g loss is: 3254.2031, the ae loss is: 0.006970316, the jacobian loss is:0.101396725\n",
            "This is the iter 7788, the d1 loss is: 2940.586, the d2 loss is: -3137.1719, the g loss is: 3093.75, the ae loss is: 0.0065318346, the jacobian loss is:0.08868386\n",
            "This is the iter 7789, the d1 loss is: 3107.0938, the d2 loss is: -3295.4688, the g loss is: 3334.9531, the ae loss is: 0.006390491, the jacobian loss is:0.11021845\n",
            "This is the iter 7790, the d1 loss is: 3122.9453, the d2 loss is: -3306.8594, the g loss is: 3239.1719, the ae loss is: 0.009029508, the jacobian loss is:0.10426744\n",
            "This is the iter 7791, the d1 loss is: 3205.4688, the d2 loss is: -3359.3047, the g loss is: 3332.8125, the ae loss is: 0.0069243903, the jacobian loss is:0.1069456\n",
            "This is the iter 7792, the d1 loss is: 3087.3203, the d2 loss is: -3267.539, the g loss is: 3209.25, the ae loss is: 0.0054299366, the jacobian loss is:0.09958917\n",
            "This is the iter 7793, the d1 loss is: 3111.1016, the d2 loss is: -3306.0781, the g loss is: 3269.1016, the ae loss is: 0.006055263, the jacobian loss is:0.08799505\n",
            "This is the iter 7794, the d1 loss is: 2962.2578, the d2 loss is: -3138.0156, the g loss is: 3154.25, the ae loss is: 0.005819058, the jacobian loss is:0.10341752\n",
            "This is the iter 7795, the d1 loss is: 2764.5312, the d2 loss is: -2948.1172, the g loss is: 2977.7188, the ae loss is: 0.0056496006, the jacobian loss is:0.08057109\n",
            "This is the iter 7796, the d1 loss is: 3102.1328, the d2 loss is: -3297.2578, the g loss is: 3290.5, the ae loss is: 0.0073255873, the jacobian loss is:0.11003106\n",
            "This is the iter 7797, the d1 loss is: 3025.8594, the d2 loss is: -3203.1562, the g loss is: 3213.4531, the ae loss is: 0.0072467537, the jacobian loss is:0.075753845\n",
            "This is the iter 7798, the d1 loss is: 3167.7344, the d2 loss is: -3353.8984, the g loss is: 3390.414, the ae loss is: 0.00556748, the jacobian loss is:0.08442184\n",
            "This is the iter 7799, the d1 loss is: 3247.8984, the d2 loss is: -3421.9688, the g loss is: 3382.1484, the ae loss is: 0.006421422, the jacobian loss is:0.084918566\n",
            "This is the iter 7800, the d1 loss is: 3002.5938, the d2 loss is: -3201.25, the g loss is: 3206.3047, the ae loss is: 0.007143067, the jacobian loss is:0.10303652\n",
            "0.2567468\n",
            "0.99197954\n",
            "This is the iter 7801, the d1 loss is: 2937.2656, the d2 loss is: -3122.0, the g loss is: 3102.6797, the ae loss is: 0.0073871957, the jacobian loss is:0.08411222\n",
            "This is the iter 7802, the d1 loss is: 3053.3672, the d2 loss is: -3234.2734, the g loss is: 3254.5938, the ae loss is: 0.0048461873, the jacobian loss is:0.1538567\n",
            "This is the iter 7803, the d1 loss is: 3038.414, the d2 loss is: -3231.961, the g loss is: 3264.8594, the ae loss is: 0.0077246735, the jacobian loss is:0.096690714\n",
            "This is the iter 7804, the d1 loss is: 3050.3125, the d2 loss is: -3257.3984, the g loss is: 3272.5781, the ae loss is: 0.0071979873, the jacobian loss is:0.073921666\n",
            "This is the iter 7805, the d1 loss is: 3033.5938, the d2 loss is: -3196.1719, the g loss is: 3193.5469, the ae loss is: 0.006763758, the jacobian loss is:0.11515771\n",
            "This is the iter 7806, the d1 loss is: 3088.2031, the d2 loss is: -3278.7422, the g loss is: 3249.3984, the ae loss is: 0.005733252, the jacobian loss is:0.105788425\n",
            "This is the iter 7807, the d1 loss is: 3091.6094, the d2 loss is: -3232.289, the g loss is: 3239.0938, the ae loss is: 0.006635663, the jacobian loss is:0.12200343\n",
            "This is the iter 7808, the d1 loss is: 3077.2031, the d2 loss is: -3235.4688, the g loss is: 3245.1328, the ae loss is: 0.007308407, the jacobian loss is:0.0893876\n",
            "This is the iter 7809, the d1 loss is: 3055.0469, the d2 loss is: -3209.4297, the g loss is: 3233.8047, the ae loss is: 0.008216532, the jacobian loss is:0.12148737\n",
            "This is the iter 7810, the d1 loss is: 3107.4531, the d2 loss is: -3285.164, the g loss is: 3321.1875, the ae loss is: 0.009144325, the jacobian loss is:0.12247115\n",
            "This is the iter 7811, the d1 loss is: 2860.2812, the d2 loss is: -3022.3906, the g loss is: 3026.5938, the ae loss is: 0.0060330387, the jacobian loss is:0.09075916\n",
            "This is the iter 7812, the d1 loss is: 2907.9375, the d2 loss is: -3079.4297, the g loss is: 3098.375, the ae loss is: 0.0072143134, the jacobian loss is:0.113627225\n",
            "This is the iter 7813, the d1 loss is: 3347.3438, the d2 loss is: -3523.7578, the g loss is: 3537.5078, the ae loss is: 0.005382605, the jacobian loss is:0.08828976\n",
            "This is the iter 7814, the d1 loss is: 3169.7812, the d2 loss is: -3341.0156, the g loss is: 3347.4297, the ae loss is: 0.0058273855, the jacobian loss is:0.09331778\n",
            "This is the iter 7815, the d1 loss is: 3105.1016, the d2 loss is: -3265.75, the g loss is: 3264.8438, the ae loss is: 0.0060688937, the jacobian loss is:0.08248255\n",
            "This is the iter 7816, the d1 loss is: 3002.9453, the d2 loss is: -3199.25, the g loss is: 3217.2422, the ae loss is: 0.0075579993, the jacobian loss is:0.123745784\n",
            "This is the iter 7817, the d1 loss is: 3163.1172, the d2 loss is: -3346.2266, the g loss is: 3285.6719, the ae loss is: 0.0060628774, the jacobian loss is:0.08833631\n",
            "This is the iter 7818, the d1 loss is: 2957.4766, the d2 loss is: -3094.6797, the g loss is: 3187.6094, the ae loss is: 0.006421026, the jacobian loss is:0.109430656\n",
            "This is the iter 7819, the d1 loss is: 3094.9531, the d2 loss is: -3267.2969, the g loss is: 3250.9219, the ae loss is: 0.006163493, the jacobian loss is:0.094666675\n",
            "This is the iter 7820, the d1 loss is: 3056.7031, the d2 loss is: -3212.4844, the g loss is: 3289.1562, the ae loss is: 0.009403365, the jacobian loss is:0.09922182\n",
            "This is the iter 7821, the d1 loss is: 3069.2656, the d2 loss is: -3230.1719, the g loss is: 3252.625, the ae loss is: 0.004616725, the jacobian loss is:0.09165903\n",
            "This is the iter 7822, the d1 loss is: 3125.2656, the d2 loss is: -3296.4219, the g loss is: 3310.75, the ae loss is: 0.008227178, the jacobian loss is:0.092627145\n",
            "This is the iter 7823, the d1 loss is: 3001.5312, the d2 loss is: -3182.8516, the g loss is: 3156.7656, the ae loss is: 0.008549791, the jacobian loss is:0.07756544\n",
            "This is the iter 7824, the d1 loss is: 2884.539, the d2 loss is: -3042.3906, the g loss is: 3112.2188, the ae loss is: 0.008486799, the jacobian loss is:0.11997379\n",
            "This is the iter 7825, the d1 loss is: 3223.3125, the d2 loss is: -3426.4922, the g loss is: 3402.6172, the ae loss is: 0.0046187434, the jacobian loss is:0.09864033\n",
            "This is the iter 7826, the d1 loss is: 3003.0156, the d2 loss is: -3200.7344, the g loss is: 3208.3438, the ae loss is: 0.006239521, the jacobian loss is:0.12584254\n",
            "This is the iter 7827, the d1 loss is: 3031.7031, the d2 loss is: -3256.5, the g loss is: 3279.9922, the ae loss is: 0.0062389113, the jacobian loss is:0.09092433\n",
            "This is the iter 7828, the d1 loss is: 3076.8906, the d2 loss is: -3244.8672, the g loss is: 3240.375, the ae loss is: 0.0073082424, the jacobian loss is:0.10000188\n",
            "This is the iter 7829, the d1 loss is: 3104.4375, the d2 loss is: -3265.9297, the g loss is: 3242.5078, the ae loss is: 0.005798405, the jacobian loss is:0.102025256\n",
            "This is the iter 7830, the d1 loss is: 3180.0703, the d2 loss is: -3337.4453, the g loss is: 3356.5, the ae loss is: 0.007294978, the jacobian loss is:0.08377965\n",
            "This is the iter 7831, the d1 loss is: 3111.1875, the d2 loss is: -3282.961, the g loss is: 3288.1172, the ae loss is: 0.007268112, the jacobian loss is:0.1305783\n",
            "This is the iter 7832, the d1 loss is: 3108.6562, the d2 loss is: -3300.9844, the g loss is: 3274.6562, the ae loss is: 0.008742524, the jacobian loss is:0.16589886\n",
            "This is the iter 7833, the d1 loss is: 3014.9219, the d2 loss is: -3186.6797, the g loss is: 3148.664, the ae loss is: 0.0045327838, the jacobian loss is:0.1328926\n",
            "This is the iter 7834, the d1 loss is: 3075.3516, the d2 loss is: -3236.0312, the g loss is: 3255.164, the ae loss is: 0.007420357, the jacobian loss is:0.091822416\n",
            "This is the iter 7835, the d1 loss is: 3051.5781, the d2 loss is: -3229.6797, the g loss is: 3198.25, the ae loss is: 0.007515192, the jacobian loss is:0.09316464\n",
            "This is the iter 7836, the d1 loss is: 3285.0, the d2 loss is: -3426.0703, the g loss is: 3512.9688, the ae loss is: 0.010057022, the jacobian loss is:0.15289854\n",
            "This is the iter 7837, the d1 loss is: 3056.1172, the d2 loss is: -3195.9297, the g loss is: 3211.0938, the ae loss is: 0.00752516, the jacobian loss is:0.08273202\n",
            "This is the iter 7838, the d1 loss is: 2844.1562, the d2 loss is: -2996.5938, the g loss is: 3046.0156, the ae loss is: 0.0072639585, the jacobian loss is:0.106181905\n",
            "This is the iter 7839, the d1 loss is: 3210.5156, the d2 loss is: -3344.6875, the g loss is: 3349.0625, the ae loss is: 0.009004261, the jacobian loss is:0.11015005\n",
            "This is the iter 7840, the d1 loss is: 3057.8516, the d2 loss is: -3228.4219, the g loss is: 3326.1328, the ae loss is: 0.0076847593, the jacobian loss is:0.078802094\n",
            "This is the iter 7841, the d1 loss is: 3032.3281, the d2 loss is: -3208.0469, the g loss is: 3232.3438, the ae loss is: 0.0073696794, the jacobian loss is:0.094696596\n",
            "This is the iter 7842, the d1 loss is: 3092.1016, the d2 loss is: -3294.4375, the g loss is: 3300.5, the ae loss is: 0.0064837583, the jacobian loss is:0.10710761\n",
            "This is the iter 7843, the d1 loss is: 3047.539, the d2 loss is: -3224.789, the g loss is: 3223.336, the ae loss is: 0.004337764, the jacobian loss is:0.109944455\n",
            "This is the iter 7844, the d1 loss is: 3102.0156, the d2 loss is: -3295.7266, the g loss is: 3286.1562, the ae loss is: 0.005698907, the jacobian loss is:0.120951585\n",
            "This is the iter 7845, the d1 loss is: 3229.2812, the d2 loss is: -3395.4062, the g loss is: 3392.0156, the ae loss is: 0.011041695, the jacobian loss is:0.102012925\n",
            "This is the iter 7846, the d1 loss is: 3036.1875, the d2 loss is: -3219.4844, the g loss is: 3256.9062, the ae loss is: 0.006012298, the jacobian loss is:0.09323261\n",
            "This is the iter 7847, the d1 loss is: 3030.125, the d2 loss is: -3183.2969, the g loss is: 3229.414, the ae loss is: 0.0074755186, the jacobian loss is:0.076726116\n",
            "This is the iter 7848, the d1 loss is: 2983.7656, the d2 loss is: -3154.9219, the g loss is: 3151.6406, the ae loss is: 0.009012653, the jacobian loss is:0.08871753\n",
            "This is the iter 7849, the d1 loss is: 2960.5469, the d2 loss is: -3125.9531, the g loss is: 3114.5781, the ae loss is: 0.0068363827, the jacobian loss is:0.17679092\n",
            "This is the iter 7850, the d1 loss is: 3115.7344, the d2 loss is: -3306.7344, the g loss is: 3325.0312, the ae loss is: 0.0067941574, the jacobian loss is:0.21137722\n",
            "This is the iter 7851, the d1 loss is: 3261.0078, the d2 loss is: -3465.9531, the g loss is: 3441.6406, the ae loss is: 0.005528838, the jacobian loss is:0.09029948\n",
            "This is the iter 7852, the d1 loss is: 3114.0469, the d2 loss is: -3220.1094, the g loss is: 3292.7266, the ae loss is: 0.007769654, the jacobian loss is:0.09276599\n",
            "This is the iter 7853, the d1 loss is: 3036.5234, the d2 loss is: -3234.6016, the g loss is: 3252.6406, the ae loss is: 0.0060022147, the jacobian loss is:0.094302066\n",
            "This is the iter 7854, the d1 loss is: 3065.0156, the d2 loss is: -3236.2188, the g loss is: 3273.3516, the ae loss is: 0.005689211, the jacobian loss is:0.102948755\n",
            "This is the iter 7855, the d1 loss is: 3135.6016, the d2 loss is: -3308.6016, the g loss is: 3317.3672, the ae loss is: 0.005222327, the jacobian loss is:0.1037413\n",
            "This is the iter 7856, the d1 loss is: 3101.25, the d2 loss is: -3258.9531, the g loss is: 3238.414, the ae loss is: 0.009497716, the jacobian loss is:0.1263499\n",
            "This is the iter 7857, the d1 loss is: 2930.0312, the d2 loss is: -3084.9531, the g loss is: 3095.7188, the ae loss is: 0.0058035823, the jacobian loss is:0.10048439\n",
            "This is the iter 7858, the d1 loss is: 2858.0156, the d2 loss is: -3025.25, the g loss is: 3014.1172, the ae loss is: 0.0065318067, the jacobian loss is:0.090589054\n",
            "This is the iter 7859, the d1 loss is: 2878.2266, the d2 loss is: -3074.7812, the g loss is: 3085.8281, the ae loss is: 0.007316929, the jacobian loss is:0.09819249\n",
            "This is the iter 7860, the d1 loss is: 3331.289, the d2 loss is: -3507.164, the g loss is: 3521.5234, the ae loss is: 0.0057307174, the jacobian loss is:0.10499436\n",
            "This is the iter 7861, the d1 loss is: 2980.3516, the d2 loss is: -3150.9531, the g loss is: 3163.414, the ae loss is: 0.007230485, the jacobian loss is:0.089409545\n",
            "This is the iter 7862, the d1 loss is: 3161.7812, the d2 loss is: -3353.8281, the g loss is: 3274.6719, the ae loss is: 0.006723715, the jacobian loss is:0.089693055\n",
            "This is the iter 7863, the d1 loss is: 3094.2266, the d2 loss is: -3283.7578, the g loss is: 3272.5625, the ae loss is: 0.007825665, the jacobian loss is:0.08637428\n",
            "This is the iter 7864, the d1 loss is: 3288.3047, the d2 loss is: -3461.4219, the g loss is: 3547.6016, the ae loss is: 0.005913946, the jacobian loss is:0.10736232\n",
            "This is the iter 7865, the d1 loss is: 3019.211, the d2 loss is: -3193.25, the g loss is: 3189.336, the ae loss is: 0.0070616705, the jacobian loss is:0.12999626\n",
            "This is the iter 7866, the d1 loss is: 3295.1328, the d2 loss is: -3473.8047, the g loss is: 3469.2188, the ae loss is: 0.0077908197, the jacobian loss is:0.104350105\n",
            "This is the iter 7867, the d1 loss is: 2978.5312, the d2 loss is: -3153.6094, the g loss is: 3171.0547, the ae loss is: 0.008281416, the jacobian loss is:0.19020991\n",
            "This is the iter 7868, the d1 loss is: 3078.2031, the d2 loss is: -3232.2812, the g loss is: 3232.3438, the ae loss is: 0.0053626834, the jacobian loss is:0.07262335\n",
            "This is the iter 7869, the d1 loss is: 3092.2812, the d2 loss is: -3265.1875, the g loss is: 3247.7656, the ae loss is: 0.0098632295, the jacobian loss is:0.081212595\n",
            "This is the iter 7870, the d1 loss is: 3206.6172, the d2 loss is: -3367.961, the g loss is: 3415.7266, the ae loss is: 0.006443154, the jacobian loss is:0.10756252\n",
            "This is the iter 7871, the d1 loss is: 3176.9062, the d2 loss is: -3330.5156, the g loss is: 3343.0625, the ae loss is: 0.0086944625, the jacobian loss is:0.10408733\n",
            "This is the iter 7872, the d1 loss is: 3363.6328, the d2 loss is: -3537.711, the g loss is: 3452.3281, the ae loss is: 0.006312623, the jacobian loss is:0.09689003\n",
            "This is the iter 7873, the d1 loss is: 2959.7422, the d2 loss is: -3156.336, the g loss is: 3167.5, the ae loss is: 0.0066429786, the jacobian loss is:0.076952994\n",
            "This is the iter 7874, the d1 loss is: 2968.211, the d2 loss is: -3130.1562, the g loss is: 3163.4062, the ae loss is: 0.006852283, the jacobian loss is:0.11444968\n",
            "This is the iter 7875, the d1 loss is: 3035.211, the d2 loss is: -3223.0078, the g loss is: 3220.5312, the ae loss is: 0.006669797, the jacobian loss is:0.0985383\n",
            "This is the iter 7876, the d1 loss is: 3143.6562, the d2 loss is: -3323.3281, the g loss is: 3320.1562, the ae loss is: 0.007887986, the jacobian loss is:0.10428213\n",
            "This is the iter 7877, the d1 loss is: 2815.6172, the d2 loss is: -2968.7344, the g loss is: 2985.1953, the ae loss is: 0.0064913556, the jacobian loss is:0.0797162\n",
            "This is the iter 7878, the d1 loss is: 2947.5781, the d2 loss is: -3113.8125, the g loss is: 3114.9922, the ae loss is: 0.00594927, the jacobian loss is:0.11630283\n",
            "This is the iter 7879, the d1 loss is: 3016.5312, the d2 loss is: -3218.789, the g loss is: 3216.8594, the ae loss is: 0.006655527, the jacobian loss is:0.10215486\n",
            "This is the iter 7880, the d1 loss is: 3056.711, the d2 loss is: -3263.0, the g loss is: 3270.164, the ae loss is: 0.006631301, the jacobian loss is:0.090511486\n",
            "This is the iter 7881, the d1 loss is: 3118.3125, the d2 loss is: -3253.539, the g loss is: 3295.5469, the ae loss is: 0.006386491, the jacobian loss is:0.09704897\n",
            "This is the iter 7882, the d1 loss is: 2917.6328, the d2 loss is: -3099.4062, the g loss is: 3082.8984, the ae loss is: 0.0075725038, the jacobian loss is:0.11579648\n",
            "This is the iter 7883, the d1 loss is: 3113.125, the d2 loss is: -3296.6719, the g loss is: 3251.4766, the ae loss is: 0.007747571, the jacobian loss is:0.09441847\n",
            "This is the iter 7884, the d1 loss is: 3282.0469, the d2 loss is: -3467.8906, the g loss is: 3520.6016, the ae loss is: 0.005961648, the jacobian loss is:0.08732337\n",
            "This is the iter 7885, the d1 loss is: 3031.3984, the d2 loss is: -3224.5078, the g loss is: 3208.1016, the ae loss is: 0.0060359193, the jacobian loss is:0.07812006\n",
            "This is the iter 7886, the d1 loss is: 3151.6953, the d2 loss is: -3325.2734, the g loss is: 3342.1719, the ae loss is: 0.008646789, the jacobian loss is:0.104494855\n",
            "This is the iter 7887, the d1 loss is: 2804.7969, the d2 loss is: -3009.7031, the g loss is: 3013.8594, the ae loss is: 0.0059271194, the jacobian loss is:0.11773526\n",
            "This is the iter 7888, the d1 loss is: 3309.8438, the d2 loss is: -3516.6094, the g loss is: 3504.875, the ae loss is: 0.008039228, the jacobian loss is:0.08494075\n",
            "This is the iter 7889, the d1 loss is: 3173.6016, the d2 loss is: -3317.8906, the g loss is: 3302.5, the ae loss is: 0.0053057764, the jacobian loss is:0.098443374\n",
            "This is the iter 7890, the d1 loss is: 3116.7969, the d2 loss is: -3290.289, the g loss is: 3263.1875, the ae loss is: 0.0061084507, the jacobian loss is:0.09102251\n",
            "This is the iter 7891, the d1 loss is: 3237.1797, the d2 loss is: -3378.3672, the g loss is: 3413.4844, the ae loss is: 0.006657287, the jacobian loss is:0.10404671\n",
            "This is the iter 7892, the d1 loss is: 3150.8281, the d2 loss is: -3334.3125, the g loss is: 3358.1719, the ae loss is: 0.00674331, the jacobian loss is:0.11157718\n",
            "This is the iter 7893, the d1 loss is: 3039.3594, the d2 loss is: -3229.6562, the g loss is: 3189.6562, the ae loss is: 0.0074883513, the jacobian loss is:0.1012484\n",
            "This is the iter 7894, the d1 loss is: 3139.414, the d2 loss is: -3311.1484, the g loss is: 3281.461, the ae loss is: 0.006310967, the jacobian loss is:0.13720427\n",
            "This is the iter 7895, the d1 loss is: 3120.0547, the d2 loss is: -3276.75, the g loss is: 3335.7812, the ae loss is: 0.006924321, the jacobian loss is:0.11132583\n",
            "This is the iter 7896, the d1 loss is: 3080.6875, the d2 loss is: -3230.086, the g loss is: 3257.4766, the ae loss is: 0.0053370628, the jacobian loss is:0.087220825\n",
            "This is the iter 7897, the d1 loss is: 3121.3281, the d2 loss is: -3264.7422, the g loss is: 3214.6094, the ae loss is: 0.0063936003, the jacobian loss is:0.09819327\n",
            "This is the iter 7898, the d1 loss is: 3228.7188, the d2 loss is: -3391.7031, the g loss is: 3392.2969, the ae loss is: 0.0059228167, the jacobian loss is:0.122181684\n",
            "This is the iter 7899, the d1 loss is: 3022.5703, the d2 loss is: -3194.8125, the g loss is: 3205.7266, the ae loss is: 0.005956767, the jacobian loss is:0.09069929\n",
            "This is the iter 7900, the d1 loss is: 3213.3516, the d2 loss is: -3363.9219, the g loss is: 3305.5234, the ae loss is: 0.007000167, the jacobian loss is:0.089801155\n",
            "0.252228\n",
            "0.96367854\n",
            "This is the iter 7901, the d1 loss is: 3142.461, the d2 loss is: -3292.9375, the g loss is: 3287.3906, the ae loss is: 0.0058324873, the jacobian loss is:0.08932708\n",
            "This is the iter 7902, the d1 loss is: 3133.1484, the d2 loss is: -3312.4219, the g loss is: 3308.1406, the ae loss is: 0.008647294, the jacobian loss is:0.09736042\n",
            "This is the iter 7903, the d1 loss is: 3187.1484, the d2 loss is: -3340.6016, the g loss is: 3336.8516, the ae loss is: 0.007777597, the jacobian loss is:0.09523196\n",
            "This is the iter 7904, the d1 loss is: 3155.9375, the d2 loss is: -3325.6328, the g loss is: 3377.1562, the ae loss is: 0.006755366, the jacobian loss is:0.101831906\n",
            "This is the iter 7905, the d1 loss is: 3197.5469, the d2 loss is: -3376.461, the g loss is: 3396.7266, the ae loss is: 0.008603919, the jacobian loss is:0.13350087\n",
            "This is the iter 7906, the d1 loss is: 3107.8984, the d2 loss is: -3284.2969, the g loss is: 3278.3438, the ae loss is: 0.005938224, the jacobian loss is:0.085442156\n",
            "This is the iter 7907, the d1 loss is: 3092.3281, the d2 loss is: -3259.4922, the g loss is: 3227.461, the ae loss is: 0.007723904, the jacobian loss is:0.10190762\n",
            "This is the iter 7908, the d1 loss is: 3126.2188, the d2 loss is: -3287.3125, the g loss is: 3285.375, the ae loss is: 0.0059375684, the jacobian loss is:0.09611518\n",
            "This is the iter 7909, the d1 loss is: 3007.9453, the d2 loss is: -3160.3047, the g loss is: 3194.2969, the ae loss is: 0.0055670016, the jacobian loss is:0.11672906\n",
            "This is the iter 7910, the d1 loss is: 3058.164, the d2 loss is: -3201.9688, the g loss is: 3223.0781, the ae loss is: 0.0069292984, the jacobian loss is:0.09293662\n",
            "This is the iter 7911, the d1 loss is: 2897.3438, the d2 loss is: -3056.625, the g loss is: 3029.7422, the ae loss is: 0.0063143196, the jacobian loss is:0.0968969\n",
            "This is the iter 7912, the d1 loss is: 3131.8203, the d2 loss is: -3270.3047, the g loss is: 3270.5938, the ae loss is: 0.0082189385, the jacobian loss is:0.092863254\n",
            "This is the iter 7913, the d1 loss is: 3077.914, the d2 loss is: -3218.3516, the g loss is: 3234.8594, the ae loss is: 0.008005306, the jacobian loss is:0.08500211\n",
            "This is the iter 7914, the d1 loss is: 2928.6094, the d2 loss is: -3097.0625, the g loss is: 3054.2031, the ae loss is: 0.008604279, the jacobian loss is:0.094482645\n",
            "This is the iter 7915, the d1 loss is: 3067.3984, the d2 loss is: -3249.7422, the g loss is: 3228.6016, the ae loss is: 0.0068731257, the jacobian loss is:0.095201775\n",
            "This is the iter 7916, the d1 loss is: 2903.2734, the d2 loss is: -3101.2188, the g loss is: 3080.75, the ae loss is: 0.009767529, the jacobian loss is:0.09377444\n",
            "This is the iter 7917, the d1 loss is: 2815.7266, the d2 loss is: -2979.75, the g loss is: 2991.8438, the ae loss is: 0.0056396555, the jacobian loss is:0.088085465\n",
            "This is the iter 7918, the d1 loss is: 3112.7344, the d2 loss is: -3259.4453, the g loss is: 3308.6172, the ae loss is: 0.008877248, the jacobian loss is:0.11118239\n",
            "This is the iter 7919, the d1 loss is: 2923.1562, the d2 loss is: -3096.2969, the g loss is: 3038.2969, the ae loss is: 0.008136427, the jacobian loss is:0.10130927\n",
            "This is the iter 7920, the d1 loss is: 2907.375, the d2 loss is: -3070.1484, the g loss is: 3027.3281, the ae loss is: 0.0074630785, the jacobian loss is:0.084893994\n",
            "This is the iter 7921, the d1 loss is: 3018.6328, the d2 loss is: -3177.5938, the g loss is: 3138.625, the ae loss is: 0.006151408, the jacobian loss is:0.122663096\n",
            "This is the iter 7922, the d1 loss is: 3223.414, the d2 loss is: -3400.6562, the g loss is: 3428.75, the ae loss is: 0.0083687175, the jacobian loss is:0.120121166\n",
            "This is the iter 7923, the d1 loss is: 2966.2969, the d2 loss is: -3138.7578, the g loss is: 3100.0312, the ae loss is: 0.006565391, the jacobian loss is:0.09320077\n",
            "This is the iter 7924, the d1 loss is: 3209.5156, the d2 loss is: -3365.5, the g loss is: 3387.4062, the ae loss is: 0.0077120964, the jacobian loss is:0.104483455\n",
            "This is the iter 7925, the d1 loss is: 3033.3281, the d2 loss is: -3214.039, the g loss is: 3201.0078, the ae loss is: 0.008662959, the jacobian loss is:0.0941471\n",
            "This is the iter 7926, the d1 loss is: 3023.4688, the d2 loss is: -3154.4453, the g loss is: 3176.5078, the ae loss is: 0.0068370057, the jacobian loss is:0.073979944\n",
            "This is the iter 7927, the d1 loss is: 3432.7344, the d2 loss is: -3591.9844, the g loss is: 3545.9844, the ae loss is: 0.00771928, the jacobian loss is:0.08557886\n",
            "This is the iter 7928, the d1 loss is: 3014.5312, the d2 loss is: -3198.6328, the g loss is: 3232.7812, the ae loss is: 0.008112451, the jacobian loss is:0.095319144\n",
            "This is the iter 7929, the d1 loss is: 3428.0, the d2 loss is: -3606.789, the g loss is: 3586.6875, the ae loss is: 0.009910179, the jacobian loss is:0.097414725\n",
            "This is the iter 7930, the d1 loss is: 3403.2734, the d2 loss is: -3553.7734, the g loss is: 3522.9688, the ae loss is: 0.008297403, the jacobian loss is:0.10750389\n",
            "This is the iter 7931, the d1 loss is: 3048.0156, the d2 loss is: -3185.4375, the g loss is: 3184.9219, the ae loss is: 0.011726874, the jacobian loss is:0.097025976\n",
            "This is the iter 7932, the d1 loss is: 3089.836, the d2 loss is: -3239.625, the g loss is: 3269.9297, the ae loss is: 0.005830474, the jacobian loss is:0.11938217\n",
            "This is the iter 7933, the d1 loss is: 3223.461, the d2 loss is: -3373.039, the g loss is: 3356.375, the ae loss is: 0.0063630724, the jacobian loss is:0.09328232\n",
            "This is the iter 7934, the d1 loss is: 3132.539, the d2 loss is: -3293.6719, the g loss is: 3241.6875, the ae loss is: 0.00935648, the jacobian loss is:0.10416592\n",
            "This is the iter 7935, the d1 loss is: 3242.9297, the d2 loss is: -3432.2422, the g loss is: 3393.25, the ae loss is: 0.00687917, the jacobian loss is:0.088547155\n",
            "This is the iter 7936, the d1 loss is: 2915.1484, the d2 loss is: -3073.7578, the g loss is: 3064.875, the ae loss is: 0.008484393, the jacobian loss is:0.097753614\n",
            "This is the iter 7937, the d1 loss is: 3201.6094, the d2 loss is: -3368.4531, the g loss is: 3425.6875, the ae loss is: 0.006539004, the jacobian loss is:0.09657377\n",
            "This is the iter 7938, the d1 loss is: 2896.1172, the d2 loss is: -3065.9219, the g loss is: 3063.7812, the ae loss is: 0.0051826, the jacobian loss is:0.0803839\n",
            "This is the iter 7939, the d1 loss is: 3194.414, the d2 loss is: -3332.4766, the g loss is: 3327.0234, the ae loss is: 0.007824487, the jacobian loss is:0.09685039\n",
            "This is the iter 7940, the d1 loss is: 3054.3906, the d2 loss is: -3219.2969, the g loss is: 3186.4922, the ae loss is: 0.009131123, the jacobian loss is:0.118483834\n",
            "This is the iter 7941, the d1 loss is: 3155.3438, the d2 loss is: -3308.9766, the g loss is: 3298.7656, the ae loss is: 0.0080538485, the jacobian loss is:0.08815659\n",
            "This is the iter 7942, the d1 loss is: 2968.9844, the d2 loss is: -3114.875, the g loss is: 3122.7344, the ae loss is: 0.010001745, the jacobian loss is:0.09523563\n",
            "This is the iter 7943, the d1 loss is: 3321.7969, the d2 loss is: -3490.7734, the g loss is: 3487.8125, the ae loss is: 0.007249823, the jacobian loss is:0.0996601\n",
            "This is the iter 7944, the d1 loss is: 3178.9531, the d2 loss is: -3346.3672, the g loss is: 3298.5234, the ae loss is: 0.0077711344, the jacobian loss is:0.078899235\n",
            "This is the iter 7945, the d1 loss is: 3285.5469, the d2 loss is: -3445.6953, the g loss is: 3415.8984, the ae loss is: 0.005424251, the jacobian loss is:0.09703763\n",
            "This is the iter 7946, the d1 loss is: 3100.1719, the d2 loss is: -3249.7344, the g loss is: 3272.2266, the ae loss is: 0.008801423, the jacobian loss is:0.24525453\n",
            "This is the iter 7947, the d1 loss is: 3164.039, the d2 loss is: -3331.0156, the g loss is: 3340.5938, the ae loss is: 0.0056468546, the jacobian loss is:0.08543316\n",
            "This is the iter 7948, the d1 loss is: 3367.3125, the d2 loss is: -3534.8047, the g loss is: 3516.9219, the ae loss is: 0.0066268835, the jacobian loss is:0.10580148\n",
            "This is the iter 7949, the d1 loss is: 3026.3203, the d2 loss is: -3194.5547, the g loss is: 3191.1172, the ae loss is: 0.008912604, the jacobian loss is:0.10065406\n",
            "This is the iter 7950, the d1 loss is: 3132.8125, the d2 loss is: -3300.75, the g loss is: 3224.2031, the ae loss is: 0.0077065937, the jacobian loss is:0.11370256\n",
            "This is the iter 7951, the d1 loss is: 3157.1406, the d2 loss is: -3329.8438, the g loss is: 3268.1797, the ae loss is: 0.00511076, the jacobian loss is:0.11171995\n",
            "This is the iter 7952, the d1 loss is: 3127.711, the d2 loss is: -3283.1016, the g loss is: 3368.7031, the ae loss is: 0.0073520243, the jacobian loss is:0.13941726\n",
            "This is the iter 7953, the d1 loss is: 2909.7188, the d2 loss is: -3082.7578, the g loss is: 3091.5781, the ae loss is: 0.0068674, the jacobian loss is:0.07642626\n",
            "This is the iter 7954, the d1 loss is: 3319.75, the d2 loss is: -3453.0469, the g loss is: 3430.2031, the ae loss is: 0.0072621545, the jacobian loss is:0.12058326\n",
            "This is the iter 7955, the d1 loss is: 3186.125, the d2 loss is: -3361.2188, the g loss is: 3298.0625, the ae loss is: 0.006624623, the jacobian loss is:0.09857692\n",
            "This is the iter 7956, the d1 loss is: 2973.7812, the d2 loss is: -3120.7422, the g loss is: 3102.9219, the ae loss is: 0.008416646, the jacobian loss is:0.07998139\n",
            "This is the iter 7957, the d1 loss is: 3221.7344, the d2 loss is: -3399.7188, the g loss is: 3433.836, the ae loss is: 0.007733011, the jacobian loss is:0.08861672\n",
            "This is the iter 7958, the d1 loss is: 3119.5781, the d2 loss is: -3319.7578, the g loss is: 3226.7812, the ae loss is: 0.010336907, the jacobian loss is:0.07604179\n",
            "This is the iter 7959, the d1 loss is: 3094.5938, the d2 loss is: -3259.9844, the g loss is: 3302.0938, the ae loss is: 0.008514262, the jacobian loss is:0.10012235\n",
            "This is the iter 7960, the d1 loss is: 3108.7344, the d2 loss is: -3303.9219, the g loss is: 3280.0156, the ae loss is: 0.0050893757, the jacobian loss is:0.10848034\n",
            "This is the iter 7961, the d1 loss is: 3089.6094, the d2 loss is: -3264.2656, the g loss is: 3214.9531, the ae loss is: 0.0078960555, the jacobian loss is:0.10193235\n",
            "This is the iter 7962, the d1 loss is: 2953.961, the d2 loss is: -3126.2188, the g loss is: 3096.7578, the ae loss is: 0.008128015, the jacobian loss is:0.10667144\n",
            "This is the iter 7963, the d1 loss is: 2970.9688, the d2 loss is: -3123.3125, the g loss is: 3106.961, the ae loss is: 0.008348153, the jacobian loss is:0.07315398\n",
            "This is the iter 7964, the d1 loss is: 2919.5938, the d2 loss is: -3071.1719, the g loss is: 3066.8906, the ae loss is: 0.0068698362, the jacobian loss is:0.07373352\n",
            "This is the iter 7965, the d1 loss is: 3079.1328, the d2 loss is: -3252.3203, the g loss is: 3193.8672, the ae loss is: 0.0066232537, the jacobian loss is:0.12303573\n",
            "This is the iter 7966, the d1 loss is: 3321.6328, the d2 loss is: -3484.8125, the g loss is: 3437.836, the ae loss is: 0.007425814, the jacobian loss is:0.09324581\n",
            "This is the iter 7967, the d1 loss is: 2977.2578, the d2 loss is: -3175.9766, the g loss is: 3133.4844, the ae loss is: 0.0076394337, the jacobian loss is:0.09970223\n",
            "This is the iter 7968, the d1 loss is: 2837.539, the d2 loss is: -2989.5, the g loss is: 2996.961, the ae loss is: 0.006917675, the jacobian loss is:0.09136349\n",
            "This is the iter 7969, the d1 loss is: 3078.6719, the d2 loss is: -3234.7578, the g loss is: 3212.3594, the ae loss is: 0.008259145, the jacobian loss is:0.13176468\n",
            "This is the iter 7970, the d1 loss is: 3131.4062, the d2 loss is: -3257.25, the g loss is: 3303.1328, the ae loss is: 0.011802439, the jacobian loss is:0.10302162\n",
            "This is the iter 7971, the d1 loss is: 3161.164, the d2 loss is: -3297.1719, the g loss is: 3368.039, the ae loss is: 0.0061636427, the jacobian loss is:0.092893854\n",
            "This is the iter 7972, the d1 loss is: 3284.0312, the d2 loss is: -3449.6094, the g loss is: 3435.7734, the ae loss is: 0.0065894355, the jacobian loss is:0.09941109\n",
            "This is the iter 7973, the d1 loss is: 3040.4062, the d2 loss is: -3212.211, the g loss is: 3213.7656, the ae loss is: 0.005714125, the jacobian loss is:0.1164019\n",
            "This is the iter 7974, the d1 loss is: 3194.539, the d2 loss is: -3356.2422, the g loss is: 3280.6016, the ae loss is: 0.006864432, the jacobian loss is:0.12488834\n",
            "This is the iter 7975, the d1 loss is: 3039.8516, the d2 loss is: -3195.3906, the g loss is: 3210.1328, the ae loss is: 0.0058492394, the jacobian loss is:0.08320781\n",
            "This is the iter 7976, the d1 loss is: 3146.9531, the d2 loss is: -3286.5, the g loss is: 3270.0, the ae loss is: 0.009365479, the jacobian loss is:0.10157332\n",
            "This is the iter 7977, the d1 loss is: 2947.836, the d2 loss is: -3124.4219, the g loss is: 3102.1406, the ae loss is: 0.007712359, the jacobian loss is:0.08406195\n",
            "This is the iter 7978, the d1 loss is: 3373.0625, the d2 loss is: -3510.9844, the g loss is: 3461.1562, the ae loss is: 0.007816422, the jacobian loss is:0.090357795\n",
            "This is the iter 7979, the d1 loss is: 3023.2969, the d2 loss is: -3201.7188, the g loss is: 3206.789, the ae loss is: 0.007234446, the jacobian loss is:0.09359653\n",
            "This is the iter 7980, the d1 loss is: 3348.461, the d2 loss is: -3547.5078, the g loss is: 3489.9531, the ae loss is: 0.008566253, the jacobian loss is:0.12881915\n",
            "This is the iter 7981, the d1 loss is: 3018.4688, the d2 loss is: -3221.3516, the g loss is: 3172.4844, the ae loss is: 0.008299853, the jacobian loss is:0.09543407\n",
            "This is the iter 7982, the d1 loss is: 3086.5547, the d2 loss is: -3250.3984, the g loss is: 3253.1875, the ae loss is: 0.006704696, the jacobian loss is:0.10512339\n",
            "This is the iter 7983, the d1 loss is: 3129.3906, the d2 loss is: -3238.125, the g loss is: 3209.211, the ae loss is: 0.0059827333, the jacobian loss is:0.09242699\n",
            "This is the iter 7984, the d1 loss is: 3146.9453, the d2 loss is: -3313.1562, the g loss is: 3258.75, the ae loss is: 0.0070373532, the jacobian loss is:0.11183271\n",
            "This is the iter 7985, the d1 loss is: 3064.4062, the d2 loss is: -3230.7969, the g loss is: 3223.9688, the ae loss is: 0.00797066, the jacobian loss is:0.11641583\n",
            "This is the iter 7986, the d1 loss is: 3112.9453, the d2 loss is: -3288.3047, the g loss is: 3327.5938, the ae loss is: 0.008535415, the jacobian loss is:0.10636591\n",
            "This is the iter 7987, the d1 loss is: 3187.414, the d2 loss is: -3363.2734, the g loss is: 3321.8594, the ae loss is: 0.009007793, the jacobian loss is:0.1034632\n",
            "This is the iter 7988, the d1 loss is: 3195.1875, the d2 loss is: -3348.3672, the g loss is: 3305.7266, the ae loss is: 0.008954936, the jacobian loss is:0.10918018\n",
            "This is the iter 7989, the d1 loss is: 3127.3906, the d2 loss is: -3291.1875, the g loss is: 3250.4375, the ae loss is: 0.0061587803, the jacobian loss is:0.11071942\n",
            "This is the iter 7990, the d1 loss is: 3193.9219, the d2 loss is: -3342.9375, the g loss is: 3340.4453, the ae loss is: 0.006102617, the jacobian loss is:0.12258598\n",
            "This is the iter 7991, the d1 loss is: 3062.2266, the d2 loss is: -3235.375, the g loss is: 3203.25, the ae loss is: 0.011413388, the jacobian loss is:0.107755065\n",
            "This is the iter 7992, the d1 loss is: 3172.9922, the d2 loss is: -3329.1562, the g loss is: 3335.9375, the ae loss is: 0.008762642, the jacobian loss is:0.06711872\n",
            "This is the iter 7993, the d1 loss is: 3040.461, the d2 loss is: -3167.7266, the g loss is: 3295.0078, the ae loss is: 0.009722576, the jacobian loss is:0.10876419\n",
            "This is the iter 7994, the d1 loss is: 3172.2422, the d2 loss is: -3325.0469, the g loss is: 3275.6094, the ae loss is: 0.007975299, the jacobian loss is:0.123499334\n",
            "This is the iter 7995, the d1 loss is: 3069.1484, the d2 loss is: -3255.5, the g loss is: 3292.0781, the ae loss is: 0.0062612444, the jacobian loss is:0.09711771\n",
            "This is the iter 7996, the d1 loss is: 3189.0312, the d2 loss is: -3345.4844, the g loss is: 3321.586, the ae loss is: 0.009435135, the jacobian loss is:0.1069826\n",
            "This is the iter 7997, the d1 loss is: 3110.4922, the d2 loss is: -3252.0781, the g loss is: 3285.1719, the ae loss is: 0.006835, the jacobian loss is:0.07528909\n",
            "This is the iter 7998, the d1 loss is: 2951.0547, the d2 loss is: -3072.9375, the g loss is: 3058.4531, the ae loss is: 0.008265564, the jacobian loss is:0.07227534\n",
            "This is the iter 7999, the d1 loss is: 3294.7578, the d2 loss is: -3465.6172, the g loss is: 3453.4297, the ae loss is: 0.0063353926, the jacobian loss is:0.09678068\n",
            "This is the iter 8000, the d1 loss is: 2924.0156, the d2 loss is: -3061.8594, the g loss is: 3048.336, the ae loss is: 0.005866152, the jacobian loss is:0.11248551\n",
            "0.25286794\n",
            "0.96235704\n",
            "This is the iter 8001, the d1 loss is: 2979.5625, the d2 loss is: -3139.0, the g loss is: 3160.9375, the ae loss is: 0.0076630805, the jacobian loss is:0.08218836\n",
            "This is the iter 8002, the d1 loss is: 3146.9219, the d2 loss is: -3305.5625, the g loss is: 3312.1094, the ae loss is: 0.0068074376, the jacobian loss is:0.08703773\n",
            "This is the iter 8003, the d1 loss is: 3080.3203, the d2 loss is: -3226.8828, the g loss is: 3227.3281, the ae loss is: 0.006957522, the jacobian loss is:0.11498267\n",
            "This is the iter 8004, the d1 loss is: 3174.0625, the d2 loss is: -3325.5469, the g loss is: 3299.414, the ae loss is: 0.005402628, the jacobian loss is:0.08088834\n",
            "This is the iter 8005, the d1 loss is: 3216.211, the d2 loss is: -3362.8906, the g loss is: 3394.6016, the ae loss is: 0.009309545, the jacobian loss is:0.100487515\n",
            "This is the iter 8006, the d1 loss is: 3070.8438, the d2 loss is: -3236.2969, the g loss is: 3221.4844, the ae loss is: 0.006550081, the jacobian loss is:0.089684114\n",
            "This is the iter 8007, the d1 loss is: 3075.7188, the d2 loss is: -3222.9688, the g loss is: 3242.0469, the ae loss is: 0.0066199033, the jacobian loss is:0.08803848\n",
            "This is the iter 8008, the d1 loss is: 3092.6406, the d2 loss is: -3268.9844, the g loss is: 3250.4062, the ae loss is: 0.00732359, the jacobian loss is:0.07917889\n",
            "This is the iter 8009, the d1 loss is: 2912.4922, the d2 loss is: -3068.2266, the g loss is: 3026.289, the ae loss is: 0.008319899, the jacobian loss is:0.07651423\n",
            "This is the iter 8010, the d1 loss is: 3177.4688, the d2 loss is: -3338.586, the g loss is: 3270.0547, the ae loss is: 0.007307096, the jacobian loss is:0.1054957\n",
            "This is the iter 8011, the d1 loss is: 3099.8828, the d2 loss is: -3247.836, the g loss is: 3272.5234, the ae loss is: 0.005668101, the jacobian loss is:0.08152582\n",
            "This is the iter 8012, the d1 loss is: 3185.8281, the d2 loss is: -3336.336, the g loss is: 3327.5, the ae loss is: 0.0070126075, the jacobian loss is:0.11222996\n",
            "This is the iter 8013, the d1 loss is: 3111.6406, the d2 loss is: -3268.0547, the g loss is: 3245.6719, the ae loss is: 0.012316506, the jacobian loss is:0.09555191\n",
            "This is the iter 8014, the d1 loss is: 3121.3281, the d2 loss is: -3273.4219, the g loss is: 3234.1172, the ae loss is: 0.007601527, the jacobian loss is:0.0858715\n",
            "This is the iter 8015, the d1 loss is: 2955.0156, the d2 loss is: -3112.8125, the g loss is: 3206.7344, the ae loss is: 0.007647956, the jacobian loss is:0.078760356\n",
            "This is the iter 8016, the d1 loss is: 3307.4062, the d2 loss is: -3477.6328, the g loss is: 3466.3672, the ae loss is: 0.0063017616, the jacobian loss is:0.082660705\n",
            "This is the iter 8017, the d1 loss is: 3134.375, the d2 loss is: -3291.4375, the g loss is: 3203.2656, the ae loss is: 0.009055697, the jacobian loss is:0.14390124\n",
            "This is the iter 8018, the d1 loss is: 2999.836, the d2 loss is: -3148.3828, the g loss is: 3114.9766, the ae loss is: 0.007651016, the jacobian loss is:0.1069209\n",
            "This is the iter 8019, the d1 loss is: 3000.4375, the d2 loss is: -3178.1719, the g loss is: 3181.1328, the ae loss is: 0.007560132, the jacobian loss is:0.087648615\n",
            "This is the iter 8020, the d1 loss is: 3119.2812, the d2 loss is: -3253.6953, the g loss is: 3234.1562, the ae loss is: 0.0072378158, the jacobian loss is:0.073656954\n",
            "This is the iter 8021, the d1 loss is: 3098.9688, the d2 loss is: -3246.6719, the g loss is: 3278.625, the ae loss is: 0.0062108017, the jacobian loss is:0.09388262\n",
            "This is the iter 8022, the d1 loss is: 3109.9453, the d2 loss is: -3279.6562, the g loss is: 3294.3125, the ae loss is: 0.0081685055, the jacobian loss is:0.14845042\n",
            "This is the iter 8023, the d1 loss is: 3160.5312, the d2 loss is: -3291.5234, the g loss is: 3304.375, the ae loss is: 0.008137951, the jacobian loss is:0.07173433\n",
            "This is the iter 8024, the d1 loss is: 3121.1562, the d2 loss is: -3299.2344, the g loss is: 3268.6719, the ae loss is: 0.009895505, the jacobian loss is:0.10861789\n",
            "This is the iter 8025, the d1 loss is: 2834.5312, the d2 loss is: -2969.1484, the g loss is: 2960.6094, the ae loss is: 0.0062455507, the jacobian loss is:0.078399345\n",
            "This is the iter 8026, the d1 loss is: 3087.3438, the d2 loss is: -3237.3438, the g loss is: 3334.5312, the ae loss is: 0.0061275056, the jacobian loss is:0.09718914\n",
            "This is the iter 8027, the d1 loss is: 3038.7578, the d2 loss is: -3180.7656, the g loss is: 3264.1562, the ae loss is: 0.010250342, the jacobian loss is:0.10604756\n",
            "This is the iter 8028, the d1 loss is: 3294.3828, the d2 loss is: -3447.1719, the g loss is: 3419.6016, the ae loss is: 0.0060754134, the jacobian loss is:0.083255544\n",
            "This is the iter 8029, the d1 loss is: 2882.1484, the d2 loss is: -3069.4297, the g loss is: 3062.0, the ae loss is: 0.0061455416, the jacobian loss is:0.08180935\n",
            "This is the iter 8030, the d1 loss is: 3125.711, the d2 loss is: -3268.0, the g loss is: 3334.0547, the ae loss is: 0.007417108, the jacobian loss is:0.09644394\n",
            "This is the iter 8031, the d1 loss is: 3013.9219, the d2 loss is: -3148.4219, the g loss is: 3181.7188, the ae loss is: 0.0051001497, the jacobian loss is:0.078723356\n",
            "This is the iter 8032, the d1 loss is: 3073.086, the d2 loss is: -3264.75, the g loss is: 3232.5781, the ae loss is: 0.007854291, the jacobian loss is:0.116451606\n",
            "This is the iter 8033, the d1 loss is: 3228.5625, the d2 loss is: -3365.2031, the g loss is: 3340.7578, the ae loss is: 0.005493462, the jacobian loss is:0.08403481\n",
            "This is the iter 8034, the d1 loss is: 2941.5781, the d2 loss is: -3101.75, the g loss is: 3115.2344, the ae loss is: 0.008054274, the jacobian loss is:0.11685435\n",
            "This is the iter 8035, the d1 loss is: 3100.75, the d2 loss is: -3252.7969, the g loss is: 3253.0547, the ae loss is: 0.0066998764, the jacobian loss is:0.09037574\n",
            "This is the iter 8036, the d1 loss is: 3137.1953, the d2 loss is: -3309.3047, the g loss is: 3297.0703, the ae loss is: 0.0076067215, the jacobian loss is:0.085749\n",
            "This is the iter 8037, the d1 loss is: 3098.8906, the d2 loss is: -3240.3828, the g loss is: 3290.9062, the ae loss is: 0.008962959, the jacobian loss is:0.082621254\n",
            "This is the iter 8038, the d1 loss is: 3411.8203, the d2 loss is: -3539.4844, the g loss is: 3496.6719, the ae loss is: 0.0061575505, the jacobian loss is:0.095905006\n",
            "This is the iter 8039, the d1 loss is: 3016.4531, the d2 loss is: -3165.8906, the g loss is: 3205.0703, the ae loss is: 0.0065879487, the jacobian loss is:0.096012875\n",
            "This is the iter 8040, the d1 loss is: 3031.1016, the d2 loss is: -3174.3047, the g loss is: 3215.8672, the ae loss is: 0.0056786453, the jacobian loss is:0.08986806\n",
            "This is the iter 8041, the d1 loss is: 3086.8516, the d2 loss is: -3239.0156, the g loss is: 3190.4219, the ae loss is: 0.0062118284, the jacobian loss is:0.09653585\n",
            "This is the iter 8042, the d1 loss is: 3107.836, the d2 loss is: -3253.664, the g loss is: 3236.9375, the ae loss is: 0.004936801, the jacobian loss is:0.10342928\n",
            "This is the iter 8043, the d1 loss is: 3069.3828, the d2 loss is: -3219.5156, the g loss is: 3222.7266, the ae loss is: 0.009402465, the jacobian loss is:0.10509411\n",
            "This is the iter 8044, the d1 loss is: 3154.3438, the d2 loss is: -3308.2266, the g loss is: 3295.8906, the ae loss is: 0.007863315, the jacobian loss is:0.07930291\n",
            "This is the iter 8045, the d1 loss is: 3055.3047, the d2 loss is: -3234.5156, the g loss is: 3205.2734, the ae loss is: 0.0067814672, the jacobian loss is:0.09102763\n",
            "This is the iter 8046, the d1 loss is: 3219.914, the d2 loss is: -3369.5156, the g loss is: 3339.875, the ae loss is: 0.007986931, the jacobian loss is:0.10139668\n",
            "This is the iter 8047, the d1 loss is: 3061.1562, the d2 loss is: -3217.6094, the g loss is: 3203.461, the ae loss is: 0.005502099, the jacobian loss is:0.09320961\n",
            "This is the iter 8048, the d1 loss is: 3069.4531, the d2 loss is: -3229.3438, the g loss is: 3130.6016, the ae loss is: 0.010157738, the jacobian loss is:0.08443549\n",
            "This is the iter 8049, the d1 loss is: 3114.6406, the d2 loss is: -3262.8672, the g loss is: 3204.961, the ae loss is: 0.006089747, the jacobian loss is:0.07852418\n",
            "This is the iter 8050, the d1 loss is: 3131.8281, the d2 loss is: -3283.211, the g loss is: 3288.1328, the ae loss is: 0.006395408, the jacobian loss is:0.08331549\n",
            "This is the iter 8051, the d1 loss is: 2989.5938, the d2 loss is: -3140.961, the g loss is: 3132.9844, the ae loss is: 0.006277591, the jacobian loss is:0.07951442\n",
            "This is the iter 8052, the d1 loss is: 3114.086, the d2 loss is: -3252.9922, the g loss is: 3234.375, the ae loss is: 0.0068526426, the jacobian loss is:0.08510298\n",
            "This is the iter 8053, the d1 loss is: 2986.9375, the d2 loss is: -3163.7188, the g loss is: 3158.7578, the ae loss is: 0.009039096, the jacobian loss is:0.097978115\n",
            "This is the iter 8054, the d1 loss is: 3084.1484, the d2 loss is: -3246.2812, the g loss is: 3251.375, the ae loss is: 0.0059421216, the jacobian loss is:0.0905676\n",
            "This is the iter 8055, the d1 loss is: 3050.6406, the d2 loss is: -3230.75, the g loss is: 3175.9688, the ae loss is: 0.008057371, the jacobian loss is:0.099372976\n",
            "This is the iter 8056, the d1 loss is: 3109.5703, the d2 loss is: -3263.4844, the g loss is: 3268.9453, the ae loss is: 0.006525848, the jacobian loss is:0.07049236\n",
            "This is the iter 8057, the d1 loss is: 3415.1094, the d2 loss is: -3570.6953, the g loss is: 3511.7812, the ae loss is: 0.0055551156, the jacobian loss is:0.10127909\n",
            "This is the iter 8058, the d1 loss is: 2949.9375, the d2 loss is: -3102.75, the g loss is: 3058.3672, the ae loss is: 0.0085406825, the jacobian loss is:0.07671946\n",
            "This is the iter 8059, the d1 loss is: 2960.664, the d2 loss is: -3108.5, the g loss is: 3063.9688, the ae loss is: 0.008218598, the jacobian loss is:0.113693416\n",
            "This is the iter 8060, the d1 loss is: 3064.5781, the d2 loss is: -3212.5781, the g loss is: 3282.961, the ae loss is: 0.009208872, the jacobian loss is:0.06979371\n",
            "This is the iter 8061, the d1 loss is: 3114.7578, the d2 loss is: -3284.289, the g loss is: 3256.7266, the ae loss is: 0.0060929814, the jacobian loss is:0.08009269\n",
            "This is the iter 8062, the d1 loss is: 3061.3281, the d2 loss is: -3246.8203, the g loss is: 3231.8203, the ae loss is: 0.0062698424, the jacobian loss is:0.07819332\n",
            "This is the iter 8063, the d1 loss is: 3085.1484, the d2 loss is: -3221.8906, the g loss is: 3173.2734, the ae loss is: 0.008401984, the jacobian loss is:0.105947345\n",
            "This is the iter 8064, the d1 loss is: 3131.4297, the d2 loss is: -3283.1328, the g loss is: 3312.9297, the ae loss is: 0.0068653105, the jacobian loss is:0.07811545\n",
            "This is the iter 8065, the d1 loss is: 3161.5078, the d2 loss is: -3329.6875, the g loss is: 3318.164, the ae loss is: 0.0077134976, the jacobian loss is:0.08298627\n",
            "This is the iter 8066, the d1 loss is: 3261.625, the d2 loss is: -3400.7188, the g loss is: 3457.6328, the ae loss is: 0.008227175, the jacobian loss is:0.08310863\n",
            "This is the iter 8067, the d1 loss is: 2888.2188, the d2 loss is: -3038.1484, the g loss is: 3047.2656, the ae loss is: 0.009979281, the jacobian loss is:0.077661306\n",
            "This is the iter 8068, the d1 loss is: 3309.9297, the d2 loss is: -3456.2031, the g loss is: 3462.3516, the ae loss is: 0.00494596, the jacobian loss is:0.08713461\n",
            "This is the iter 8069, the d1 loss is: 3009.586, the d2 loss is: -3161.7969, the g loss is: 3173.9375, the ae loss is: 0.007980456, the jacobian loss is:0.110422865\n",
            "This is the iter 8070, the d1 loss is: 3042.4062, the d2 loss is: -3204.461, the g loss is: 3247.6094, the ae loss is: 0.0060039056, the jacobian loss is:0.1253867\n",
            "This is the iter 8071, the d1 loss is: 3162.8906, the d2 loss is: -3307.0469, the g loss is: 3278.0469, the ae loss is: 0.00587472, the jacobian loss is:0.11093409\n",
            "This is the iter 8072, the d1 loss is: 3138.8438, the d2 loss is: -3275.7266, the g loss is: 3253.8594, the ae loss is: 0.0059061265, the jacobian loss is:0.11378147\n",
            "This is the iter 8073, the d1 loss is: 3380.6484, the d2 loss is: -3516.1797, the g loss is: 3447.9766, the ae loss is: 0.007944291, the jacobian loss is:0.08826578\n",
            "This is the iter 8074, the d1 loss is: 3053.4375, the d2 loss is: -3199.2812, the g loss is: 3264.4766, the ae loss is: 0.0056691896, the jacobian loss is:0.092606045\n",
            "This is the iter 8075, the d1 loss is: 3231.7188, the d2 loss is: -3412.8984, the g loss is: 3405.8828, the ae loss is: 0.0069591175, the jacobian loss is:0.10795785\n",
            "This is the iter 8076, the d1 loss is: 2918.4844, the d2 loss is: -3097.0938, the g loss is: 3077.375, the ae loss is: 0.006259583, the jacobian loss is:0.11436815\n",
            "This is the iter 8077, the d1 loss is: 2922.2188, the d2 loss is: -3051.4922, the g loss is: 3077.7578, the ae loss is: 0.0067419363, the jacobian loss is:0.09404936\n",
            "This is the iter 8078, the d1 loss is: 3377.2812, the d2 loss is: -3547.8125, the g loss is: 3534.3906, the ae loss is: 0.008842086, the jacobian loss is:0.11019411\n",
            "This is the iter 8079, the d1 loss is: 2963.211, the d2 loss is: -3115.3984, the g loss is: 3097.75, the ae loss is: 0.0077579967, the jacobian loss is:0.119713426\n",
            "This is the iter 8080, the d1 loss is: 3180.289, the d2 loss is: -3350.3125, the g loss is: 3394.3672, the ae loss is: 0.0063330373, the jacobian loss is:0.084298104\n",
            "This is the iter 8081, the d1 loss is: 3002.3281, the d2 loss is: -3189.3984, the g loss is: 3212.9844, the ae loss is: 0.0063017383, the jacobian loss is:0.11201553\n",
            "This is the iter 8082, the d1 loss is: 3138.9688, the d2 loss is: -3303.0078, the g loss is: 3307.0781, the ae loss is: 0.009138068, the jacobian loss is:0.08386931\n",
            "This is the iter 8083, the d1 loss is: 3116.4844, the d2 loss is: -3239.4219, the g loss is: 3179.3828, the ae loss is: 0.005953964, the jacobian loss is:0.103169546\n",
            "This is the iter 8084, the d1 loss is: 3059.1719, the d2 loss is: -3223.7578, the g loss is: 3211.3516, the ae loss is: 0.007936049, the jacobian loss is:0.0889384\n",
            "This is the iter 8085, the d1 loss is: 3078.3828, the d2 loss is: -3217.2188, the g loss is: 3228.2344, the ae loss is: 0.005087368, the jacobian loss is:0.07091512\n",
            "This is the iter 8086, the d1 loss is: 3150.5625, the d2 loss is: -3311.9766, the g loss is: 3356.0781, the ae loss is: 0.0067452667, the jacobian loss is:0.078915864\n",
            "This is the iter 8087, the d1 loss is: 3032.1719, the d2 loss is: -3207.2188, the g loss is: 3200.8906, the ae loss is: 0.0058257477, the jacobian loss is:0.14140873\n",
            "This is the iter 8088, the d1 loss is: 3127.2969, the d2 loss is: -3298.1953, the g loss is: 3313.164, the ae loss is: 0.008053942, the jacobian loss is:0.087505355\n",
            "This is the iter 8089, the d1 loss is: 2957.086, the d2 loss is: -3066.3594, the g loss is: 3086.8203, the ae loss is: 0.006230934, the jacobian loss is:0.08527761\n",
            "This is the iter 8090, the d1 loss is: 3066.9844, the d2 loss is: -3199.7969, the g loss is: 3253.4688, the ae loss is: 0.0066102818, the jacobian loss is:0.10745487\n",
            "This is the iter 8091, the d1 loss is: 3308.8594, the d2 loss is: -3479.375, the g loss is: 3490.6328, the ae loss is: 0.0057570226, the jacobian loss is:0.088173\n",
            "This is the iter 8092, the d1 loss is: 3107.711, the d2 loss is: -3259.5703, the g loss is: 3261.9844, the ae loss is: 0.007314371, the jacobian loss is:0.110864475\n",
            "This is the iter 8093, the d1 loss is: 3184.9922, the d2 loss is: -3335.586, the g loss is: 3324.7266, the ae loss is: 0.007160768, the jacobian loss is:0.094446726\n",
            "This is the iter 8094, the d1 loss is: 3285.375, the d2 loss is: -3424.6562, the g loss is: 3385.5312, the ae loss is: 0.00819803, the jacobian loss is:0.09120074\n",
            "This is the iter 8095, the d1 loss is: 3023.4844, the d2 loss is: -3153.2344, the g loss is: 3243.289, the ae loss is: 0.009723314, the jacobian loss is:0.08219882\n",
            "This is the iter 8096, the d1 loss is: 3015.3281, the d2 loss is: -3189.836, the g loss is: 3185.1953, the ae loss is: 0.008215876, the jacobian loss is:0.11937129\n",
            "This is the iter 8097, the d1 loss is: 3287.5938, the d2 loss is: -3435.6328, the g loss is: 3492.0625, the ae loss is: 0.0068666693, the jacobian loss is:0.10105923\n",
            "This is the iter 8098, the d1 loss is: 3033.6172, the d2 loss is: -3183.5625, the g loss is: 3287.336, the ae loss is: 0.008168805, the jacobian loss is:0.083461106\n",
            "This is the iter 8099, the d1 loss is: 3058.539, the d2 loss is: -3228.2344, the g loss is: 3207.3438, the ae loss is: 0.009096567, the jacobian loss is:0.08954833\n",
            "This is the iter 8100, the d1 loss is: 3046.3438, the d2 loss is: -3208.164, the g loss is: 3260.9297, the ae loss is: 0.0074792597, the jacobian loss is:0.09739342\n",
            "0.2503304\n",
            "0.95242083\n",
            "This is the iter 8101, the d1 loss is: 3207.1797, the d2 loss is: -3360.7812, the g loss is: 3356.125, the ae loss is: 0.0075569586, the jacobian loss is:0.11246287\n",
            "This is the iter 8102, the d1 loss is: 3223.1016, the d2 loss is: -3388.1094, the g loss is: 3373.9453, the ae loss is: 0.0070083793, the jacobian loss is:0.10733052\n",
            "This is the iter 8103, the d1 loss is: 3244.414, the d2 loss is: -3391.75, the g loss is: 3403.7188, the ae loss is: 0.0077628857, the jacobian loss is:0.08075845\n",
            "This is the iter 8104, the d1 loss is: 3184.2812, the d2 loss is: -3321.7656, the g loss is: 3343.5156, the ae loss is: 0.006105138, the jacobian loss is:0.094809696\n",
            "This is the iter 8105, the d1 loss is: 3253.8594, the d2 loss is: -3403.875, the g loss is: 3324.3516, the ae loss is: 0.006103236, the jacobian loss is:0.14778164\n",
            "This is the iter 8106, the d1 loss is: 2867.5234, the d2 loss is: -3017.9844, the g loss is: 3055.4531, the ae loss is: 0.007233429, the jacobian loss is:0.06951448\n",
            "This is the iter 8107, the d1 loss is: 3047.8828, the d2 loss is: -3202.2812, the g loss is: 3170.375, the ae loss is: 0.006709519, the jacobian loss is:0.09545342\n",
            "This is the iter 8108, the d1 loss is: 3206.5469, the d2 loss is: -3367.3906, the g loss is: 3342.6875, the ae loss is: 0.009517565, the jacobian loss is:0.11634395\n",
            "This is the iter 8109, the d1 loss is: 3149.5234, the d2 loss is: -3290.9922, the g loss is: 3258.125, the ae loss is: 0.006276306, the jacobian loss is:0.07473753\n",
            "This is the iter 8110, the d1 loss is: 3091.5312, the d2 loss is: -3261.5078, the g loss is: 3280.4922, the ae loss is: 0.006154283, the jacobian loss is:0.08525197\n",
            "This is the iter 8111, the d1 loss is: 3111.4375, the d2 loss is: -3271.6797, the g loss is: 3233.8281, the ae loss is: 0.007562509, the jacobian loss is:0.07928498\n",
            "This is the iter 8112, the d1 loss is: 2866.2344, the d2 loss is: -3016.2266, the g loss is: 2978.2734, the ae loss is: 0.0059079262, the jacobian loss is:0.08759986\n",
            "This is the iter 8113, the d1 loss is: 2993.3125, the d2 loss is: -3144.586, the g loss is: 3230.8984, the ae loss is: 0.0057995203, the jacobian loss is:0.08785379\n",
            "This is the iter 8114, the d1 loss is: 3116.0312, the d2 loss is: -3240.6562, the g loss is: 3286.7656, the ae loss is: 0.005226433, the jacobian loss is:0.080913596\n",
            "This is the iter 8115, the d1 loss is: 3138.5312, the d2 loss is: -3270.9922, the g loss is: 3318.7422, the ae loss is: 0.006815614, the jacobian loss is:0.08551999\n",
            "This is the iter 8116, the d1 loss is: 3492.1719, the d2 loss is: -3652.3594, the g loss is: 3612.2031, the ae loss is: 0.0074848495, the jacobian loss is:0.09038299\n",
            "This is the iter 8117, the d1 loss is: 2853.086, the d2 loss is: -3004.8438, the g loss is: 2996.8516, the ae loss is: 0.005819424, the jacobian loss is:0.074339174\n",
            "This is the iter 8118, the d1 loss is: 3170.8516, the d2 loss is: -3323.9062, the g loss is: 3313.5469, the ae loss is: 0.0053517898, the jacobian loss is:0.07485716\n",
            "This is the iter 8119, the d1 loss is: 2973.3438, the d2 loss is: -3165.414, the g loss is: 3168.7188, the ae loss is: 0.0048838253, the jacobian loss is:0.097215265\n",
            "This is the iter 8120, the d1 loss is: 3147.3828, the d2 loss is: -3309.086, the g loss is: 3287.3594, the ae loss is: 0.006639606, the jacobian loss is:0.08324199\n",
            "This is the iter 8121, the d1 loss is: 3183.3828, the d2 loss is: -3339.461, the g loss is: 3377.711, the ae loss is: 0.007778896, the jacobian loss is:0.089818396\n",
            "This is the iter 8122, the d1 loss is: 3075.336, the d2 loss is: -3253.6562, the g loss is: 3222.1562, the ae loss is: 0.0066187084, the jacobian loss is:0.101905786\n",
            "This is the iter 8123, the d1 loss is: 3298.8594, the d2 loss is: -3428.3438, the g loss is: 3460.1719, the ae loss is: 0.008871943, the jacobian loss is:0.07266522\n",
            "This is the iter 8124, the d1 loss is: 3128.4219, the d2 loss is: -3267.789, the g loss is: 3307.5312, the ae loss is: 0.0059999037, the jacobian loss is:0.07580282\n",
            "This is the iter 8125, the d1 loss is: 3093.3125, the d2 loss is: -3239.2188, the g loss is: 3236.3125, the ae loss is: 0.0057126116, the jacobian loss is:0.071969084\n",
            "This is the iter 8126, the d1 loss is: 3083.3281, the d2 loss is: -3248.375, the g loss is: 3205.0938, the ae loss is: 0.00800791, the jacobian loss is:0.116982296\n",
            "This is the iter 8127, the d1 loss is: 3047.0156, the d2 loss is: -3236.7266, the g loss is: 3195.0938, the ae loss is: 0.007974765, the jacobian loss is:0.06342614\n",
            "This is the iter 8128, the d1 loss is: 3118.7578, the d2 loss is: -3268.4219, the g loss is: 3255.7266, the ae loss is: 0.005611783, the jacobian loss is:0.11983709\n",
            "This is the iter 8129, the d1 loss is: 3084.6953, the d2 loss is: -3265.336, the g loss is: 3284.2031, the ae loss is: 0.006230808, the jacobian loss is:0.10059879\n",
            "This is the iter 8130, the d1 loss is: 3239.3203, the d2 loss is: -3388.9766, the g loss is: 3374.0547, the ae loss is: 0.006577447, the jacobian loss is:0.08345019\n",
            "This is the iter 8131, the d1 loss is: 3116.4922, the d2 loss is: -3277.4844, the g loss is: 3262.5078, the ae loss is: 0.0069108536, the jacobian loss is:0.08006293\n",
            "This is the iter 8132, the d1 loss is: 3343.6484, the d2 loss is: -3493.5625, the g loss is: 3470.586, the ae loss is: 0.007493375, the jacobian loss is:0.09617991\n",
            "This is the iter 8133, the d1 loss is: 3027.375, the d2 loss is: -3152.4766, the g loss is: 3136.4844, the ae loss is: 0.0061504506, the jacobian loss is:0.0891677\n",
            "This is the iter 8134, the d1 loss is: 3146.289, the d2 loss is: -3311.4922, the g loss is: 3314.7188, the ae loss is: 0.008078684, the jacobian loss is:0.08951338\n",
            "This is the iter 8135, the d1 loss is: 3089.1797, the d2 loss is: -3238.1484, the g loss is: 3252.3906, the ae loss is: 0.0061184643, the jacobian loss is:0.07941543\n",
            "This is the iter 8136, the d1 loss is: 3185.3594, the d2 loss is: -3315.8906, the g loss is: 3265.8594, the ae loss is: 0.0074447133, the jacobian loss is:0.08115847\n",
            "This is the iter 8137, the d1 loss is: 3358.8672, the d2 loss is: -3502.8828, the g loss is: 3445.9922, the ae loss is: 0.0096057225, the jacobian loss is:0.09291984\n",
            "This is the iter 8138, the d1 loss is: 3162.0156, the d2 loss is: -3307.9219, the g loss is: 3308.25, the ae loss is: 0.008042693, the jacobian loss is:0.10933891\n",
            "This is the iter 8139, the d1 loss is: 3082.6016, the d2 loss is: -3243.5156, the g loss is: 3225.789, the ae loss is: 0.005223415, the jacobian loss is:0.10097271\n",
            "This is the iter 8140, the d1 loss is: 3159.3906, the d2 loss is: -3312.5078, the g loss is: 3295.5312, the ae loss is: 0.005888558, the jacobian loss is:0.08017565\n",
            "This is the iter 8141, the d1 loss is: 3084.2344, the d2 loss is: -3227.75, the g loss is: 3185.2812, the ae loss is: 0.00683681, the jacobian loss is:0.13513334\n",
            "This is the iter 8142, the d1 loss is: 3339.6797, the d2 loss is: -3507.789, the g loss is: 3478.7656, the ae loss is: 0.0063817017, the jacobian loss is:0.1146477\n",
            "This is the iter 8143, the d1 loss is: 3190.7031, the d2 loss is: -3346.5156, the g loss is: 3351.211, the ae loss is: 0.008960679, the jacobian loss is:0.08312623\n",
            "This is the iter 8144, the d1 loss is: 3065.6406, the d2 loss is: -3194.2969, the g loss is: 3234.5312, the ae loss is: 0.0055553997, the jacobian loss is:0.09542382\n",
            "This is the iter 8145, the d1 loss is: 3106.7734, the d2 loss is: -3261.9219, the g loss is: 3208.336, the ae loss is: 0.008901218, the jacobian loss is:0.09142123\n",
            "This is the iter 8146, the d1 loss is: 3138.125, the d2 loss is: -3267.2422, the g loss is: 3241.2188, the ae loss is: 0.004589819, the jacobian loss is:0.113574624\n",
            "This is the iter 8147, the d1 loss is: 3059.0078, the d2 loss is: -3206.789, the g loss is: 3219.9531, the ae loss is: 0.0076855766, the jacobian loss is:0.090695076\n",
            "This is the iter 8148, the d1 loss is: 3179.961, the d2 loss is: -3327.4219, the g loss is: 3266.0078, the ae loss is: 0.0065735197, the jacobian loss is:0.11790506\n",
            "This is the iter 8149, the d1 loss is: 3145.8281, the d2 loss is: -3261.8047, the g loss is: 3239.8672, the ae loss is: 0.0070948084, the jacobian loss is:0.08450264\n",
            "This is the iter 8150, the d1 loss is: 2934.5, the d2 loss is: -3055.664, the g loss is: 3040.4375, the ae loss is: 0.00657468, the jacobian loss is:0.11716208\n",
            "This is the iter 8151, the d1 loss is: 2987.664, the d2 loss is: -3149.6094, the g loss is: 3191.6328, the ae loss is: 0.0072312765, the jacobian loss is:0.09812597\n",
            "This is the iter 8152, the d1 loss is: 3156.6719, the d2 loss is: -3290.9766, the g loss is: 3306.2656, the ae loss is: 0.0061032213, the jacobian loss is:0.10086486\n",
            "This is the iter 8153, the d1 loss is: 3107.0312, the d2 loss is: -3228.8672, the g loss is: 3194.8828, the ae loss is: 0.0074014813, the jacobian loss is:0.07622339\n",
            "This is the iter 8154, the d1 loss is: 3095.8906, the d2 loss is: -3203.5625, the g loss is: 3292.6484, the ae loss is: 0.0061357087, the jacobian loss is:0.09768218\n",
            "This is the iter 8155, the d1 loss is: 3008.3516, the d2 loss is: -3176.9297, the g loss is: 3257.0312, the ae loss is: 0.0070249448, the jacobian loss is:0.09330255\n",
            "This is the iter 8156, the d1 loss is: 3165.5, the d2 loss is: -3301.5703, the g loss is: 3256.4844, the ae loss is: 0.008483091, the jacobian loss is:0.06971905\n",
            "This is the iter 8157, the d1 loss is: 3072.961, the d2 loss is: -3215.7812, the g loss is: 3202.0938, the ae loss is: 0.0074141524, the jacobian loss is:0.09074862\n",
            "This is the iter 8158, the d1 loss is: 2993.414, the d2 loss is: -3133.6875, the g loss is: 3142.625, the ae loss is: 0.0067585884, the jacobian loss is:0.084828265\n",
            "This is the iter 8159, the d1 loss is: 3074.8203, the d2 loss is: -3219.6094, the g loss is: 3180.375, the ae loss is: 0.009600483, the jacobian loss is:0.1324437\n",
            "This is the iter 8160, the d1 loss is: 3260.8438, the d2 loss is: -3425.3203, the g loss is: 3399.1875, the ae loss is: 0.0074026613, the jacobian loss is:0.08582229\n",
            "This is the iter 8161, the d1 loss is: 3027.4531, the d2 loss is: -3164.3281, the g loss is: 3161.0156, the ae loss is: 0.0077612335, the jacobian loss is:0.1109496\n",
            "This is the iter 8162, the d1 loss is: 3182.0469, the d2 loss is: -3342.6406, the g loss is: 3264.0625, the ae loss is: 0.006440624, the jacobian loss is:0.14467624\n",
            "This is the iter 8163, the d1 loss is: 3081.2188, the d2 loss is: -3231.8984, the g loss is: 3245.1172, the ae loss is: 0.0045718695, the jacobian loss is:0.10140879\n",
            "This is the iter 8164, the d1 loss is: 3133.9219, the d2 loss is: -3292.0078, the g loss is: 3266.6953, the ae loss is: 0.0075350106, the jacobian loss is:0.096020475\n",
            "This is the iter 8165, the d1 loss is: 3088.5547, the d2 loss is: -3226.1094, the g loss is: 3230.9922, the ae loss is: 0.008775111, the jacobian loss is:0.10055653\n",
            "This is the iter 8166, the d1 loss is: 3047.9844, the d2 loss is: -3210.5703, the g loss is: 3202.039, the ae loss is: 0.007253968, the jacobian loss is:0.087905504\n",
            "This is the iter 8167, the d1 loss is: 3147.5781, the d2 loss is: -3295.8438, the g loss is: 3287.8281, the ae loss is: 0.007211784, the jacobian loss is:0.08843171\n",
            "This is the iter 8168, the d1 loss is: 3384.7812, the d2 loss is: -3513.4375, the g loss is: 3493.9062, the ae loss is: 0.006210167, the jacobian loss is:0.11554929\n",
            "This is the iter 8169, the d1 loss is: 2874.9062, the d2 loss is: -3028.0547, the g loss is: 3023.9375, the ae loss is: 0.0066046626, the jacobian loss is:0.11582016\n",
            "This is the iter 8170, the d1 loss is: 3153.8672, the d2 loss is: -3324.0469, the g loss is: 3324.1172, the ae loss is: 0.0062479284, the jacobian loss is:0.11797234\n",
            "This is the iter 8171, the d1 loss is: 3048.9453, the d2 loss is: -3179.5781, the g loss is: 3219.586, the ae loss is: 0.004921407, the jacobian loss is:0.13683197\n",
            "This is the iter 8172, the d1 loss is: 3092.9844, the d2 loss is: -3210.3125, the g loss is: 3197.1719, the ae loss is: 0.0072283065, the jacobian loss is:0.07916029\n",
            "This is the iter 8173, the d1 loss is: 3347.9688, the d2 loss is: -3502.5781, the g loss is: 3494.7734, the ae loss is: 0.0069078663, the jacobian loss is:0.07494371\n",
            "This is the iter 8174, the d1 loss is: 3202.9766, the d2 loss is: -3347.8438, the g loss is: 3357.4844, the ae loss is: 0.008979093, the jacobian loss is:0.09643786\n",
            "This is the iter 8175, the d1 loss is: 2872.5781, the d2 loss is: -3001.5, the g loss is: 3078.1406, the ae loss is: 0.005215154, the jacobian loss is:0.08930072\n",
            "This is the iter 8176, the d1 loss is: 3194.1094, the d2 loss is: -3319.8438, the g loss is: 3295.9844, the ae loss is: 0.006391431, the jacobian loss is:0.09880386\n",
            "This is the iter 8177, the d1 loss is: 3088.9766, the d2 loss is: -3229.1875, the g loss is: 3231.125, the ae loss is: 0.005045104, the jacobian loss is:0.08551836\n",
            "This is the iter 8178, the d1 loss is: 3135.625, the d2 loss is: -3281.8906, the g loss is: 3222.1562, the ae loss is: 0.0063885516, the jacobian loss is:0.11598286\n",
            "This is the iter 8179, the d1 loss is: 3049.2969, the d2 loss is: -3212.2969, the g loss is: 3198.711, the ae loss is: 0.0079763625, the jacobian loss is:0.13209532\n",
            "This is the iter 8180, the d1 loss is: 3160.0156, the d2 loss is: -3294.711, the g loss is: 3337.539, the ae loss is: 0.0066240574, the jacobian loss is:0.07592815\n",
            "This is the iter 8181, the d1 loss is: 3060.0547, the d2 loss is: -3215.5938, the g loss is: 3163.4219, the ae loss is: 0.0066534327, the jacobian loss is:0.08865728\n",
            "This is the iter 8182, the d1 loss is: 3201.3828, the d2 loss is: -3352.4844, the g loss is: 3331.4766, the ae loss is: 0.006949426, the jacobian loss is:0.096048385\n",
            "This is the iter 8183, the d1 loss is: 3022.7578, the d2 loss is: -3173.7344, the g loss is: 3182.211, the ae loss is: 0.007487122, the jacobian loss is:0.08844196\n",
            "This is the iter 8184, the d1 loss is: 2948.8125, the d2 loss is: -3090.586, the g loss is: 3026.2656, the ae loss is: 0.0052005043, the jacobian loss is:0.10475923\n",
            "This is the iter 8185, the d1 loss is: 3070.2031, the d2 loss is: -3242.1016, the g loss is: 3197.1562, the ae loss is: 0.005826758, the jacobian loss is:0.071978115\n",
            "This is the iter 8186, the d1 loss is: 3120.211, the d2 loss is: -3252.9844, the g loss is: 3286.2812, the ae loss is: 0.0049161026, the jacobian loss is:0.09087728\n",
            "This is the iter 8187, the d1 loss is: 3110.0156, the d2 loss is: -3253.3281, the g loss is: 3222.3594, the ae loss is: 0.006518762, the jacobian loss is:0.07557836\n",
            "This is the iter 8188, the d1 loss is: 3240.125, the d2 loss is: -3391.0703, the g loss is: 3342.3594, the ae loss is: 0.0048717693, the jacobian loss is:0.061668873\n",
            "This is the iter 8189, the d1 loss is: 3294.5469, the d2 loss is: -3421.7344, the g loss is: 3401.5469, the ae loss is: 0.0067002457, the jacobian loss is:0.08793834\n",
            "This is the iter 8190, the d1 loss is: 2985.0312, the d2 loss is: -3121.8594, the g loss is: 3119.7969, the ae loss is: 0.0047063217, the jacobian loss is:0.119206086\n",
            "This is the iter 8191, the d1 loss is: 3063.75, the d2 loss is: -3194.4219, the g loss is: 3197.7969, the ae loss is: 0.0072817323, the jacobian loss is:0.0743184\n",
            "This is the iter 8192, the d1 loss is: 3070.1719, the d2 loss is: -3216.3516, the g loss is: 3223.375, the ae loss is: 0.008412659, the jacobian loss is:0.095623456\n",
            "This is the iter 8193, the d1 loss is: 3085.5703, the d2 loss is: -3221.3281, the g loss is: 3187.3438, the ae loss is: 0.008554909, the jacobian loss is:0.08151655\n",
            "This is the iter 8194, the d1 loss is: 3274.7188, the d2 loss is: -3404.5078, the g loss is: 3407.9688, the ae loss is: 0.008423011, the jacobian loss is:0.12257223\n",
            "This is the iter 8195, the d1 loss is: 3059.6094, the d2 loss is: -3193.6875, the g loss is: 3141.3672, the ae loss is: 0.0065310774, the jacobian loss is:0.07231153\n",
            "This is the iter 8196, the d1 loss is: 3081.3438, the d2 loss is: -3202.7578, the g loss is: 3243.5938, the ae loss is: 0.003728282, the jacobian loss is:0.0882885\n",
            "This is the iter 8197, the d1 loss is: 3025.6094, the d2 loss is: -3165.9375, the g loss is: 3183.1562, the ae loss is: 0.0075942418, the jacobian loss is:0.085509375\n",
            "This is the iter 8198, the d1 loss is: 3142.6406, the d2 loss is: -3282.7812, the g loss is: 3284.4219, the ae loss is: 0.0071059125, the jacobian loss is:0.0791743\n",
            "This is the iter 8199, the d1 loss is: 3188.2422, the d2 loss is: -3292.5312, the g loss is: 3288.6328, the ae loss is: 0.007760706, the jacobian loss is:0.10143836\n",
            "This is the iter 8200, the d1 loss is: 2962.5703, the d2 loss is: -3118.25, the g loss is: 3143.0781, the ae loss is: 0.0043876665, the jacobian loss is:0.07197058\n",
            "0.25287014\n",
            "0.9566198\n",
            "This is the iter 8201, the d1 loss is: 3065.0625, the d2 loss is: -3205.7031, the g loss is: 3247.7734, the ae loss is: 0.005591769, the jacobian loss is:0.087358296\n",
            "This is the iter 8202, the d1 loss is: 3100.6484, the d2 loss is: -3248.6719, the g loss is: 3231.086, the ae loss is: 0.006607252, the jacobian loss is:0.0799352\n",
            "This is the iter 8203, the d1 loss is: 3086.0938, the d2 loss is: -3202.3906, the g loss is: 3210.7969, the ae loss is: 0.0056312857, the jacobian loss is:0.10745987\n",
            "This is the iter 8204, the d1 loss is: 3092.7031, the d2 loss is: -3250.2422, the g loss is: 3286.8984, the ae loss is: 0.011110377, the jacobian loss is:0.07020109\n",
            "This is the iter 8205, the d1 loss is: 3055.5, the d2 loss is: -3187.8125, the g loss is: 3168.0078, the ae loss is: 0.0068574836, the jacobian loss is:0.09013627\n",
            "This is the iter 8206, the d1 loss is: 3222.1875, the d2 loss is: -3351.9219, the g loss is: 3323.9688, the ae loss is: 0.0051560597, the jacobian loss is:0.09514058\n",
            "This is the iter 8207, the d1 loss is: 3106.836, the d2 loss is: -3262.5, the g loss is: 3251.461, the ae loss is: 0.0057579232, the jacobian loss is:0.07610378\n",
            "This is the iter 8208, the d1 loss is: 3110.289, the d2 loss is: -3251.7969, the g loss is: 3241.4922, the ae loss is: 0.0057344288, the jacobian loss is:0.0945792\n",
            "This is the iter 8209, the d1 loss is: 3026.7578, the d2 loss is: -3160.3984, the g loss is: 3250.5625, the ae loss is: 0.006201397, the jacobian loss is:0.1528403\n",
            "This is the iter 8210, the d1 loss is: 3063.1719, the d2 loss is: -3208.1016, the g loss is: 3238.9766, the ae loss is: 0.01086973, the jacobian loss is:0.07398434\n",
            "This is the iter 8211, the d1 loss is: 3082.5, the d2 loss is: -3242.625, the g loss is: 3232.0547, the ae loss is: 0.006518674, the jacobian loss is:0.11264534\n",
            "This is the iter 8212, the d1 loss is: 3040.0781, the d2 loss is: -3198.7031, the g loss is: 3259.4844, the ae loss is: 0.008758262, the jacobian loss is:0.07917711\n",
            "This is the iter 8213, the d1 loss is: 2806.3828, the d2 loss is: -2964.414, the g loss is: 2977.4688, the ae loss is: 0.008760872, the jacobian loss is:0.09765256\n",
            "This is the iter 8214, the d1 loss is: 3093.3203, the d2 loss is: -3230.6406, the g loss is: 3248.9062, the ae loss is: 0.008267997, the jacobian loss is:0.08358144\n",
            "This is the iter 8215, the d1 loss is: 3051.6406, the d2 loss is: -3196.2422, the g loss is: 3164.0078, the ae loss is: 0.0060749566, the jacobian loss is:0.10438\n",
            "This is the iter 8216, the d1 loss is: 2969.836, the d2 loss is: -3133.2812, the g loss is: 3074.1016, the ae loss is: 0.010191677, the jacobian loss is:0.11060553\n",
            "This is the iter 8217, the d1 loss is: 3329.1797, the d2 loss is: -3461.7031, the g loss is: 3385.5469, the ae loss is: 0.008495811, the jacobian loss is:0.12978844\n",
            "This is the iter 8218, the d1 loss is: 3097.6953, the d2 loss is: -3234.6719, the g loss is: 3212.5469, the ae loss is: 0.0069655934, the jacobian loss is:0.09171728\n",
            "This is the iter 8219, the d1 loss is: 3291.664, the d2 loss is: -3424.2656, the g loss is: 3415.9297, the ae loss is: 0.007886003, the jacobian loss is:0.09957082\n",
            "This is the iter 8220, the d1 loss is: 3141.1094, the d2 loss is: -3267.0938, the g loss is: 3272.8984, the ae loss is: 0.013213893, the jacobian loss is:0.1653927\n",
            "This is the iter 8221, the d1 loss is: 3042.711, the d2 loss is: -3197.25, the g loss is: 3203.3281, the ae loss is: 0.0073888157, the jacobian loss is:0.11162978\n",
            "This is the iter 8222, the d1 loss is: 2990.7344, the d2 loss is: -3141.5312, the g loss is: 3168.6484, the ae loss is: 0.0072347065, the jacobian loss is:0.10219957\n",
            "This is the iter 8223, the d1 loss is: 3256.3672, the d2 loss is: -3406.7344, the g loss is: 3378.086, the ae loss is: 0.009867948, the jacobian loss is:0.14578818\n",
            "This is the iter 8224, the d1 loss is: 3062.5, the d2 loss is: -3204.9766, the g loss is: 3213.1484, the ae loss is: 0.006258094, the jacobian loss is:0.08978737\n",
            "This is the iter 8225, the d1 loss is: 3130.3125, the d2 loss is: -3242.289, the g loss is: 3176.4531, the ae loss is: 0.005731822, the jacobian loss is:0.127496\n",
            "This is the iter 8226, the d1 loss is: 3077.0547, the d2 loss is: -3222.961, the g loss is: 3259.4688, the ae loss is: 0.009822352, the jacobian loss is:0.07021771\n",
            "This is the iter 8227, the d1 loss is: 3087.7031, the d2 loss is: -3226.7344, the g loss is: 3195.2344, the ae loss is: 0.007892001, the jacobian loss is:0.0953986\n",
            "This is the iter 8228, the d1 loss is: 3203.8203, the d2 loss is: -3301.5312, the g loss is: 3201.461, the ae loss is: 0.006691334, the jacobian loss is:0.11686757\n",
            "This is the iter 8229, the d1 loss is: 3139.2969, the d2 loss is: -3259.7031, the g loss is: 3187.1797, the ae loss is: 0.009128382, the jacobian loss is:0.114065684\n",
            "This is the iter 8230, the d1 loss is: 2790.9531, the d2 loss is: -2924.1328, the g loss is: 2951.3828, the ae loss is: 0.005043475, the jacobian loss is:0.055163763\n",
            "This is the iter 8231, the d1 loss is: 3131.2422, the d2 loss is: -3265.5781, the g loss is: 3185.2188, the ae loss is: 0.00736, the jacobian loss is:0.10474066\n",
            "This is the iter 8232, the d1 loss is: 3152.2031, the d2 loss is: -3291.2656, the g loss is: 3234.7812, the ae loss is: 0.007844233, the jacobian loss is:0.08328449\n",
            "This is the iter 8233, the d1 loss is: 3084.4062, the d2 loss is: -3210.2344, the g loss is: 3223.2266, the ae loss is: 0.0056364564, the jacobian loss is:0.073258564\n",
            "This is the iter 8234, the d1 loss is: 3106.5547, the d2 loss is: -3241.0938, the g loss is: 3286.336, the ae loss is: 0.0065570483, the jacobian loss is:0.16102485\n",
            "This is the iter 8235, the d1 loss is: 3128.2422, the d2 loss is: -3263.375, the g loss is: 3303.0938, the ae loss is: 0.008730689, the jacobian loss is:0.075805455\n",
            "This is the iter 8236, the d1 loss is: 3036.2656, the d2 loss is: -3177.2422, the g loss is: 3212.5938, the ae loss is: 0.0072368197, the jacobian loss is:0.1014309\n",
            "This is the iter 8237, the d1 loss is: 3076.3672, the d2 loss is: -3198.5547, the g loss is: 3291.3906, the ae loss is: 0.0073386887, the jacobian loss is:0.08100584\n",
            "This is the iter 8238, the d1 loss is: 3237.1719, the d2 loss is: -3389.711, the g loss is: 3398.7344, the ae loss is: 0.0070383307, the jacobian loss is:0.075401075\n",
            "This is the iter 8239, the d1 loss is: 2878.8828, the d2 loss is: -3008.7734, the g loss is: 2958.0625, the ae loss is: 0.007650066, the jacobian loss is:0.13013972\n",
            "This is the iter 8240, the d1 loss is: 3181.4922, the d2 loss is: -3303.1953, the g loss is: 3288.7188, the ae loss is: 0.0073349606, the jacobian loss is:0.09140099\n",
            "This is the iter 8241, the d1 loss is: 3064.8125, the d2 loss is: -3207.289, the g loss is: 3203.5156, the ae loss is: 0.006383624, the jacobian loss is:0.06437178\n",
            "This is the iter 8242, the d1 loss is: 3076.8438, the d2 loss is: -3228.8281, the g loss is: 3193.8203, the ae loss is: 0.0061846483, the jacobian loss is:0.11917012\n",
            "This is the iter 8243, the d1 loss is: 2998.7812, the d2 loss is: -3140.1797, the g loss is: 3160.0781, the ae loss is: 0.0049582263, the jacobian loss is:0.105445474\n",
            "This is the iter 8244, the d1 loss is: 3047.2734, the d2 loss is: -3174.539, the g loss is: 3196.625, the ae loss is: 0.008065441, the jacobian loss is:0.09767609\n",
            "This is the iter 8245, the d1 loss is: 2895.375, the d2 loss is: -3042.2422, the g loss is: 3037.336, the ae loss is: 0.0069822986, the jacobian loss is:0.09035992\n",
            "This is the iter 8246, the d1 loss is: 3175.336, the d2 loss is: -3289.7031, the g loss is: 3332.6406, the ae loss is: 0.0076197064, the jacobian loss is:0.1330763\n",
            "This is the iter 8247, the d1 loss is: 3088.5625, the d2 loss is: -3232.8906, the g loss is: 3243.875, the ae loss is: 0.005754683, the jacobian loss is:0.08623848\n",
            "This is the iter 8248, the d1 loss is: 2955.9375, the d2 loss is: -3126.6328, the g loss is: 3113.1172, the ae loss is: 0.005925902, the jacobian loss is:0.110143825\n",
            "This is the iter 8249, the d1 loss is: 2923.5547, the d2 loss is: -3083.7969, the g loss is: 3057.7812, the ae loss is: 0.005521663, the jacobian loss is:0.070425294\n",
            "This is the iter 8250, the d1 loss is: 2902.4844, the d2 loss is: -3012.0781, the g loss is: 3058.5547, the ae loss is: 0.0062055616, the jacobian loss is:0.09438964\n",
            "This is the iter 8251, the d1 loss is: 3019.6797, the d2 loss is: -3182.3906, the g loss is: 3187.5469, the ae loss is: 0.0075597567, the jacobian loss is:0.085745655\n",
            "This is the iter 8252, the d1 loss is: 3090.1406, the d2 loss is: -3215.2734, the g loss is: 3204.4531, the ae loss is: 0.0069025047, the jacobian loss is:0.115091\n",
            "This is the iter 8253, the d1 loss is: 3243.5234, the d2 loss is: -3412.375, the g loss is: 3397.3906, the ae loss is: 0.0073723495, the jacobian loss is:0.103805475\n",
            "This is the iter 8254, the d1 loss is: 3036.9766, the d2 loss is: -3167.875, the g loss is: 3126.9062, the ae loss is: 0.0065828464, the jacobian loss is:0.10227486\n",
            "This is the iter 8255, the d1 loss is: 3370.7266, the d2 loss is: -3517.0078, the g loss is: 3548.711, the ae loss is: 0.006303766, the jacobian loss is:0.08585818\n",
            "This is the iter 8256, the d1 loss is: 3077.4922, the d2 loss is: -3219.0938, the g loss is: 3245.1875, the ae loss is: 0.007379818, the jacobian loss is:0.10007369\n",
            "This is the iter 8257, the d1 loss is: 3275.1797, the d2 loss is: -3408.961, the g loss is: 3403.9219, the ae loss is: 0.0059852013, the jacobian loss is:0.09975683\n",
            "This is the iter 8258, the d1 loss is: 3316.0156, the d2 loss is: -3475.8281, the g loss is: 3445.5938, the ae loss is: 0.0055662207, the jacobian loss is:0.067083515\n",
            "This is the iter 8259, the d1 loss is: 2960.375, the d2 loss is: -3092.8281, the g loss is: 3114.5156, the ae loss is: 0.0065677157, the jacobian loss is:0.123888336\n",
            "This is the iter 8260, the d1 loss is: 3254.1562, the d2 loss is: -3419.789, the g loss is: 3427.4844, the ae loss is: 0.007686113, the jacobian loss is:0.087625\n",
            "This is the iter 8261, the d1 loss is: 3023.164, the d2 loss is: -3130.8125, the g loss is: 3163.7578, the ae loss is: 0.005437498, the jacobian loss is:0.099775024\n",
            "This is the iter 8262, the d1 loss is: 3207.6953, the d2 loss is: -3335.4297, the g loss is: 3292.4844, the ae loss is: 0.0057466705, the jacobian loss is:0.062254366\n",
            "This is the iter 8263, the d1 loss is: 3093.1016, the d2 loss is: -3239.6484, the g loss is: 3252.6484, the ae loss is: 0.00654832, the jacobian loss is:0.09652937\n",
            "This is the iter 8264, the d1 loss is: 2949.5703, the d2 loss is: -3073.0078, the g loss is: 3009.7578, the ae loss is: 0.005723203, the jacobian loss is:0.07891908\n",
            "This is the iter 8265, the d1 loss is: 3376.5312, the d2 loss is: -3509.1953, the g loss is: 3481.2344, the ae loss is: 0.007477495, the jacobian loss is:0.093791716\n",
            "This is the iter 8266, the d1 loss is: 3125.8125, the d2 loss is: -3259.5625, the g loss is: 3286.7969, the ae loss is: 0.0070597967, the jacobian loss is:0.08322813\n",
            "This is the iter 8267, the d1 loss is: 3371.211, the d2 loss is: -3516.6406, the g loss is: 3513.4766, the ae loss is: 0.0063657695, the jacobian loss is:0.09940325\n",
            "This is the iter 8268, the d1 loss is: 3119.6875, the d2 loss is: -3244.1406, the g loss is: 3250.2969, the ae loss is: 0.007343823, the jacobian loss is:0.10962123\n",
            "This is the iter 8269, the d1 loss is: 3294.8906, the d2 loss is: -3421.9297, the g loss is: 3427.2188, the ae loss is: 0.007861025, the jacobian loss is:0.1480239\n",
            "This is the iter 8270, the d1 loss is: 3299.039, the d2 loss is: -3441.25, the g loss is: 3467.0781, the ae loss is: 0.006317701, the jacobian loss is:0.10562446\n",
            "This is the iter 8271, the d1 loss is: 3133.1719, the d2 loss is: -3280.8281, the g loss is: 3293.3984, the ae loss is: 0.0062861857, the jacobian loss is:0.08552637\n",
            "This is the iter 8272, the d1 loss is: 2923.6875, the d2 loss is: -3085.2266, the g loss is: 3097.211, the ae loss is: 0.00819705, the jacobian loss is:0.08610537\n",
            "This is the iter 8273, the d1 loss is: 3139.7812, the d2 loss is: -3306.5547, the g loss is: 3282.039, the ae loss is: 0.00605888, the jacobian loss is:0.0929024\n",
            "This is the iter 8274, the d1 loss is: 3177.0, the d2 loss is: -3295.3281, the g loss is: 3348.9531, the ae loss is: 0.006805279, the jacobian loss is:0.07767761\n",
            "This is the iter 8275, the d1 loss is: 3084.9844, the d2 loss is: -3208.039, the g loss is: 3254.375, the ae loss is: 0.0066506765, the jacobian loss is:0.13468194\n",
            "This is the iter 8276, the d1 loss is: 3164.1875, the d2 loss is: -3297.3438, the g loss is: 3267.9453, the ae loss is: 0.008570632, the jacobian loss is:0.068642266\n",
            "This is the iter 8277, the d1 loss is: 3111.461, the d2 loss is: -3240.8438, the g loss is: 3268.3828, the ae loss is: 0.0061689634, the jacobian loss is:0.09472011\n",
            "This is the iter 8278, the d1 loss is: 3206.0234, the d2 loss is: -3354.9375, the g loss is: 3311.836, the ae loss is: 0.006706836, the jacobian loss is:0.07829891\n",
            "This is the iter 8279, the d1 loss is: 2880.2969, the d2 loss is: -3012.0625, the g loss is: 2985.6719, the ae loss is: 0.007849271, the jacobian loss is:0.11621485\n",
            "This is the iter 8280, the d1 loss is: 3239.2578, the d2 loss is: -3384.8984, the g loss is: 3352.3906, the ae loss is: 0.006154599, the jacobian loss is:0.12919505\n",
            "This is the iter 8281, the d1 loss is: 2917.0469, the d2 loss is: -3051.8047, the g loss is: 3091.3125, the ae loss is: 0.006183685, the jacobian loss is:0.10258875\n",
            "This is the iter 8282, the d1 loss is: 3215.0469, the d2 loss is: -3326.211, the g loss is: 3324.1875, the ae loss is: 0.0079530515, the jacobian loss is:0.08128835\n",
            "This is the iter 8283, the d1 loss is: 3090.164, the d2 loss is: -3201.5547, the g loss is: 3155.3047, the ae loss is: 0.008657216, the jacobian loss is:0.12978883\n",
            "This is the iter 8284, the d1 loss is: 3044.164, the d2 loss is: -3186.164, the g loss is: 3207.2734, the ae loss is: 0.0047515994, the jacobian loss is:0.1049453\n",
            "This is the iter 8285, the d1 loss is: 2956.9688, the d2 loss is: -3085.25, the g loss is: 3127.8438, the ae loss is: 0.00520797, the jacobian loss is:0.09778131\n",
            "This is the iter 8286, the d1 loss is: 3179.5781, the d2 loss is: -3317.5781, the g loss is: 3283.1484, the ae loss is: 0.008532708, the jacobian loss is:0.13492431\n",
            "This is the iter 8287, the d1 loss is: 3004.9531, the d2 loss is: -3128.1406, the g loss is: 3183.2656, the ae loss is: 0.008074134, the jacobian loss is:0.09683643\n",
            "This is the iter 8288, the d1 loss is: 3031.4219, the d2 loss is: -3185.8516, the g loss is: 3154.6172, the ae loss is: 0.008544487, the jacobian loss is:0.10146555\n",
            "This is the iter 8289, the d1 loss is: 3115.8438, the d2 loss is: -3267.289, the g loss is: 3252.2266, the ae loss is: 0.007516416, the jacobian loss is:0.09903337\n",
            "This is the iter 8290, the d1 loss is: 3295.9766, the d2 loss is: -3430.25, the g loss is: 3367.875, the ae loss is: 0.006913104, the jacobian loss is:0.0922204\n",
            "This is the iter 8291, the d1 loss is: 3210.3438, the d2 loss is: -3346.9844, the g loss is: 3394.3516, the ae loss is: 0.007744223, the jacobian loss is:0.08526543\n",
            "This is the iter 8292, the d1 loss is: 3184.4531, the d2 loss is: -3334.5781, the g loss is: 3326.6484, the ae loss is: 0.008876127, the jacobian loss is:0.080795355\n",
            "This is the iter 8293, the d1 loss is: 3027.6328, the d2 loss is: -3171.8125, the g loss is: 3189.5625, the ae loss is: 0.0075922953, the jacobian loss is:0.13038309\n",
            "This is the iter 8294, the d1 loss is: 3065.4453, the d2 loss is: -3215.5625, the g loss is: 3208.711, the ae loss is: 0.008486694, the jacobian loss is:0.07795964\n",
            "This is the iter 8295, the d1 loss is: 3140.1406, the d2 loss is: -3249.6016, the g loss is: 3218.0547, the ae loss is: 0.009827308, the jacobian loss is:0.08736466\n",
            "This is the iter 8296, the d1 loss is: 3156.7188, the d2 loss is: -3278.7734, the g loss is: 3307.4766, the ae loss is: 0.006373996, the jacobian loss is:0.11908226\n",
            "This is the iter 8297, the d1 loss is: 3041.125, the d2 loss is: -3147.9844, the g loss is: 3181.4062, the ae loss is: 0.010200306, the jacobian loss is:0.07648923\n",
            "This is the iter 8298, the d1 loss is: 3049.1172, the d2 loss is: -3176.2266, the g loss is: 3244.6406, the ae loss is: 0.008246602, the jacobian loss is:0.10785462\n",
            "This is the iter 8299, the d1 loss is: 3141.375, the d2 loss is: -3262.586, the g loss is: 3255.7344, the ae loss is: 0.006151832, the jacobian loss is:0.09120834\n",
            "This is the iter 8300, the d1 loss is: 3079.7578, the d2 loss is: -3246.8984, the g loss is: 3232.8906, the ae loss is: 0.005896451, the jacobian loss is:0.09696923\n",
            "0.2556787\n",
            "0.967151\n",
            "This is the iter 8301, the d1 loss is: 3057.5, the d2 loss is: -3180.3125, the g loss is: 3206.4766, the ae loss is: 0.0058167404, the jacobian loss is:0.067587815\n",
            "This is the iter 8302, the d1 loss is: 3080.6875, the d2 loss is: -3204.9844, the g loss is: 3205.9922, the ae loss is: 0.0071143312, the jacobian loss is:0.103781916\n",
            "This is the iter 8303, the d1 loss is: 3081.7969, the d2 loss is: -3212.1797, the g loss is: 3239.3594, the ae loss is: 0.0057352306, the jacobian loss is:0.07156864\n",
            "This is the iter 8304, the d1 loss is: 2979.5234, the d2 loss is: -3109.5312, the g loss is: 3130.0, the ae loss is: 0.006629369, the jacobian loss is:0.08012657\n",
            "This is the iter 8305, the d1 loss is: 3133.5, the d2 loss is: -3289.9062, the g loss is: 3356.7344, the ae loss is: 0.0067804186, the jacobian loss is:0.1312983\n",
            "This is the iter 8306, the d1 loss is: 3420.7031, the d2 loss is: -3539.3047, the g loss is: 3488.8047, the ae loss is: 0.0070375428, the jacobian loss is:0.092731826\n",
            "This is the iter 8307, the d1 loss is: 3070.6797, the d2 loss is: -3197.9297, the g loss is: 3217.4844, the ae loss is: 0.0052572186, the jacobian loss is:0.093942925\n",
            "This is the iter 8308, the d1 loss is: 3271.9766, the d2 loss is: -3364.4688, the g loss is: 3342.0312, the ae loss is: 0.0061646686, the jacobian loss is:0.084131174\n",
            "This is the iter 8309, the d1 loss is: 3019.4844, the d2 loss is: -3144.211, the g loss is: 3175.9922, the ae loss is: 0.008956779, the jacobian loss is:0.09141922\n",
            "This is the iter 8310, the d1 loss is: 3112.3906, the d2 loss is: -3252.25, the g loss is: 3248.7266, the ae loss is: 0.0063610272, the jacobian loss is:0.068109415\n",
            "This is the iter 8311, the d1 loss is: 3085.625, the d2 loss is: -3223.7188, the g loss is: 3204.625, the ae loss is: 0.0043543517, the jacobian loss is:0.09884267\n",
            "This is the iter 8312, the d1 loss is: 3261.4531, the d2 loss is: -3399.086, the g loss is: 3439.6719, the ae loss is: 0.0065941093, the jacobian loss is:0.08967296\n",
            "This is the iter 8313, the d1 loss is: 3196.4688, the d2 loss is: -3347.7656, the g loss is: 3373.5625, the ae loss is: 0.008255503, the jacobian loss is:0.097390614\n",
            "This is the iter 8314, the d1 loss is: 3089.6562, the d2 loss is: -3201.3438, the g loss is: 3218.7812, the ae loss is: 0.0067353365, the jacobian loss is:0.07280928\n",
            "This is the iter 8315, the d1 loss is: 3060.4453, the d2 loss is: -3196.6562, the g loss is: 3217.5938, the ae loss is: 0.0068537057, the jacobian loss is:0.079926014\n",
            "This is the iter 8316, the d1 loss is: 3207.0312, the d2 loss is: -3336.1406, the g loss is: 3352.4688, the ae loss is: 0.0069322865, the jacobian loss is:0.099627465\n",
            "This is the iter 8317, the d1 loss is: 3076.6875, the d2 loss is: -3200.6406, the g loss is: 3273.9844, the ae loss is: 0.007405095, the jacobian loss is:0.11563102\n",
            "This is the iter 8318, the d1 loss is: 3176.0547, the d2 loss is: -3318.5078, the g loss is: 3335.7734, the ae loss is: 0.0076960768, the jacobian loss is:0.09762468\n",
            "This is the iter 8319, the d1 loss is: 3079.4375, the d2 loss is: -3213.5547, the g loss is: 3165.1484, the ae loss is: 0.006749539, the jacobian loss is:0.09986808\n",
            "This is the iter 8320, the d1 loss is: 3146.8438, the d2 loss is: -3278.8125, the g loss is: 3229.5078, the ae loss is: 0.008616834, the jacobian loss is:0.096235484\n",
            "This is the iter 8321, the d1 loss is: 3308.3203, the d2 loss is: -3418.875, the g loss is: 3373.461, the ae loss is: 0.0046107215, the jacobian loss is:0.10030525\n",
            "This is the iter 8322, the d1 loss is: 3148.7266, the d2 loss is: -3303.3516, the g loss is: 3306.6875, the ae loss is: 0.008182213, the jacobian loss is:0.099060625\n",
            "This is the iter 8323, the d1 loss is: 3142.5469, the d2 loss is: -3272.25, the g loss is: 3260.5469, the ae loss is: 0.0052477648, the jacobian loss is:0.094816014\n",
            "This is the iter 8324, the d1 loss is: 3160.7656, the d2 loss is: -3292.0156, the g loss is: 3248.9531, the ae loss is: 0.0076755225, the jacobian loss is:0.121387355\n",
            "This is the iter 8325, the d1 loss is: 3040.4453, the d2 loss is: -3169.3047, the g loss is: 3187.0234, the ae loss is: 0.008420058, the jacobian loss is:0.13472335\n",
            "This is the iter 8326, the d1 loss is: 3165.836, the d2 loss is: -3284.4297, the g loss is: 3275.9453, the ae loss is: 0.005566918, the jacobian loss is:0.06792071\n",
            "This is the iter 8327, the d1 loss is: 3047.1797, the d2 loss is: -3187.3203, the g loss is: 3164.2031, the ae loss is: 0.008766826, the jacobian loss is:0.21080883\n",
            "This is the iter 8328, the d1 loss is: 3170.4844, the d2 loss is: -3284.625, the g loss is: 3272.2344, the ae loss is: 0.006339527, the jacobian loss is:0.074901775\n",
            "This is the iter 8329, the d1 loss is: 3141.5625, the d2 loss is: -3240.7578, the g loss is: 3218.4062, the ae loss is: 0.0061307033, the jacobian loss is:0.10159992\n",
            "This is the iter 8330, the d1 loss is: 3121.5781, the d2 loss is: -3289.5078, the g loss is: 3328.2188, the ae loss is: 0.008760527, the jacobian loss is:0.07518167\n",
            "This is the iter 8331, the d1 loss is: 3086.4219, the d2 loss is: -3200.75, the g loss is: 3288.8828, the ae loss is: 0.0042268527, the jacobian loss is:0.09832067\n",
            "This is the iter 8332, the d1 loss is: 3146.1094, the d2 loss is: -3293.5078, the g loss is: 3269.9844, the ae loss is: 0.005757084, the jacobian loss is:0.06808006\n",
            "This is the iter 8333, the d1 loss is: 3139.8906, the d2 loss is: -3287.2422, the g loss is: 3295.8125, the ae loss is: 0.0071111443, the jacobian loss is:0.094287574\n",
            "This is the iter 8334, the d1 loss is: 3287.7422, the d2 loss is: -3438.6875, the g loss is: 3428.5938, the ae loss is: 0.007380161, the jacobian loss is:0.09805555\n",
            "This is the iter 8335, the d1 loss is: 2851.7188, the d2 loss is: -2986.2188, the g loss is: 2980.664, the ae loss is: 0.0072784666, the jacobian loss is:0.10149128\n",
            "This is the iter 8336, the d1 loss is: 3359.8203, the d2 loss is: -3488.7188, the g loss is: 3516.4844, the ae loss is: 0.0062839612, the jacobian loss is:0.08147476\n",
            "This is the iter 8337, the d1 loss is: 2938.8594, the d2 loss is: -3065.0469, the g loss is: 2998.7578, the ae loss is: 0.0064110775, the jacobian loss is:0.08741161\n",
            "This is the iter 8338, the d1 loss is: 3206.9688, the d2 loss is: -3371.711, the g loss is: 3351.539, the ae loss is: 0.005513482, the jacobian loss is:0.116906576\n",
            "This is the iter 8339, the d1 loss is: 3064.2031, the d2 loss is: -3195.4062, the g loss is: 3216.3828, the ae loss is: 0.0068687377, the jacobian loss is:0.10816673\n",
            "This is the iter 8340, the d1 loss is: 3172.414, the d2 loss is: -3314.5078, the g loss is: 3318.2422, the ae loss is: 0.008455962, the jacobian loss is:0.09278694\n",
            "This is the iter 8341, the d1 loss is: 3191.9844, the d2 loss is: -3326.4219, the g loss is: 3334.3438, the ae loss is: 0.0055289343, the jacobian loss is:0.080983646\n",
            "This is the iter 8342, the d1 loss is: 2915.8203, the d2 loss is: -3023.2578, the g loss is: 3013.7031, the ae loss is: 0.0046697, the jacobian loss is:0.084900744\n",
            "This is the iter 8343, the d1 loss is: 3054.6406, the d2 loss is: -3218.4531, the g loss is: 3201.8516, the ae loss is: 0.0059029823, the jacobian loss is:0.08329434\n",
            "This is the iter 8344, the d1 loss is: 3113.5234, the d2 loss is: -3225.1797, the g loss is: 3252.0, the ae loss is: 0.0067666383, the jacobian loss is:0.12456899\n",
            "This is the iter 8345, the d1 loss is: 3005.039, the d2 loss is: -3140.5, the g loss is: 3177.9062, the ae loss is: 0.0061839866, the jacobian loss is:0.072816856\n",
            "This is the iter 8346, the d1 loss is: 3289.5078, the d2 loss is: -3438.3828, the g loss is: 3413.0, the ae loss is: 0.0058060368, the jacobian loss is:0.10998304\n",
            "This is the iter 8347, the d1 loss is: 3374.3828, the d2 loss is: -3506.6797, the g loss is: 3492.5703, the ae loss is: 0.006458954, the jacobian loss is:0.06867693\n",
            "This is the iter 8348, the d1 loss is: 3097.625, the d2 loss is: -3248.6797, the g loss is: 3279.8594, the ae loss is: 0.005613478, the jacobian loss is:0.083400875\n",
            "This is the iter 8349, the d1 loss is: 3243.0078, the d2 loss is: -3367.5234, the g loss is: 3414.0625, the ae loss is: 0.00853227, the jacobian loss is:0.06935197\n",
            "This is the iter 8350, the d1 loss is: 3182.836, the d2 loss is: -3330.875, the g loss is: 3289.6406, the ae loss is: 0.007044233, the jacobian loss is:0.09922059\n",
            "This is the iter 8351, the d1 loss is: 3080.1484, the d2 loss is: -3220.8672, the g loss is: 3306.875, the ae loss is: 0.0066866893, the jacobian loss is:0.083805926\n",
            "This is the iter 8352, the d1 loss is: 3148.0078, the d2 loss is: -3269.1797, the g loss is: 3322.6875, the ae loss is: 0.0053939032, the jacobian loss is:0.079343304\n",
            "This is the iter 8353, the d1 loss is: 3160.3281, the d2 loss is: -3315.414, the g loss is: 3303.4766, the ae loss is: 0.0070702164, the jacobian loss is:0.11010783\n",
            "This is the iter 8354, the d1 loss is: 3144.2188, the d2 loss is: -3290.3047, the g loss is: 3309.4688, the ae loss is: 0.0063236337, the jacobian loss is:0.070078544\n",
            "This is the iter 8355, the d1 loss is: 3085.1094, the d2 loss is: -3210.0469, the g loss is: 3218.039, the ae loss is: 0.003782732, the jacobian loss is:0.080764376\n",
            "This is the iter 8356, the d1 loss is: 3302.2734, the d2 loss is: -3443.75, the g loss is: 3467.2031, the ae loss is: 0.004327579, the jacobian loss is:0.09648211\n",
            "This is the iter 8357, the d1 loss is: 3305.7266, the d2 loss is: -3421.6484, the g loss is: 3429.0156, the ae loss is: 0.0054834415, the jacobian loss is:0.08230366\n",
            "This is the iter 8358, the d1 loss is: 3141.8594, the d2 loss is: -3271.1562, the g loss is: 3329.2344, the ae loss is: 0.0052113137, the jacobian loss is:0.06300017\n",
            "This is the iter 8359, the d1 loss is: 3051.3594, the d2 loss is: -3196.0469, the g loss is: 3108.9844, the ae loss is: 0.0040098736, the jacobian loss is:0.08934386\n",
            "This is the iter 8360, the d1 loss is: 3169.1094, the d2 loss is: -3309.711, the g loss is: 3258.3047, the ae loss is: 0.0065854983, the jacobian loss is:0.0718116\n",
            "This is the iter 8361, the d1 loss is: 3060.3906, the d2 loss is: -3184.5703, the g loss is: 3125.1953, the ae loss is: 0.0054835565, the jacobian loss is:0.10966923\n",
            "This is the iter 8362, the d1 loss is: 2987.0, the d2 loss is: -3124.336, the g loss is: 3097.711, the ae loss is: 0.0056714755, the jacobian loss is:0.06857874\n",
            "This is the iter 8363, the d1 loss is: 3349.3906, the d2 loss is: -3486.0469, the g loss is: 3448.8438, the ae loss is: 0.010090651, the jacobian loss is:0.10179602\n",
            "This is the iter 8364, the d1 loss is: 3095.625, the d2 loss is: -3230.3125, the g loss is: 3238.4219, the ae loss is: 0.0062422073, the jacobian loss is:0.09289544\n",
            "This is the iter 8365, the d1 loss is: 3092.8281, the d2 loss is: -3236.6094, the g loss is: 3249.3828, the ae loss is: 0.012876939, the jacobian loss is:0.0976879\n",
            "This is the iter 8366, the d1 loss is: 3158.7734, the d2 loss is: -3295.0078, the g loss is: 3294.1016, the ae loss is: 0.008740675, the jacobian loss is:0.09203507\n",
            "This is the iter 8367, the d1 loss is: 3152.5938, the d2 loss is: -3245.039, the g loss is: 3284.211, the ae loss is: 0.009990414, the jacobian loss is:0.10419933\n",
            "This is the iter 8368, the d1 loss is: 2933.7188, the d2 loss is: -3065.1406, the g loss is: 3034.9453, the ae loss is: 0.010005976, the jacobian loss is:0.1039683\n",
            "This is the iter 8369, the d1 loss is: 3036.8906, the d2 loss is: -3164.6406, the g loss is: 3207.25, the ae loss is: 0.01027791, the jacobian loss is:0.103087135\n",
            "This is the iter 8370, the d1 loss is: 3343.9219, the d2 loss is: -3487.2344, the g loss is: 3448.6719, the ae loss is: 0.004920957, the jacobian loss is:0.08057334\n",
            "This is the iter 8371, the d1 loss is: 3115.3125, the d2 loss is: -3230.1484, the g loss is: 3236.3516, the ae loss is: 0.00806312, the jacobian loss is:0.08175242\n",
            "This is the iter 8372, the d1 loss is: 3135.4375, the d2 loss is: -3279.0312, the g loss is: 3260.3125, the ae loss is: 0.008470226, the jacobian loss is:0.081839316\n",
            "This is the iter 8373, the d1 loss is: 3042.6172, the d2 loss is: -3122.5625, the g loss is: 3312.1953, the ae loss is: 0.009940392, the jacobian loss is:0.15065174\n",
            "This is the iter 8374, the d1 loss is: 3084.875, the d2 loss is: -3228.7188, the g loss is: 3246.7578, the ae loss is: 0.006424821, the jacobian loss is:0.060990956\n",
            "This is the iter 8375, the d1 loss is: 3161.336, the d2 loss is: -3296.3516, the g loss is: 3280.5469, the ae loss is: 0.005077701, the jacobian loss is:0.1053337\n",
            "This is the iter 8376, the d1 loss is: 3162.9844, the d2 loss is: -3272.0938, the g loss is: 3310.8516, the ae loss is: 0.008318421, the jacobian loss is:0.08054119\n",
            "This is the iter 8377, the d1 loss is: 2829.5703, the d2 loss is: -2992.4062, the g loss is: 3009.9844, the ae loss is: 0.0055342885, the jacobian loss is:0.07694471\n",
            "This is the iter 8378, the d1 loss is: 3222.1719, the d2 loss is: -3377.4922, the g loss is: 3357.9375, the ae loss is: 0.0067895474, the jacobian loss is:0.10448815\n",
            "This is the iter 8379, the d1 loss is: 3085.039, the d2 loss is: -3196.2734, the g loss is: 3193.7812, the ae loss is: 0.0061523113, the jacobian loss is:0.09453728\n",
            "This is the iter 8380, the d1 loss is: 3094.7188, the d2 loss is: -3243.2266, the g loss is: 3257.0938, the ae loss is: 0.007175515, the jacobian loss is:0.08457698\n",
            "This is the iter 8381, the d1 loss is: 2860.1016, the d2 loss is: -3009.6562, the g loss is: 2954.5547, the ae loss is: 0.008478971, the jacobian loss is:0.100255385\n",
            "This is the iter 8382, the d1 loss is: 3366.7188, the d2 loss is: -3519.6172, the g loss is: 3511.4531, the ae loss is: 0.007492841, the jacobian loss is:0.07371655\n",
            "This is the iter 8383, the d1 loss is: 3042.9531, the d2 loss is: -3157.3828, the g loss is: 3127.711, the ae loss is: 0.0068282373, the jacobian loss is:0.070772864\n",
            "This is the iter 8384, the d1 loss is: 2843.9766, the d2 loss is: -2976.414, the g loss is: 2967.5703, the ae loss is: 0.00492308, the jacobian loss is:0.09802484\n",
            "This is the iter 8385, the d1 loss is: 2991.664, the d2 loss is: -3119.7188, the g loss is: 3182.8984, the ae loss is: 0.006244995, the jacobian loss is:0.079317436\n",
            "This is the iter 8386, the d1 loss is: 3076.586, the d2 loss is: -3221.0547, the g loss is: 3234.3984, the ae loss is: 0.006039308, the jacobian loss is:0.082633585\n",
            "This is the iter 8387, the d1 loss is: 3077.836, the d2 loss is: -3230.9688, the g loss is: 3188.1016, the ae loss is: 0.008828733, the jacobian loss is:0.105558895\n",
            "This is the iter 8388, the d1 loss is: 3158.961, the d2 loss is: -3285.7188, the g loss is: 3287.3281, the ae loss is: 0.006207829, the jacobian loss is:0.08579513\n",
            "This is the iter 8389, the d1 loss is: 3052.4062, the d2 loss is: -3180.9531, the g loss is: 3203.0781, the ae loss is: 0.009213824, the jacobian loss is:0.09668866\n",
            "This is the iter 8390, the d1 loss is: 3349.1172, the d2 loss is: -3484.0312, the g loss is: 3466.4766, the ae loss is: 0.00443707, the jacobian loss is:0.11614889\n",
            "This is the iter 8391, the d1 loss is: 2964.8203, the d2 loss is: -3098.336, the g loss is: 3077.7344, the ae loss is: 0.009229725, the jacobian loss is:0.08544638\n",
            "This is the iter 8392, the d1 loss is: 3206.5469, the d2 loss is: -3332.6562, the g loss is: 3405.6016, the ae loss is: 0.0045270743, the jacobian loss is:0.08990168\n",
            "This is the iter 8393, the d1 loss is: 2827.5469, the d2 loss is: -2969.5, the g loss is: 2972.2656, the ae loss is: 0.00785104, the jacobian loss is:0.10877446\n",
            "This is the iter 8394, the d1 loss is: 3311.5469, the d2 loss is: -3447.2188, the g loss is: 3467.625, the ae loss is: 0.005021507, the jacobian loss is:0.09061504\n",
            "This is the iter 8395, the d1 loss is: 2966.3594, the d2 loss is: -3103.1016, the g loss is: 3112.3281, the ae loss is: 0.005721435, the jacobian loss is:0.10386583\n",
            "This is the iter 8396, the d1 loss is: 2813.3672, the d2 loss is: -2917.7344, the g loss is: 2956.5, the ae loss is: 0.006720761, the jacobian loss is:0.06966667\n",
            "This is the iter 8397, the d1 loss is: 2974.1797, the d2 loss is: -3120.4062, the g loss is: 3142.625, the ae loss is: 0.0044576633, the jacobian loss is:0.11299261\n",
            "This is the iter 8398, the d1 loss is: 3083.8047, the d2 loss is: -3197.1406, the g loss is: 3233.2031, the ae loss is: 0.005776077, the jacobian loss is:0.08775406\n",
            "This is the iter 8399, the d1 loss is: 3185.6094, the d2 loss is: -3289.0703, the g loss is: 3293.5781, the ae loss is: 0.0108103035, the jacobian loss is:0.09133688\n",
            "This is the iter 8400, the d1 loss is: 3122.4062, the d2 loss is: -3260.6875, the g loss is: 3275.6406, the ae loss is: 0.007034687, the jacobian loss is:0.095806226\n",
            "0.25868136\n",
            "0.98284894\n",
            "This is the iter 8401, the d1 loss is: 3131.1406, the d2 loss is: -3257.4844, the g loss is: 3352.8594, the ae loss is: 0.0066756634, the jacobian loss is:0.101399116\n",
            "This is the iter 8402, the d1 loss is: 3062.4062, the d2 loss is: -3215.8125, the g loss is: 3209.1797, the ae loss is: 0.0063090352, the jacobian loss is:0.093294516\n",
            "This is the iter 8403, the d1 loss is: 3084.9766, the d2 loss is: -3242.25, the g loss is: 3208.1719, the ae loss is: 0.0054448307, the jacobian loss is:0.09789\n",
            "This is the iter 8404, the d1 loss is: 3129.461, the d2 loss is: -3279.0469, the g loss is: 3237.7578, the ae loss is: 0.008087997, the jacobian loss is:0.09317239\n",
            "This is the iter 8405, the d1 loss is: 3028.9688, the d2 loss is: -3167.5234, the g loss is: 3156.1094, the ae loss is: 0.009034021, the jacobian loss is:0.11505385\n",
            "This is the iter 8406, the d1 loss is: 3194.4766, the d2 loss is: -3343.4375, the g loss is: 3332.5625, the ae loss is: 0.007256833, the jacobian loss is:0.08235064\n",
            "This is the iter 8407, the d1 loss is: 2841.4844, the d2 loss is: -2948.4922, the g loss is: 2956.9297, the ae loss is: 0.0057576043, the jacobian loss is:0.07812911\n",
            "This is the iter 8408, the d1 loss is: 3105.6797, the d2 loss is: -3248.0938, the g loss is: 3282.6719, the ae loss is: 0.0057094726, the jacobian loss is:0.06537666\n",
            "This is the iter 8409, the d1 loss is: 2952.625, the d2 loss is: -3106.961, the g loss is: 3085.8203, the ae loss is: 0.008812274, the jacobian loss is:0.08137585\n",
            "This is the iter 8410, the d1 loss is: 3139.6016, the d2 loss is: -3268.7969, the g loss is: 3241.7266, the ae loss is: 0.008388378, the jacobian loss is:0.09914729\n",
            "This is the iter 8411, the d1 loss is: 3058.1172, the d2 loss is: -3183.7812, the g loss is: 3165.6016, the ae loss is: 0.0060721524, the jacobian loss is:0.08789357\n",
            "This is the iter 8412, the d1 loss is: 2984.7031, the d2 loss is: -3122.5312, the g loss is: 3165.6719, the ae loss is: 0.0060089906, the jacobian loss is:0.10985014\n",
            "This is the iter 8413, the d1 loss is: 3008.1875, the d2 loss is: -3137.0625, the g loss is: 3197.7344, the ae loss is: 0.005651019, the jacobian loss is:0.10333543\n",
            "This is the iter 8414, the d1 loss is: 3138.9922, the d2 loss is: -3260.0781, the g loss is: 3296.3516, the ae loss is: 0.006590355, the jacobian loss is:0.06589598\n",
            "This is the iter 8415, the d1 loss is: 2974.5156, the d2 loss is: -3103.5625, the g loss is: 3130.6719, the ae loss is: 0.008739492, the jacobian loss is:0.14224362\n",
            "This is the iter 8416, the d1 loss is: 3105.0078, the d2 loss is: -3207.6562, the g loss is: 3159.711, the ae loss is: 0.007951337, the jacobian loss is:0.098501\n",
            "This is the iter 8417, the d1 loss is: 3061.8281, the d2 loss is: -3192.6484, the g loss is: 3135.3203, the ae loss is: 0.005094188, the jacobian loss is:0.08820513\n",
            "This is the iter 8418, the d1 loss is: 3042.2969, the d2 loss is: -3154.6953, the g loss is: 3189.125, the ae loss is: 0.004410756, the jacobian loss is:0.1422968\n",
            "This is the iter 8419, the d1 loss is: 2981.3203, the d2 loss is: -3104.8203, the g loss is: 3196.0, the ae loss is: 0.0075853732, the jacobian loss is:0.08760502\n",
            "This is the iter 8420, the d1 loss is: 3084.5, the d2 loss is: -3218.6562, the g loss is: 3250.789, the ae loss is: 0.0048659835, the jacobian loss is:0.080218025\n",
            "This is the iter 8421, the d1 loss is: 3248.9453, the d2 loss is: -3378.2031, the g loss is: 3376.3281, the ae loss is: 0.009335725, the jacobian loss is:0.12702297\n",
            "This is the iter 8422, the d1 loss is: 3306.0547, the d2 loss is: -3426.0078, the g loss is: 3440.336, the ae loss is: 0.008769755, the jacobian loss is:0.06904055\n",
            "This is the iter 8423, the d1 loss is: 2786.2734, the d2 loss is: -2916.1094, the g loss is: 2950.961, the ae loss is: 0.0057863593, the jacobian loss is:0.07874811\n",
            "This is the iter 8424, the d1 loss is: 3077.0781, the d2 loss is: -3207.4375, the g loss is: 3211.586, the ae loss is: 0.007811795, the jacobian loss is:0.074395776\n",
            "This is the iter 8425, the d1 loss is: 3039.0703, the d2 loss is: -3168.5781, the g loss is: 3183.4766, the ae loss is: 0.007657321, the jacobian loss is:0.107075125\n",
            "This is the iter 8426, the d1 loss is: 3206.9844, the d2 loss is: -3348.4766, the g loss is: 3356.711, the ae loss is: 0.0075939763, the jacobian loss is:0.10331098\n",
            "This is the iter 8427, the d1 loss is: 3171.9531, the d2 loss is: -3310.5156, the g loss is: 3320.2344, the ae loss is: 0.0073538977, the jacobian loss is:0.10071284\n",
            "This is the iter 8428, the d1 loss is: 3200.6016, the d2 loss is: -3353.8906, the g loss is: 3288.3125, the ae loss is: 0.007136819, the jacobian loss is:0.16006118\n",
            "This is the iter 8429, the d1 loss is: 3029.1719, the d2 loss is: -3159.7031, the g loss is: 3168.0156, the ae loss is: 0.0057471804, the jacobian loss is:0.113322094\n",
            "This is the iter 8430, the d1 loss is: 3061.039, the d2 loss is: -3212.0625, the g loss is: 3195.1953, the ae loss is: 0.0071537783, the jacobian loss is:0.13289815\n",
            "This is the iter 8431, the d1 loss is: 3094.4062, the d2 loss is: -3225.6094, the g loss is: 3198.0625, the ae loss is: 0.006069997, the jacobian loss is:0.08830869\n",
            "This is the iter 8432, the d1 loss is: 3123.6953, the d2 loss is: -3254.8672, the g loss is: 3266.3672, the ae loss is: 0.0070497557, the jacobian loss is:0.0926951\n",
            "This is the iter 8433, the d1 loss is: 2978.9062, the d2 loss is: -3115.8516, the g loss is: 3103.4062, the ae loss is: 0.0070641534, the jacobian loss is:0.10070396\n",
            "This is the iter 8434, the d1 loss is: 3089.4688, the d2 loss is: -3211.4062, the g loss is: 3188.6875, the ae loss is: 0.0060639908, the jacobian loss is:0.12358973\n",
            "This is the iter 8435, the d1 loss is: 3082.586, the d2 loss is: -3223.7969, the g loss is: 3160.336, the ae loss is: 0.0052095065, the jacobian loss is:0.09134543\n",
            "This is the iter 8436, the d1 loss is: 2970.1406, the d2 loss is: -3103.9453, the g loss is: 3043.9062, the ae loss is: 0.008988921, the jacobian loss is:0.15014607\n",
            "This is the iter 8437, the d1 loss is: 2870.2188, the d2 loss is: -3019.836, the g loss is: 2978.0938, the ae loss is: 0.007327572, the jacobian loss is:0.06393236\n",
            "This is the iter 8438, the d1 loss is: 3013.086, the d2 loss is: -3121.4453, the g loss is: 3125.8828, the ae loss is: 0.005364914, the jacobian loss is:0.07550561\n",
            "This is the iter 8439, the d1 loss is: 3164.75, the d2 loss is: -3287.7422, the g loss is: 3230.9219, the ae loss is: 0.006335956, the jacobian loss is:0.10304917\n",
            "This is the iter 8440, the d1 loss is: 2984.6719, the d2 loss is: -3109.25, the g loss is: 3133.4688, the ae loss is: 0.0075373566, the jacobian loss is:0.10362268\n",
            "This is the iter 8441, the d1 loss is: 3160.2344, the d2 loss is: -3289.1797, the g loss is: 3355.461, the ae loss is: 0.0043366943, the jacobian loss is:0.09893366\n",
            "This is the iter 8442, the d1 loss is: 3052.9219, the d2 loss is: -3176.7656, the g loss is: 3134.4531, the ae loss is: 0.00388326, the jacobian loss is:0.123467185\n",
            "This is the iter 8443, the d1 loss is: 3082.6328, the d2 loss is: -3213.5703, the g loss is: 3186.7422, the ae loss is: 0.0062041814, the jacobian loss is:0.09684365\n",
            "This is the iter 8444, the d1 loss is: 3030.8125, the d2 loss is: -3153.5078, the g loss is: 3182.0938, the ae loss is: 0.006412804, the jacobian loss is:0.0926638\n",
            "This is the iter 8445, the d1 loss is: 2956.875, the d2 loss is: -3102.8516, the g loss is: 3145.875, the ae loss is: 0.006537233, the jacobian loss is:0.06841437\n",
            "This is the iter 8446, the d1 loss is: 2932.9219, the d2 loss is: -3040.4453, the g loss is: 3014.7031, the ae loss is: 0.007105883, the jacobian loss is:0.08382451\n",
            "This is the iter 8447, the d1 loss is: 2946.8281, the d2 loss is: -3098.1875, the g loss is: 3113.2422, the ae loss is: 0.008436309, the jacobian loss is:0.08172866\n",
            "This is the iter 8448, the d1 loss is: 3078.75, the d2 loss is: -3188.7656, the g loss is: 3214.2812, the ae loss is: 0.0070863264, the jacobian loss is:0.08098339\n",
            "This is the iter 8449, the d1 loss is: 3042.2578, the d2 loss is: -3165.5625, the g loss is: 3157.3047, the ae loss is: 0.006373294, the jacobian loss is:0.11313305\n",
            "This is the iter 8450, the d1 loss is: 2983.2656, the d2 loss is: -3091.961, the g loss is: 3155.7344, the ae loss is: 0.006203479, the jacobian loss is:0.12850443\n",
            "This is the iter 8451, the d1 loss is: 2888.1875, the d2 loss is: -3002.9297, the g loss is: 2993.4922, the ae loss is: 0.007530707, the jacobian loss is:0.07460597\n",
            "This is the iter 8452, the d1 loss is: 3187.3203, the d2 loss is: -3315.461, the g loss is: 3346.5078, the ae loss is: 0.006483482, the jacobian loss is:0.07866588\n",
            "This is the iter 8453, the d1 loss is: 3251.711, the d2 loss is: -3377.6172, the g loss is: 3355.586, the ae loss is: 0.006631057, the jacobian loss is:0.100238904\n",
            "This is the iter 8454, the d1 loss is: 3074.0078, the d2 loss is: -3201.7734, the g loss is: 3212.1562, the ae loss is: 0.006905534, the jacobian loss is:0.099011324\n",
            "This is the iter 8455, the d1 loss is: 2842.2344, the d2 loss is: -2980.4062, the g loss is: 2972.8906, the ae loss is: 0.007335299, the jacobian loss is:0.09336368\n",
            "This is the iter 8456, the d1 loss is: 3125.6875, the d2 loss is: -3213.2266, the g loss is: 3236.1562, the ae loss is: 0.005536481, the jacobian loss is:0.116763584\n",
            "This is the iter 8457, the d1 loss is: 2989.8125, the d2 loss is: -3133.8984, the g loss is: 3099.125, the ae loss is: 0.0050048786, the jacobian loss is:0.11647409\n",
            "This is the iter 8458, the d1 loss is: 3073.0, the d2 loss is: -3186.1484, the g loss is: 3211.5781, the ae loss is: 0.006233207, the jacobian loss is:0.10555846\n",
            "This is the iter 8459, the d1 loss is: 3021.5156, the d2 loss is: -3153.0625, the g loss is: 3114.5781, the ae loss is: 0.0066330307, the jacobian loss is:0.15324667\n",
            "This is the iter 8460, the d1 loss is: 3145.625, the d2 loss is: -3247.3828, the g loss is: 3235.789, the ae loss is: 0.005510486, the jacobian loss is:0.15075734\n",
            "This is the iter 8461, the d1 loss is: 3103.25, the d2 loss is: -3239.5938, the g loss is: 3209.6953, the ae loss is: 0.0069392794, the jacobian loss is:0.0847999\n",
            "This is the iter 8462, the d1 loss is: 3093.164, the d2 loss is: -3212.8516, the g loss is: 3204.9375, the ae loss is: 0.0074980245, the jacobian loss is:0.1254676\n",
            "This is the iter 8463, the d1 loss is: 2922.9219, the d2 loss is: -3061.8594, the g loss is: 3061.5078, the ae loss is: 0.008521393, the jacobian loss is:0.13088325\n",
            "This is the iter 8464, the d1 loss is: 3347.2578, the d2 loss is: -3473.9297, the g loss is: 3468.1328, the ae loss is: 0.007992902, the jacobian loss is:0.10004384\n",
            "This is the iter 8465, the d1 loss is: 3022.164, the d2 loss is: -3140.7734, the g loss is: 3111.6953, the ae loss is: 0.0070473594, the jacobian loss is:0.13473223\n",
            "This is the iter 8466, the d1 loss is: 3225.6562, the d2 loss is: -3353.1484, the g loss is: 3330.5938, the ae loss is: 0.00782464, the jacobian loss is:0.09268177\n",
            "This is the iter 8467, the d1 loss is: 3070.9062, the d2 loss is: -3189.1406, the g loss is: 3192.1719, the ae loss is: 0.004607429, the jacobian loss is:0.10531022\n",
            "This is the iter 8468, the d1 loss is: 3133.8203, the d2 loss is: -3242.1328, the g loss is: 3210.164, the ae loss is: 0.0049861176, the jacobian loss is:0.08945786\n",
            "This is the iter 8469, the d1 loss is: 3091.3438, the d2 loss is: -3192.1484, the g loss is: 3211.3125, the ae loss is: 0.0067229746, the jacobian loss is:0.08995454\n",
            "This is the iter 8470, the d1 loss is: 3145.375, the d2 loss is: -3259.1719, the g loss is: 3247.6406, the ae loss is: 0.009292538, the jacobian loss is:0.08239215\n",
            "This is the iter 8471, the d1 loss is: 3065.625, the d2 loss is: -3218.0156, the g loss is: 3182.8438, the ae loss is: 0.008095055, the jacobian loss is:0.111434035\n",
            "This is the iter 8472, the d1 loss is: 2938.3672, the d2 loss is: -3073.1797, the g loss is: 3126.4219, the ae loss is: 0.0071335863, the jacobian loss is:0.07233365\n",
            "This is the iter 8473, the d1 loss is: 2987.4766, the d2 loss is: -3103.625, the g loss is: 3132.5625, the ae loss is: 0.007250513, the jacobian loss is:0.12200917\n",
            "This is the iter 8474, the d1 loss is: 3067.6953, the d2 loss is: -3195.3438, the g loss is: 3152.4219, the ae loss is: 0.007898362, the jacobian loss is:0.113857284\n",
            "This is the iter 8475, the d1 loss is: 3046.211, the d2 loss is: -3166.2266, the g loss is: 3197.961, the ae loss is: 0.006960004, the jacobian loss is:0.10585615\n",
            "This is the iter 8476, the d1 loss is: 3059.4062, the d2 loss is: -3210.2734, the g loss is: 3202.2734, the ae loss is: 0.006463415, the jacobian loss is:0.0879714\n",
            "This is the iter 8477, the d1 loss is: 3241.0703, the d2 loss is: -3376.5781, the g loss is: 3347.9375, the ae loss is: 0.00906511, the jacobian loss is:0.13420221\n",
            "This is the iter 8478, the d1 loss is: 3213.1797, the d2 loss is: -3344.7812, the g loss is: 3399.8672, the ae loss is: 0.005871417, the jacobian loss is:0.07530136\n",
            "This is the iter 8479, the d1 loss is: 3011.7734, the d2 loss is: -3118.3984, the g loss is: 3091.9062, the ae loss is: 0.010177996, the jacobian loss is:0.08319942\n",
            "This is the iter 8480, the d1 loss is: 3156.2266, the d2 loss is: -3259.0938, the g loss is: 3264.9922, the ae loss is: 0.0064620753, the jacobian loss is:0.093239784\n",
            "This is the iter 8481, the d1 loss is: 2994.1094, the d2 loss is: -3133.2031, the g loss is: 3147.6406, the ae loss is: 0.009893767, the jacobian loss is:0.17372207\n",
            "This is the iter 8482, the d1 loss is: 3063.461, the d2 loss is: -3164.7266, the g loss is: 3184.3906, the ae loss is: 0.00509697, the jacobian loss is:0.07972205\n",
            "This is the iter 8483, the d1 loss is: 3087.1172, the d2 loss is: -3237.3594, the g loss is: 3257.8203, the ae loss is: 0.0062351553, the jacobian loss is:0.11778377\n",
            "This is the iter 8484, the d1 loss is: 3051.8984, the d2 loss is: -3164.6484, the g loss is: 3212.9297, the ae loss is: 0.00684801, the jacobian loss is:0.0757059\n",
            "This is the iter 8485, the d1 loss is: 3142.7578, the d2 loss is: -3258.0078, the g loss is: 3215.1328, the ae loss is: 0.0062865773, the jacobian loss is:0.11155854\n",
            "This is the iter 8486, the d1 loss is: 3198.3672, the d2 loss is: -3291.7344, the g loss is: 3301.8906, the ae loss is: 0.0068925368, the jacobian loss is:0.14136198\n",
            "This is the iter 8487, the d1 loss is: 2858.5234, the d2 loss is: -2979.0703, the g loss is: 2971.1094, the ae loss is: 0.007473589, the jacobian loss is:0.09908315\n",
            "This is the iter 8488, the d1 loss is: 3181.4766, the d2 loss is: -3313.289, the g loss is: 3254.7969, the ae loss is: 0.0073917666, the jacobian loss is:0.102622226\n",
            "This is the iter 8489, the d1 loss is: 3027.3203, the d2 loss is: -3137.3906, the g loss is: 3147.3438, the ae loss is: 0.010187613, the jacobian loss is:0.10461065\n",
            "This is the iter 8490, the d1 loss is: 3095.9922, the d2 loss is: -3218.4453, the g loss is: 3222.3125, the ae loss is: 0.005420374, the jacobian loss is:0.068526834\n",
            "This is the iter 8491, the d1 loss is: 3009.3281, the d2 loss is: -3153.4922, the g loss is: 3118.5547, the ae loss is: 0.006114051, the jacobian loss is:0.15331757\n",
            "This is the iter 8492, the d1 loss is: 3053.336, the d2 loss is: -3179.6172, the g loss is: 3113.1172, the ae loss is: 0.0062799267, the jacobian loss is:0.07540546\n",
            "This is the iter 8493, the d1 loss is: 3013.5, the d2 loss is: -3147.1719, the g loss is: 3183.9219, the ae loss is: 0.007298406, the jacobian loss is:0.12280547\n",
            "This is the iter 8494, the d1 loss is: 3062.0469, the d2 loss is: -3175.2188, the g loss is: 3187.0078, the ae loss is: 0.0075841406, the jacobian loss is:0.112578295\n",
            "This is the iter 8495, the d1 loss is: 3130.8125, the d2 loss is: -3231.0625, the g loss is: 3198.1328, the ae loss is: 0.007558941, the jacobian loss is:0.09967705\n",
            "This is the iter 8496, the d1 loss is: 3086.7344, the d2 loss is: -3214.3438, the g loss is: 3201.4375, the ae loss is: 0.0072327117, the jacobian loss is:0.11483964\n",
            "This is the iter 8497, the d1 loss is: 3197.789, the d2 loss is: -3343.0547, the g loss is: 3360.2266, the ae loss is: 0.006466811, the jacobian loss is:0.101351686\n",
            "This is the iter 8498, the d1 loss is: 3054.1719, the d2 loss is: -3178.5938, the g loss is: 3207.6875, the ae loss is: 0.0048745405, the jacobian loss is:0.09933677\n",
            "This is the iter 8499, the d1 loss is: 3063.7188, the d2 loss is: -3196.7188, the g loss is: 3214.7969, the ae loss is: 0.007988484, the jacobian loss is:0.09455959\n",
            "This is the iter 8500, the d1 loss is: 2946.1562, the d2 loss is: -3092.5625, the g loss is: 3055.2734, the ae loss is: 0.0067590433, the jacobian loss is:0.09093962\n",
            "0.2601114\n",
            "0.9879209\n",
            "This is the iter 8501, the d1 loss is: 3076.086, the d2 loss is: -3210.1406, the g loss is: 3191.8906, the ae loss is: 0.010068039, the jacobian loss is:0.15866558\n",
            "This is the iter 8502, the d1 loss is: 3143.5625, the d2 loss is: -3257.8516, the g loss is: 3253.2656, the ae loss is: 0.00499749, the jacobian loss is:0.07008976\n",
            "This is the iter 8503, the d1 loss is: 3256.914, the d2 loss is: -3393.9766, the g loss is: 3416.4297, the ae loss is: 0.005821883, the jacobian loss is:0.08535251\n",
            "This is the iter 8504, the d1 loss is: 3135.8906, the d2 loss is: -3272.914, the g loss is: 3265.125, the ae loss is: 0.00670539, the jacobian loss is:0.07129279\n",
            "This is the iter 8505, the d1 loss is: 3187.2031, the d2 loss is: -3319.3438, the g loss is: 3348.4375, the ae loss is: 0.005938163, the jacobian loss is:0.07642419\n",
            "This is the iter 8506, the d1 loss is: 3115.9844, the d2 loss is: -3232.5547, the g loss is: 3235.1406, the ae loss is: 0.008530679, the jacobian loss is:0.07075015\n",
            "This is the iter 8507, the d1 loss is: 3050.5781, the d2 loss is: -3169.461, the g loss is: 3175.8125, the ae loss is: 0.007182185, the jacobian loss is:0.09363306\n",
            "This is the iter 8508, the d1 loss is: 3291.6562, the d2 loss is: -3432.1875, the g loss is: 3455.5078, the ae loss is: 0.006940316, the jacobian loss is:0.10571903\n",
            "This is the iter 8509, the d1 loss is: 3088.6953, the d2 loss is: -3188.7266, the g loss is: 3160.3516, the ae loss is: 0.008713134, the jacobian loss is:0.1328533\n",
            "This is the iter 8510, the d1 loss is: 3123.3906, the d2 loss is: -3228.3125, the g loss is: 3218.4453, the ae loss is: 0.006982158, the jacobian loss is:0.13720296\n",
            "This is the iter 8511, the d1 loss is: 2896.6328, the d2 loss is: -2999.4062, the g loss is: 3016.4844, the ae loss is: 0.006697638, the jacobian loss is:0.08391762\n",
            "This is the iter 8512, the d1 loss is: 3336.5, the d2 loss is: -3442.1797, the g loss is: 3442.789, the ae loss is: 0.0054093366, the jacobian loss is:0.086005375\n",
            "This is the iter 8513, the d1 loss is: 3047.8672, the d2 loss is: -3172.3906, the g loss is: 3207.5781, the ae loss is: 0.0054514077, the jacobian loss is:0.101128094\n",
            "This is the iter 8514, the d1 loss is: 2945.2344, the d2 loss is: -3092.9453, the g loss is: 3026.8125, the ae loss is: 0.0067686858, the jacobian loss is:0.09179015\n",
            "This is the iter 8515, the d1 loss is: 3027.6953, the d2 loss is: -3163.8516, the g loss is: 3149.7188, the ae loss is: 0.008149017, the jacobian loss is:0.09378804\n",
            "This is the iter 8516, the d1 loss is: 3176.1797, the d2 loss is: -3285.375, the g loss is: 3230.7656, the ae loss is: 0.0059603006, the jacobian loss is:0.07717453\n",
            "This is the iter 8517, the d1 loss is: 3167.5781, the d2 loss is: -3284.3906, the g loss is: 3233.211, the ae loss is: 0.0053704674, the jacobian loss is:0.11389898\n",
            "This is the iter 8518, the d1 loss is: 2916.0078, the d2 loss is: -3011.6406, the g loss is: 2981.4688, the ae loss is: 0.00667789, the jacobian loss is:0.107651025\n",
            "This is the iter 8519, the d1 loss is: 3151.336, the d2 loss is: -3285.7188, the g loss is: 3261.336, the ae loss is: 0.0062483093, the jacobian loss is:0.07913649\n",
            "This is the iter 8520, the d1 loss is: 3097.25, the d2 loss is: -3191.2734, the g loss is: 3176.9453, the ae loss is: 0.007490497, the jacobian loss is:0.07191031\n",
            "This is the iter 8521, the d1 loss is: 3072.4531, the d2 loss is: -3192.0703, the g loss is: 3230.6719, the ae loss is: 0.008057862, the jacobian loss is:0.10933763\n",
            "This is the iter 8522, the d1 loss is: 3156.2734, the d2 loss is: -3277.3281, the g loss is: 3244.4062, the ae loss is: 0.007461914, the jacobian loss is:0.10440827\n",
            "This is the iter 8523, the d1 loss is: 3109.8438, the d2 loss is: -3225.1484, the g loss is: 3223.2266, the ae loss is: 0.009036703, the jacobian loss is:0.08698302\n",
            "This is the iter 8524, the d1 loss is: 3149.6719, the d2 loss is: -3272.7812, the g loss is: 3259.7188, the ae loss is: 0.0071516493, the jacobian loss is:0.085346624\n",
            "This is the iter 8525, the d1 loss is: 3226.0469, the d2 loss is: -3342.2734, the g loss is: 3348.3906, the ae loss is: 0.0046532745, the jacobian loss is:0.07953385\n",
            "This is the iter 8526, the d1 loss is: 2818.6953, the d2 loss is: -2949.5, the g loss is: 2955.7031, the ae loss is: 0.0065770578, the jacobian loss is:0.0888937\n",
            "This is the iter 8527, the d1 loss is: 2974.5, the d2 loss is: -3109.3828, the g loss is: 3089.6172, the ae loss is: 0.0066008223, the jacobian loss is:0.10019236\n",
            "This is the iter 8528, the d1 loss is: 3212.3047, the d2 loss is: -3349.8906, the g loss is: 3327.2188, the ae loss is: 0.0068667755, the jacobian loss is:0.06976155\n",
            "This is the iter 8529, the d1 loss is: 2914.2422, the d2 loss is: -3030.1797, the g loss is: 3092.4219, the ae loss is: 0.0043901945, the jacobian loss is:0.09159239\n",
            "This is the iter 8530, the d1 loss is: 3126.0703, the d2 loss is: -3249.8125, the g loss is: 3193.3906, the ae loss is: 0.008139246, the jacobian loss is:0.09493085\n",
            "This is the iter 8531, the d1 loss is: 2788.8672, the d2 loss is: -2928.0625, the g loss is: 2964.3125, the ae loss is: 0.0040343693, the jacobian loss is:0.088292725\n",
            "This is the iter 8532, the d1 loss is: 3063.3281, the d2 loss is: -3183.289, the g loss is: 3232.4453, the ae loss is: 0.00604284, the jacobian loss is:0.078435816\n",
            "This is the iter 8533, the d1 loss is: 3082.5469, the d2 loss is: -3246.2578, the g loss is: 3197.9766, the ae loss is: 0.006731715, the jacobian loss is:0.12622772\n",
            "This is the iter 8534, the d1 loss is: 3055.5938, the d2 loss is: -3213.7812, the g loss is: 3172.1094, the ae loss is: 0.005742544, the jacobian loss is:0.11099989\n",
            "This is the iter 8535, the d1 loss is: 2939.3672, the d2 loss is: -3055.75, the g loss is: 3137.9219, the ae loss is: 0.007857809, the jacobian loss is:0.09838616\n",
            "This is the iter 8536, the d1 loss is: 2965.9453, the d2 loss is: -3086.2266, the g loss is: 3089.1016, the ae loss is: 0.0077665653, the jacobian loss is:0.080706805\n",
            "This is the iter 8537, the d1 loss is: 3035.2812, the d2 loss is: -3143.4297, the g loss is: 3240.125, the ae loss is: 0.006638635, the jacobian loss is:0.110349625\n",
            "This is the iter 8538, the d1 loss is: 3155.5469, the d2 loss is: -3262.625, the g loss is: 3157.711, the ae loss is: 0.009422786, the jacobian loss is:0.08987187\n",
            "This is the iter 8539, the d1 loss is: 3077.2812, the d2 loss is: -3182.5234, the g loss is: 3168.7344, the ae loss is: 0.005348498, the jacobian loss is:0.08493824\n",
            "This is the iter 8540, the d1 loss is: 3132.3594, the d2 loss is: -3271.3047, the g loss is: 3257.0469, the ae loss is: 0.005769762, the jacobian loss is:0.072180346\n",
            "This is the iter 8541, the d1 loss is: 3083.625, the d2 loss is: -3218.5781, the g loss is: 3253.289, the ae loss is: 0.0053812927, the jacobian loss is:0.090596974\n",
            "This is the iter 8542, the d1 loss is: 3178.0469, the d2 loss is: -3316.8203, the g loss is: 3261.0078, the ae loss is: 0.0080030365, the jacobian loss is:0.10556987\n",
            "This is the iter 8543, the d1 loss is: 3081.289, the d2 loss is: -3192.7344, the g loss is: 3225.2656, the ae loss is: 0.0054407157, the jacobian loss is:0.09828288\n",
            "This is the iter 8544, the d1 loss is: 3124.4297, the d2 loss is: -3258.2266, the g loss is: 3248.5, the ae loss is: 0.0045455834, the jacobian loss is:0.11752187\n",
            "This is the iter 8545, the d1 loss is: 3025.5938, the d2 loss is: -3145.6875, the g loss is: 3211.1953, the ae loss is: 0.0057557514, the jacobian loss is:0.1001392\n",
            "This is the iter 8546, the d1 loss is: 2902.1719, the d2 loss is: -3054.914, the g loss is: 3028.3438, the ae loss is: 0.007368435, the jacobian loss is:0.13567638\n",
            "This is the iter 8547, the d1 loss is: 3037.3438, the d2 loss is: -3164.6719, the g loss is: 3155.5156, the ae loss is: 0.0077104485, the jacobian loss is:0.12614776\n",
            "This is the iter 8548, the d1 loss is: 3399.4922, the d2 loss is: -3528.7188, the g loss is: 3523.4453, the ae loss is: 0.004670704, the jacobian loss is:0.12737185\n",
            "This is the iter 8549, the d1 loss is: 2852.4375, the d2 loss is: -2974.8281, the g loss is: 2974.6094, the ae loss is: 0.0064838654, the jacobian loss is:0.093126096\n",
            "This is the iter 8550, the d1 loss is: 3219.8594, the d2 loss is: -3341.375, the g loss is: 3318.6094, the ae loss is: 0.0076421145, the jacobian loss is:0.12010305\n",
            "This is the iter 8551, the d1 loss is: 3039.2969, the d2 loss is: -3164.586, the g loss is: 3153.5, the ae loss is: 0.0055802753, the jacobian loss is:0.08331732\n",
            "This is the iter 8552, the d1 loss is: 3078.5938, the d2 loss is: -3202.7344, the g loss is: 3224.8516, the ae loss is: 0.008803862, the jacobian loss is:0.08686613\n",
            "This is the iter 8553, the d1 loss is: 3221.0312, the d2 loss is: -3326.4297, the g loss is: 3319.6875, the ae loss is: 0.007754359, the jacobian loss is:0.12905324\n",
            "This is the iter 8554, the d1 loss is: 3089.4844, the d2 loss is: -3221.7031, the g loss is: 3203.8047, the ae loss is: 0.006076893, the jacobian loss is:0.12275913\n",
            "This is the iter 8555, the d1 loss is: 3254.9688, the d2 loss is: -3372.0156, the g loss is: 3397.125, the ae loss is: 0.0050818175, the jacobian loss is:0.11542952\n",
            "This is the iter 8556, the d1 loss is: 3175.625, the d2 loss is: -3269.4375, the g loss is: 3283.9531, the ae loss is: 0.006555021, the jacobian loss is:0.10025039\n",
            "This is the iter 8557, the d1 loss is: 3148.914, the d2 loss is: -3264.914, the g loss is: 3277.8906, the ae loss is: 0.0058550923, the jacobian loss is:0.13605411\n",
            "This is the iter 8558, the d1 loss is: 3193.6484, the d2 loss is: -3318.7969, the g loss is: 3293.75, the ae loss is: 0.011761628, the jacobian loss is:0.06550837\n",
            "This is the iter 8559, the d1 loss is: 3237.9688, the d2 loss is: -3390.7266, the g loss is: 3373.9062, the ae loss is: 0.0059861587, the jacobian loss is:0.12595877\n",
            "This is the iter 8560, the d1 loss is: 3324.0469, the d2 loss is: -3429.125, the g loss is: 3391.9375, the ae loss is: 0.008988703, the jacobian loss is:0.09737178\n",
            "This is the iter 8561, the d1 loss is: 3236.039, the d2 loss is: -3351.1719, the g loss is: 3378.0781, the ae loss is: 0.007943079, the jacobian loss is:0.12333078\n",
            "This is the iter 8562, the d1 loss is: 3164.8438, the d2 loss is: -3320.2031, the g loss is: 3291.1562, the ae loss is: 0.008330902, the jacobian loss is:0.13901722\n",
            "This is the iter 8563, the d1 loss is: 3172.0938, the d2 loss is: -3313.4844, the g loss is: 3329.2344, the ae loss is: 0.005465314, the jacobian loss is:0.105362244\n",
            "This is the iter 8564, the d1 loss is: 3187.8438, the d2 loss is: -3337.3672, the g loss is: 3312.375, the ae loss is: 0.006946928, the jacobian loss is:0.14547591\n",
            "This is the iter 8565, the d1 loss is: 3136.4219, the d2 loss is: -3268.586, the g loss is: 3243.5781, the ae loss is: 0.005207821, the jacobian loss is:0.11172228\n",
            "This is the iter 8566, the d1 loss is: 3192.1016, the d2 loss is: -3300.5312, the g loss is: 3304.5, the ae loss is: 0.005519487, the jacobian loss is:0.116114505\n",
            "This is the iter 8567, the d1 loss is: 3010.5625, the d2 loss is: -3123.1094, the g loss is: 3067.8047, the ae loss is: 0.0047193523, the jacobian loss is:0.100107\n",
            "This is the iter 8568, the d1 loss is: 3080.0078, the d2 loss is: -3206.4219, the g loss is: 3219.3281, the ae loss is: 0.0056538135, the jacobian loss is:0.11086858\n",
            "This is the iter 8569, the d1 loss is: 2874.0547, the d2 loss is: -2990.4531, the g loss is: 2939.9766, the ae loss is: 0.0062836017, the jacobian loss is:0.11407009\n",
            "This is the iter 8570, the d1 loss is: 3235.5469, the d2 loss is: -3323.1094, the g loss is: 3319.2812, the ae loss is: 0.0032142159, the jacobian loss is:0.08897555\n",
            "This is the iter 8571, the d1 loss is: 3032.789, the d2 loss is: -3157.2969, the g loss is: 3151.0547, the ae loss is: 0.0067623276, the jacobian loss is:0.13737923\n",
            "This is the iter 8572, the d1 loss is: 3048.086, the d2 loss is: -3156.3125, the g loss is: 3131.3125, the ae loss is: 0.0057888054, the jacobian loss is:0.09306764\n",
            "This is the iter 8573, the d1 loss is: 2996.2031, the d2 loss is: -3107.9062, the g loss is: 3180.2188, the ae loss is: 0.0037302398, the jacobian loss is:0.12645863\n",
            "This is the iter 8574, the d1 loss is: 3294.6172, the d2 loss is: -3414.789, the g loss is: 3474.75, the ae loss is: 0.005398622, the jacobian loss is:0.09986126\n",
            "This is the iter 8575, the d1 loss is: 2980.8984, the d2 loss is: -3132.625, the g loss is: 3173.2734, the ae loss is: 0.00680374, the jacobian loss is:0.099652074\n",
            "This is the iter 8576, the d1 loss is: 3214.789, the d2 loss is: -3338.0156, the g loss is: 3341.8281, the ae loss is: 0.006750171, the jacobian loss is:0.13251482\n",
            "This is the iter 8577, the d1 loss is: 2999.875, the d2 loss is: -3126.1797, the g loss is: 3082.8594, the ae loss is: 0.0046045156, the jacobian loss is:0.10775778\n",
            "This is the iter 8578, the d1 loss is: 3065.6484, the d2 loss is: -3177.711, the g loss is: 3275.6172, the ae loss is: 0.005847931, the jacobian loss is:0.07381923\n",
            "This is the iter 8579, the d1 loss is: 3094.336, the d2 loss is: -3208.5625, the g loss is: 3224.664, the ae loss is: 0.005776314, the jacobian loss is:0.14412594\n",
            "This is the iter 8580, the d1 loss is: 3052.3125, the d2 loss is: -3162.0156, the g loss is: 3176.711, the ae loss is: 0.008049661, the jacobian loss is:0.14918752\n",
            "This is the iter 8581, the d1 loss is: 3182.9688, the d2 loss is: -3320.3828, the g loss is: 3360.8125, the ae loss is: 0.007983224, the jacobian loss is:0.114357315\n",
            "This is the iter 8582, the d1 loss is: 3203.7812, the d2 loss is: -3337.7188, the g loss is: 3313.3125, the ae loss is: 0.008537348, the jacobian loss is:0.13512719\n",
            "This is the iter 8583, the d1 loss is: 3059.5781, the d2 loss is: -3187.961, the g loss is: 3200.2344, the ae loss is: 0.007411586, the jacobian loss is:0.09770338\n",
            "This is the iter 8584, the d1 loss is: 3142.625, the d2 loss is: -3280.4375, the g loss is: 3262.8125, the ae loss is: 0.011911835, the jacobian loss is:0.06695378\n",
            "This is the iter 8585, the d1 loss is: 3129.2969, the d2 loss is: -3245.0312, the g loss is: 3226.961, the ae loss is: 0.006143797, the jacobian loss is:0.13790023\n",
            "This is the iter 8586, the d1 loss is: 3047.875, the d2 loss is: -3183.1562, the g loss is: 3235.2578, the ae loss is: 0.007153552, the jacobian loss is:0.09875277\n",
            "This is the iter 8587, the d1 loss is: 3411.0078, the d2 loss is: -3531.4297, the g loss is: 3527.875, the ae loss is: 0.00759599, the jacobian loss is:0.14110121\n",
            "This is the iter 8588, the d1 loss is: 3100.1328, the d2 loss is: -3230.3281, the g loss is: 3259.5938, the ae loss is: 0.007461897, the jacobian loss is:0.08129244\n",
            "This is the iter 8589, the d1 loss is: 3386.0078, the d2 loss is: -3489.9297, the g loss is: 3460.6328, the ae loss is: 0.006969159, the jacobian loss is:0.12823685\n",
            "This is the iter 8590, the d1 loss is: 3146.7422, the d2 loss is: -3260.539, the g loss is: 3268.7422, the ae loss is: 0.005789222, the jacobian loss is:0.07053057\n",
            "This is the iter 8591, the d1 loss is: 2947.6953, the d2 loss is: -3074.789, the g loss is: 3062.3906, the ae loss is: 0.0061607147, the jacobian loss is:0.10248078\n",
            "This is the iter 8592, the d1 loss is: 3267.7656, the d2 loss is: -3386.6875, the g loss is: 3408.375, the ae loss is: 0.0057719206, the jacobian loss is:0.112335786\n",
            "This is the iter 8593, the d1 loss is: 3113.5938, the d2 loss is: -3238.3906, the g loss is: 3308.75, the ae loss is: 0.005591823, the jacobian loss is:0.122672774\n",
            "This is the iter 8594, the d1 loss is: 3169.875, the d2 loss is: -3289.125, the g loss is: 3306.7031, the ae loss is: 0.0044970447, the jacobian loss is:0.11028458\n",
            "This is the iter 8595, the d1 loss is: 3305.664, the d2 loss is: -3418.5, the g loss is: 3376.711, the ae loss is: 0.003898565, the jacobian loss is:0.09032325\n",
            "This is the iter 8596, the d1 loss is: 3325.5469, the d2 loss is: -3449.6094, the g loss is: 3476.9766, the ae loss is: 0.0061443592, the jacobian loss is:0.09337966\n",
            "This is the iter 8597, the d1 loss is: 2900.414, the d2 loss is: -3044.8047, the g loss is: 3006.3203, the ae loss is: 0.008612304, the jacobian loss is:0.09081141\n",
            "This is the iter 8598, the d1 loss is: 3157.4219, the d2 loss is: -3298.3906, the g loss is: 3319.3594, the ae loss is: 0.0058648027, the jacobian loss is:0.120642215\n",
            "This is the iter 8599, the d1 loss is: 3179.9922, the d2 loss is: -3297.1875, the g loss is: 3290.1328, the ae loss is: 0.0064160945, the jacobian loss is:0.1062574\n",
            "This is the iter 8600, the d1 loss is: 3104.375, the d2 loss is: -3234.4844, the g loss is: 3256.3672, the ae loss is: 0.0064226836, the jacobian loss is:0.07231642\n",
            "0.26218516\n",
            "0.995362\n",
            "This is the iter 8601, the d1 loss is: 2896.5469, the d2 loss is: -3003.4219, the g loss is: 3042.539, the ae loss is: 0.0063741943, the jacobian loss is:0.10895614\n",
            "This is the iter 8602, the d1 loss is: 3059.0938, the d2 loss is: -3171.4688, the g loss is: 3254.1562, the ae loss is: 0.006873922, the jacobian loss is:0.09274822\n",
            "This is the iter 8603, the d1 loss is: 3109.7734, the d2 loss is: -3257.1953, the g loss is: 3288.9375, the ae loss is: 0.0068937046, the jacobian loss is:0.08512932\n",
            "This is the iter 8604, the d1 loss is: 3126.2969, the d2 loss is: -3236.3125, the g loss is: 3237.1328, the ae loss is: 0.005806375, the jacobian loss is:0.09607511\n",
            "This is the iter 8605, the d1 loss is: 2861.336, the d2 loss is: -3005.9297, the g loss is: 3007.7422, the ae loss is: 0.008257017, the jacobian loss is:0.09799757\n",
            "This is the iter 8606, the d1 loss is: 3087.2188, the d2 loss is: -3217.0156, the g loss is: 3274.3203, the ae loss is: 0.00814987, the jacobian loss is:0.102956645\n",
            "This is the iter 8607, the d1 loss is: 3024.75, the d2 loss is: -3172.25, the g loss is: 3152.8906, the ae loss is: 0.0061598895, the jacobian loss is:0.104342006\n",
            "This is the iter 8608, the d1 loss is: 2814.4219, the d2 loss is: -2950.6016, the g loss is: 2935.9688, the ae loss is: 0.007105851, the jacobian loss is:0.15635204\n",
            "This is the iter 8609, the d1 loss is: 3082.1562, the d2 loss is: -3181.4219, the g loss is: 3146.3047, the ae loss is: 0.0052251117, the jacobian loss is:0.12347315\n",
            "This is the iter 8610, the d1 loss is: 3152.211, the d2 loss is: -3261.086, the g loss is: 3220.9844, the ae loss is: 0.0040870192, the jacobian loss is:0.107431754\n",
            "This is the iter 8611, the d1 loss is: 3021.4844, the d2 loss is: -3159.8125, the g loss is: 3133.0312, the ae loss is: 0.008792871, the jacobian loss is:0.14365229\n",
            "This is the iter 8612, the d1 loss is: 2969.5625, the d2 loss is: -3071.2812, the g loss is: 3040.6094, the ae loss is: 0.0058105784, the jacobian loss is:0.08862364\n",
            "This is the iter 8613, the d1 loss is: 3013.1719, the d2 loss is: -3144.836, the g loss is: 3152.0312, the ae loss is: 0.0070232577, the jacobian loss is:0.13495678\n",
            "This is the iter 8614, the d1 loss is: 3153.1797, the d2 loss is: -3287.6484, the g loss is: 3270.5156, the ae loss is: 0.006627401, the jacobian loss is:0.07530015\n",
            "This is the iter 8615, the d1 loss is: 2949.414, the d2 loss is: -3062.0547, the g loss is: 3055.4531, the ae loss is: 0.0043454464, the jacobian loss is:0.107774004\n",
            "This is the iter 8616, the d1 loss is: 3112.7188, the d2 loss is: -3226.7188, the g loss is: 3198.914, the ae loss is: 0.00733949, the jacobian loss is:0.10424907\n",
            "This is the iter 8617, the d1 loss is: 3232.5, the d2 loss is: -3383.8516, the g loss is: 3362.5703, the ae loss is: 0.007345555, the jacobian loss is:0.09810546\n",
            "This is the iter 8618, the d1 loss is: 3077.5469, the d2 loss is: -3222.4844, the g loss is: 3203.5703, the ae loss is: 0.00803037, the jacobian loss is:0.09910438\n",
            "This is the iter 8619, the d1 loss is: 3338.8906, the d2 loss is: -3446.336, the g loss is: 3479.8594, the ae loss is: 0.0053143026, the jacobian loss is:0.10670295\n",
            "This is the iter 8620, the d1 loss is: 2893.5, the d2 loss is: -3025.4062, the g loss is: 2982.0312, the ae loss is: 0.00553003, the jacobian loss is:0.11535013\n",
            "This is the iter 8621, the d1 loss is: 3031.3125, the d2 loss is: -3122.5625, the g loss is: 3183.9297, the ae loss is: 0.008116114, the jacobian loss is:0.10938121\n",
            "This is the iter 8622, the d1 loss is: 3109.1484, the d2 loss is: -3226.8906, the g loss is: 3251.2344, the ae loss is: 0.006325134, the jacobian loss is:0.09774689\n",
            "This is the iter 8623, the d1 loss is: 3017.6875, the d2 loss is: -3152.2344, the g loss is: 3092.4062, the ae loss is: 0.0047375783, the jacobian loss is:0.14484286\n",
            "This is the iter 8624, the d1 loss is: 3093.1328, the d2 loss is: -3217.8906, the g loss is: 3251.6953, the ae loss is: 0.005238183, the jacobian loss is:0.10332452\n",
            "This is the iter 8625, the d1 loss is: 3019.25, the d2 loss is: -3150.1016, the g loss is: 3163.6172, the ae loss is: 0.0058325166, the jacobian loss is:0.10406777\n",
            "This is the iter 8626, the d1 loss is: 3082.7031, the d2 loss is: -3217.6094, the g loss is: 3248.25, the ae loss is: 0.004776048, the jacobian loss is:0.11549246\n",
            "This is the iter 8627, the d1 loss is: 2990.5156, the d2 loss is: -3108.1719, the g loss is: 3112.4531, the ae loss is: 0.0055626775, the jacobian loss is:0.082147725\n",
            "This is the iter 8628, the d1 loss is: 3033.7734, the d2 loss is: -3182.1953, the g loss is: 3178.6953, the ae loss is: 0.0072525884, the jacobian loss is:0.12349135\n",
            "This is the iter 8629, the d1 loss is: 3027.6406, the d2 loss is: -3131.0312, the g loss is: 3099.8047, the ae loss is: 0.007896336, the jacobian loss is:0.093699954\n",
            "This is the iter 8630, the d1 loss is: 3068.6172, the d2 loss is: -3205.4297, the g loss is: 3219.9062, the ae loss is: 0.005372555, the jacobian loss is:0.1018798\n",
            "This is the iter 8631, the d1 loss is: 3095.2578, the d2 loss is: -3227.4688, the g loss is: 3336.6719, the ae loss is: 0.00661134, the jacobian loss is:0.11494375\n",
            "This is the iter 8632, the d1 loss is: 3072.3672, the d2 loss is: -3187.3281, the g loss is: 3193.289, the ae loss is: 0.006990209, the jacobian loss is:0.09485684\n",
            "This is the iter 8633, the d1 loss is: 3032.5156, the d2 loss is: -3173.7422, the g loss is: 3146.914, the ae loss is: 0.007141233, the jacobian loss is:0.12887192\n",
            "This is the iter 8634, the d1 loss is: 3100.0938, the d2 loss is: -3214.8984, the g loss is: 3198.3203, the ae loss is: 0.008204559, the jacobian loss is:0.08457974\n",
            "This is the iter 8635, the d1 loss is: 3019.3203, the d2 loss is: -3153.9219, the g loss is: 3143.0938, the ae loss is: 0.0068120793, the jacobian loss is:0.08152429\n",
            "This is the iter 8636, the d1 loss is: 3052.3516, the d2 loss is: -3150.2266, the g loss is: 3171.5312, the ae loss is: 0.005466128, the jacobian loss is:0.1585305\n",
            "This is the iter 8637, the d1 loss is: 3030.5, the d2 loss is: -3116.6953, the g loss is: 3136.1094, the ae loss is: 0.006481424, the jacobian loss is:0.093161166\n",
            "This is the iter 8638, the d1 loss is: 2894.2734, the d2 loss is: -3019.4922, the g loss is: 3036.4688, the ae loss is: 0.0052954266, the jacobian loss is:0.10100502\n",
            "This is the iter 8639, the d1 loss is: 3086.8281, the d2 loss is: -3214.6562, the g loss is: 3291.4219, the ae loss is: 0.0062020747, the jacobian loss is:0.06910361\n",
            "This is the iter 8640, the d1 loss is: 3143.4219, the d2 loss is: -3269.6875, the g loss is: 3215.2969, the ae loss is: 0.007333143, the jacobian loss is:0.10656644\n",
            "This is the iter 8641, the d1 loss is: 2945.9453, the d2 loss is: -3067.625, the g loss is: 3100.4531, the ae loss is: 0.0058340067, the jacobian loss is:0.08848264\n",
            "This is the iter 8642, the d1 loss is: 2947.3906, the d2 loss is: -3058.9297, the g loss is: 3031.3594, the ae loss is: 0.0060479534, the jacobian loss is:0.07789823\n",
            "This is the iter 8643, the d1 loss is: 2906.5156, the d2 loss is: -3003.3594, the g loss is: 3075.3828, the ae loss is: 0.0050622495, the jacobian loss is:0.08801572\n",
            "This is the iter 8644, the d1 loss is: 3148.25, the d2 loss is: -3252.7344, the g loss is: 3280.4766, the ae loss is: 0.0069569354, the jacobian loss is:0.09236203\n",
            "This is the iter 8645, the d1 loss is: 2992.9844, the d2 loss is: -3108.6016, the g loss is: 3115.4219, the ae loss is: 0.0062679383, the jacobian loss is:0.12307888\n",
            "This is the iter 8646, the d1 loss is: 2993.0156, the d2 loss is: -3123.8984, the g loss is: 3123.5781, the ae loss is: 0.005486503, the jacobian loss is:0.08642113\n",
            "This is the iter 8647, the d1 loss is: 2962.25, the d2 loss is: -3086.4453, the g loss is: 3064.5, the ae loss is: 0.008812966, the jacobian loss is:0.112038866\n",
            "This is the iter 8648, the d1 loss is: 3074.4844, the d2 loss is: -3189.3281, the g loss is: 3161.1953, the ae loss is: 0.007147001, the jacobian loss is:0.09877893\n",
            "This is the iter 8649, the d1 loss is: 3046.9531, the d2 loss is: -3166.9922, the g loss is: 3246.7422, the ae loss is: 0.0040308116, the jacobian loss is:0.10085472\n",
            "This is the iter 8650, the d1 loss is: 3030.5781, the d2 loss is: -3152.75, the g loss is: 3180.5469, the ae loss is: 0.0052758283, the jacobian loss is:0.10914446\n",
            "This is the iter 8651, the d1 loss is: 2936.0469, the d2 loss is: -3046.7031, the g loss is: 3031.3828, the ae loss is: 0.010000557, the jacobian loss is:0.14743501\n",
            "This is the iter 8652, the d1 loss is: 2893.164, the d2 loss is: -3022.4688, the g loss is: 3087.7812, the ae loss is: 0.006387357, the jacobian loss is:0.09220222\n",
            "This is the iter 8653, the d1 loss is: 3147.414, the d2 loss is: -3265.4453, the g loss is: 3285.25, the ae loss is: 0.005977217, the jacobian loss is:0.10511955\n",
            "This is the iter 8654, the d1 loss is: 3058.125, the d2 loss is: -3181.3125, the g loss is: 3100.6016, the ae loss is: 0.009101637, the jacobian loss is:0.10635339\n",
            "This is the iter 8655, the d1 loss is: 2831.9375, the d2 loss is: -2953.3984, the g loss is: 3005.039, the ae loss is: 0.008215174, the jacobian loss is:0.14708456\n",
            "This is the iter 8656, the d1 loss is: 2997.4453, the d2 loss is: -3131.8047, the g loss is: 3100.4688, the ae loss is: 0.009093433, the jacobian loss is:0.12206913\n",
            "This is the iter 8657, the d1 loss is: 3042.1406, the d2 loss is: -3150.75, the g loss is: 3136.2656, the ae loss is: 0.0056074187, the jacobian loss is:0.100373745\n",
            "This is the iter 8658, the d1 loss is: 3083.039, the d2 loss is: -3212.0312, the g loss is: 3170.8125, the ae loss is: 0.009262761, the jacobian loss is:0.11962084\n",
            "This is the iter 8659, the d1 loss is: 3057.789, the d2 loss is: -3184.836, the g loss is: 3150.289, the ae loss is: 0.006317882, the jacobian loss is:0.116330855\n",
            "This is the iter 8660, the d1 loss is: 3000.9219, the d2 loss is: -3156.9844, the g loss is: 3155.7344, the ae loss is: 0.00664445, the jacobian loss is:0.10595943\n",
            "This is the iter 8661, the d1 loss is: 3003.0156, the d2 loss is: -3119.7188, the g loss is: 3135.5078, the ae loss is: 0.0039887396, the jacobian loss is:0.112661555\n",
            "This is the iter 8662, the d1 loss is: 3004.6797, the d2 loss is: -3150.5938, the g loss is: 3187.8516, the ae loss is: 0.006589796, the jacobian loss is:0.08115057\n",
            "This is the iter 8663, the d1 loss is: 2811.625, the d2 loss is: -2894.1875, the g loss is: 2910.5469, the ae loss is: 0.0072179036, the jacobian loss is:0.120244384\n",
            "This is the iter 8664, the d1 loss is: 3215.9766, the d2 loss is: -3326.8125, the g loss is: 3374.6094, the ae loss is: 0.00444382, the jacobian loss is:0.09289232\n",
            "This is the iter 8665, the d1 loss is: 3070.5156, the d2 loss is: -3186.2266, the g loss is: 3160.3984, the ae loss is: 0.00562915, the jacobian loss is:0.1013027\n",
            "This is the iter 8666, the d1 loss is: 3014.1953, the d2 loss is: -3122.6406, the g loss is: 3122.4766, the ae loss is: 0.006638699, the jacobian loss is:0.1214218\n",
            "This is the iter 8667, the d1 loss is: 3173.8594, the d2 loss is: -3304.0938, the g loss is: 3336.2188, the ae loss is: 0.006870846, the jacobian loss is:0.07502358\n",
            "This is the iter 8668, the d1 loss is: 2788.0781, the d2 loss is: -2924.0469, the g loss is: 2957.3125, the ae loss is: 0.0049935943, the jacobian loss is:0.08687344\n",
            "This is the iter 8669, the d1 loss is: 3298.4375, the d2 loss is: -3412.1719, the g loss is: 3448.2656, the ae loss is: 0.005261018, the jacobian loss is:0.098546356\n",
            "This is the iter 8670, the d1 loss is: 3162.4844, the d2 loss is: -3286.875, the g loss is: 3278.3984, the ae loss is: 0.0065852385, the jacobian loss is:0.13889395\n",
            "This is the iter 8671, the d1 loss is: 3119.2812, the d2 loss is: -3257.0156, the g loss is: 3207.9219, the ae loss is: 0.007142695, the jacobian loss is:0.09708005\n",
            "This is the iter 8672, the d1 loss is: 3112.9844, the d2 loss is: -3220.0234, the g loss is: 3180.3125, the ae loss is: 0.006982876, the jacobian loss is:0.14071426\n",
            "This is the iter 8673, the d1 loss is: 3047.2422, the d2 loss is: -3147.9766, the g loss is: 3200.2578, the ae loss is: 0.0045723976, the jacobian loss is:0.13147324\n",
            "This is the iter 8674, the d1 loss is: 3078.6328, the d2 loss is: -3217.7422, the g loss is: 3131.2266, the ae loss is: 0.007428853, the jacobian loss is:0.105905846\n",
            "This is the iter 8675, the d1 loss is: 2910.3203, the d2 loss is: -3030.1875, the g loss is: 3142.0547, the ae loss is: 0.006515283, the jacobian loss is:0.11303583\n",
            "This is the iter 8676, the d1 loss is: 3081.711, the d2 loss is: -3202.586, the g loss is: 3255.2812, the ae loss is: 0.005727145, the jacobian loss is:0.08014298\n",
            "This is the iter 8677, the d1 loss is: 3054.2969, the d2 loss is: -3169.1484, the g loss is: 3199.9062, the ae loss is: 0.0057136295, the jacobian loss is:0.097806714\n",
            "This is the iter 8678, the d1 loss is: 3107.25, the d2 loss is: -3214.8203, the g loss is: 3235.1562, the ae loss is: 0.0061296625, the jacobian loss is:0.067350194\n",
            "This is the iter 8679, the d1 loss is: 3088.664, the d2 loss is: -3232.086, the g loss is: 3197.3906, the ae loss is: 0.0064922175, the jacobian loss is:0.2261753\n",
            "This is the iter 8680, the d1 loss is: 3196.1562, the d2 loss is: -3296.1953, the g loss is: 3253.6953, the ae loss is: 0.0083123045, the jacobian loss is:0.12672423\n",
            "This is the iter 8681, the d1 loss is: 3067.25, the d2 loss is: -3183.586, the g loss is: 3164.7031, the ae loss is: 0.0075344252, the jacobian loss is:0.080168344\n",
            "This is the iter 8682, the d1 loss is: 3117.7812, the d2 loss is: -3241.1406, the g loss is: 3227.0469, the ae loss is: 0.007584784, the jacobian loss is:0.100063875\n",
            "This is the iter 8683, the d1 loss is: 3033.1016, the d2 loss is: -3147.0469, the g loss is: 3156.5156, the ae loss is: 0.0073652836, the jacobian loss is:0.090129666\n",
            "This is the iter 8684, the d1 loss is: 3020.9688, the d2 loss is: -3140.7656, the g loss is: 3114.125, the ae loss is: 0.0045492565, the jacobian loss is:0.12306616\n",
            "This is the iter 8685, the d1 loss is: 3275.8438, the d2 loss is: -3403.6406, the g loss is: 3413.3828, the ae loss is: 0.0067764125, the jacobian loss is:0.106482476\n",
            "This is the iter 8686, the d1 loss is: 3119.5469, the d2 loss is: -3249.0156, the g loss is: 3283.5, the ae loss is: 0.0067871413, the jacobian loss is:0.08887395\n",
            "This is the iter 8687, the d1 loss is: 2939.25, the d2 loss is: -3063.914, the g loss is: 3070.9688, the ae loss is: 0.0060177995, the jacobian loss is:0.10629898\n",
            "This is the iter 8688, the d1 loss is: 3036.789, the d2 loss is: -3129.414, the g loss is: 3100.1719, the ae loss is: 0.008248618, the jacobian loss is:0.090349086\n",
            "This is the iter 8689, the d1 loss is: 2997.6719, the d2 loss is: -3127.414, the g loss is: 3106.3125, the ae loss is: 0.0047722124, the jacobian loss is:0.0855347\n",
            "This is the iter 8690, the d1 loss is: 3152.5, the d2 loss is: -3243.5547, the g loss is: 3230.2344, the ae loss is: 0.008217072, the jacobian loss is:0.15502067\n",
            "This is the iter 8691, the d1 loss is: 3076.5156, the d2 loss is: -3196.7656, the g loss is: 3214.0781, the ae loss is: 0.006822845, the jacobian loss is:0.10215806\n",
            "This is the iter 8692, the d1 loss is: 3117.0625, the d2 loss is: -3221.2344, the g loss is: 3282.4531, the ae loss is: 0.0061697876, the jacobian loss is:0.10526771\n",
            "This is the iter 8693, the d1 loss is: 2997.0469, the d2 loss is: -3092.1797, the g loss is: 3118.8516, the ae loss is: 0.005812958, the jacobian loss is:0.10494648\n",
            "This is the iter 8694, the d1 loss is: 3115.8125, the d2 loss is: -3210.4844, the g loss is: 3264.2422, the ae loss is: 0.0060074287, the jacobian loss is:0.0951338\n",
            "This is the iter 8695, the d1 loss is: 3020.0078, the d2 loss is: -3123.9219, the g loss is: 3169.7578, the ae loss is: 0.0064070392, the jacobian loss is:0.11346494\n",
            "This is the iter 8696, the d1 loss is: 3141.7031, the d2 loss is: -3249.7734, the g loss is: 3264.6875, the ae loss is: 0.006765876, the jacobian loss is:0.097210675\n",
            "This is the iter 8697, the d1 loss is: 3058.2266, the d2 loss is: -3181.7734, the g loss is: 3157.0781, the ae loss is: 0.0071784193, the jacobian loss is:0.14389616\n",
            "This is the iter 8698, the d1 loss is: 3323.5938, the d2 loss is: -3445.8125, the g loss is: 3429.0547, the ae loss is: 0.0087021375, the jacobian loss is:0.1412458\n",
            "This is the iter 8699, the d1 loss is: 3083.3203, the d2 loss is: -3197.1875, the g loss is: 3209.9531, the ae loss is: 0.008010155, the jacobian loss is:0.11477587\n",
            "This is the iter 8700, the d1 loss is: 3116.6172, the d2 loss is: -3234.3516, the g loss is: 3255.125, the ae loss is: 0.004496286, the jacobian loss is:0.0904278\n",
            "0.265358\n",
            "1.0082984\n",
            "This is the iter 8701, the d1 loss is: 2832.086, the d2 loss is: -2938.5, the g loss is: 2973.5, the ae loss is: 0.00831332, the jacobian loss is:0.10558479\n",
            "This is the iter 8702, the d1 loss is: 3174.5234, the d2 loss is: -3287.625, the g loss is: 3261.6719, the ae loss is: 0.008173016, the jacobian loss is:0.097423285\n",
            "This is the iter 8703, the d1 loss is: 2965.914, the d2 loss is: -3081.6953, the g loss is: 3076.8828, the ae loss is: 0.00736483, the jacobian loss is:0.086097635\n",
            "This is the iter 8704, the d1 loss is: 3097.4453, the d2 loss is: -3225.3281, the g loss is: 3213.3203, the ae loss is: 0.0050375494, the jacobian loss is:0.09219519\n",
            "This is the iter 8705, the d1 loss is: 3046.6094, the d2 loss is: -3160.7422, the g loss is: 3177.7188, the ae loss is: 0.008736325, the jacobian loss is:0.09679821\n",
            "This is the iter 8706, the d1 loss is: 3256.1328, the d2 loss is: -3373.5078, the g loss is: 3336.1016, the ae loss is: 0.007446606, the jacobian loss is:0.11148144\n",
            "This is the iter 8707, the d1 loss is: 3324.664, the d2 loss is: -3432.086, the g loss is: 3423.8906, the ae loss is: 0.0072964467, the jacobian loss is:0.14004013\n",
            "This is the iter 8708, the d1 loss is: 3133.6172, the d2 loss is: -3254.0, the g loss is: 3200.0703, the ae loss is: 0.009315882, the jacobian loss is:0.08707783\n",
            "This is the iter 8709, the d1 loss is: 2921.6953, the d2 loss is: -3040.0625, the g loss is: 3042.5234, the ae loss is: 0.0054905214, the jacobian loss is:0.115017466\n",
            "This is the iter 8710, the d1 loss is: 3333.9062, the d2 loss is: -3472.3828, the g loss is: 3446.9219, the ae loss is: 0.0064947046, the jacobian loss is:0.10445948\n",
            "This is the iter 8711, the d1 loss is: 3073.1484, the d2 loss is: -3193.7344, the g loss is: 3178.1875, the ae loss is: 0.0051852805, the jacobian loss is:0.0915785\n",
            "This is the iter 8712, the d1 loss is: 3141.3281, the d2 loss is: -3255.1016, the g loss is: 3228.7266, the ae loss is: 0.0077154418, the jacobian loss is:0.14759277\n",
            "This is the iter 8713, the d1 loss is: 3086.7031, the d2 loss is: -3204.1797, the g loss is: 3171.2812, the ae loss is: 0.007103029, the jacobian loss is:0.08609323\n",
            "This is the iter 8714, the d1 loss is: 3077.211, the d2 loss is: -3192.7188, the g loss is: 3223.75, the ae loss is: 0.0069157914, the jacobian loss is:0.112691075\n",
            "This is the iter 8715, the d1 loss is: 3159.8203, the d2 loss is: -3265.7812, the g loss is: 3279.7812, the ae loss is: 0.0065567205, the jacobian loss is:0.08141096\n",
            "This is the iter 8716, the d1 loss is: 2937.3438, the d2 loss is: -3054.5156, the g loss is: 3037.6875, the ae loss is: 0.0049367445, the jacobian loss is:0.066058755\n",
            "This is the iter 8717, the d1 loss is: 3104.2734, the d2 loss is: -3250.8438, the g loss is: 3172.0703, the ae loss is: 0.00815276, the jacobian loss is:0.12926392\n",
            "This is the iter 8718, the d1 loss is: 3035.4531, the d2 loss is: -3138.7734, the g loss is: 3243.0078, the ae loss is: 0.006243117, the jacobian loss is:0.11424428\n",
            "This is the iter 8719, the d1 loss is: 3105.6875, the d2 loss is: -3194.5781, the g loss is: 3220.4688, the ae loss is: 0.0037551299, the jacobian loss is:0.13646491\n",
            "This is the iter 8720, the d1 loss is: 3250.414, the d2 loss is: -3392.0156, the g loss is: 3393.0156, the ae loss is: 0.0057614613, the jacobian loss is:0.121921375\n",
            "This is the iter 8721, the d1 loss is: 2901.586, the d2 loss is: -3031.9297, the g loss is: 2955.2812, the ae loss is: 0.007831639, the jacobian loss is:0.13735549\n",
            "This is the iter 8722, the d1 loss is: 3354.9688, the d2 loss is: -3480.7422, the g loss is: 3447.8672, the ae loss is: 0.0061569884, the jacobian loss is:0.12895358\n",
            "This is the iter 8723, the d1 loss is: 3136.0625, the d2 loss is: -3258.0156, the g loss is: 3237.1172, the ae loss is: 0.0062509836, the jacobian loss is:0.1233083\n",
            "This is the iter 8724, the d1 loss is: 3184.3672, the d2 loss is: -3296.7812, the g loss is: 3268.7031, the ae loss is: 0.007614449, the jacobian loss is:0.080760196\n",
            "This is the iter 8725, the d1 loss is: 3094.7656, the d2 loss is: -3224.0625, the g loss is: 3212.8906, the ae loss is: 0.007932519, the jacobian loss is:0.093655765\n",
            "This is the iter 8726, the d1 loss is: 3349.586, the d2 loss is: -3474.4844, the g loss is: 3399.1562, the ae loss is: 0.00635158, the jacobian loss is:0.0850226\n",
            "This is the iter 8727, the d1 loss is: 2869.3672, the d2 loss is: -2988.7422, the g loss is: 2947.6016, the ae loss is: 0.0054009142, the jacobian loss is:0.14224046\n",
            "This is the iter 8728, the d1 loss is: 3203.8984, the d2 loss is: -3329.8906, the g loss is: 3349.3438, the ae loss is: 0.006834436, the jacobian loss is:0.10976518\n",
            "This is the iter 8729, the d1 loss is: 3184.0547, the d2 loss is: -3311.2578, the g loss is: 3292.375, the ae loss is: 0.0076457225, the jacobian loss is:0.12852594\n",
            "This is the iter 8730, the d1 loss is: 2865.9922, the d2 loss is: -2950.664, the g loss is: 2944.789, the ae loss is: 0.010192959, the jacobian loss is:0.19194146\n",
            "This is the iter 8731, the d1 loss is: 3066.8672, the d2 loss is: -3168.7031, the g loss is: 3176.4688, the ae loss is: 0.0053330055, the jacobian loss is:0.08022159\n",
            "This is the iter 8732, the d1 loss is: 3023.5312, the d2 loss is: -3168.1953, the g loss is: 3161.9766, the ae loss is: 0.00577615, the jacobian loss is:0.18762928\n",
            "This is the iter 8733, the d1 loss is: 3011.2734, the d2 loss is: -3139.7344, the g loss is: 3214.6875, the ae loss is: 0.0063208127, the jacobian loss is:0.087215535\n",
            "This is the iter 8734, the d1 loss is: 3023.0156, the d2 loss is: -3132.8984, the g loss is: 3055.0234, the ae loss is: 0.007335767, the jacobian loss is:0.10205445\n",
            "This is the iter 8735, the d1 loss is: 3074.6562, the d2 loss is: -3196.7656, the g loss is: 3207.2656, the ae loss is: 0.009877925, the jacobian loss is:0.15651754\n",
            "This is the iter 8736, the d1 loss is: 3233.6406, the d2 loss is: -3369.0312, the g loss is: 3366.6875, the ae loss is: 0.006101124, the jacobian loss is:0.14608218\n",
            "This is the iter 8737, the d1 loss is: 3192.3438, the d2 loss is: -3324.0781, the g loss is: 3308.0938, the ae loss is: 0.009495321, the jacobian loss is:0.17086662\n",
            "This is the iter 8738, the d1 loss is: 3075.8906, the d2 loss is: -3169.9297, the g loss is: 3141.1406, the ae loss is: 0.006883457, the jacobian loss is:0.10886095\n",
            "This is the iter 8739, the d1 loss is: 3055.789, the d2 loss is: -3172.2422, the g loss is: 3175.2656, the ae loss is: 0.0064612394, the jacobian loss is:0.10893117\n",
            "This is the iter 8740, the d1 loss is: 3460.211, the d2 loss is: -3571.6719, the g loss is: 3505.125, the ae loss is: 0.005206473, the jacobian loss is:0.1184276\n",
            "This is the iter 8741, the d1 loss is: 3337.6875, the d2 loss is: -3435.5469, the g loss is: 3429.5625, the ae loss is: 0.0055845897, the jacobian loss is:0.120662235\n",
            "This is the iter 8742, the d1 loss is: 3022.7188, the d2 loss is: -3118.625, the g loss is: 3137.1562, the ae loss is: 0.00490701, the jacobian loss is:0.097275704\n",
            "This is the iter 8743, the d1 loss is: 3083.7969, the d2 loss is: -3193.7422, the g loss is: 3178.836, the ae loss is: 0.0046265847, the jacobian loss is:0.09060299\n",
            "This is the iter 8744, the d1 loss is: 2818.3828, the d2 loss is: -2927.1953, the g loss is: 2945.5703, the ae loss is: 0.0055190604, the jacobian loss is:0.07526857\n",
            "This is the iter 8745, the d1 loss is: 3018.0078, the d2 loss is: -3147.9688, the g loss is: 3175.4688, the ae loss is: 0.00473034, the jacobian loss is:0.13794594\n",
            "This is the iter 8746, the d1 loss is: 3160.4297, the d2 loss is: -3257.1797, the g loss is: 3221.539, the ae loss is: 0.008143162, the jacobian loss is:0.10467016\n",
            "This is the iter 8747, the d1 loss is: 3106.711, the d2 loss is: -3220.414, the g loss is: 3211.2969, the ae loss is: 0.0054583396, the jacobian loss is:0.12305981\n",
            "This is the iter 8748, the d1 loss is: 2908.8047, the d2 loss is: -3038.5781, the g loss is: 3078.2344, the ae loss is: 0.005800659, the jacobian loss is:0.09138007\n",
            "This is the iter 8749, the d1 loss is: 3142.2188, the d2 loss is: -3238.6172, the g loss is: 3287.0938, the ae loss is: 0.005623702, the jacobian loss is:0.113172404\n",
            "This is the iter 8750, the d1 loss is: 2851.5078, the d2 loss is: -2957.7031, the g loss is: 2902.5781, the ae loss is: 0.004991144, the jacobian loss is:0.10496695\n",
            "This is the iter 8751, the d1 loss is: 2992.3281, the d2 loss is: -3103.5781, the g loss is: 3143.1719, the ae loss is: 0.0052458514, the jacobian loss is:0.10998979\n",
            "This is the iter 8752, the d1 loss is: 2968.9219, the d2 loss is: -3101.5, the g loss is: 3154.3281, the ae loss is: 0.006434589, the jacobian loss is:0.096947424\n",
            "This is the iter 8753, the d1 loss is: 3036.9062, the d2 loss is: -3153.1094, the g loss is: 3144.8125, the ae loss is: 0.0047572954, the jacobian loss is:0.120359875\n",
            "This is the iter 8754, the d1 loss is: 3119.3984, the d2 loss is: -3243.5312, the g loss is: 3246.0, the ae loss is: 0.0063648513, the jacobian loss is:0.10765213\n",
            "This is the iter 8755, the d1 loss is: 3035.5156, the d2 loss is: -3161.75, the g loss is: 3171.8594, the ae loss is: 0.0051177666, the jacobian loss is:0.08436445\n",
            "This is the iter 8756, the d1 loss is: 3222.5625, the d2 loss is: -3363.7188, the g loss is: 3410.0234, the ae loss is: 0.0063717547, the jacobian loss is:0.09470117\n",
            "This is the iter 8757, the d1 loss is: 3026.3438, the d2 loss is: -3140.289, the g loss is: 3178.1094, the ae loss is: 0.00564938, the jacobian loss is:0.11375198\n",
            "This is the iter 8758, the d1 loss is: 3079.0938, the d2 loss is: -3212.25, the g loss is: 3168.1953, the ae loss is: 0.006916303, the jacobian loss is:0.14460525\n",
            "This is the iter 8759, the d1 loss is: 3223.0625, the d2 loss is: -3344.3281, the g loss is: 3365.3281, the ae loss is: 0.007186615, the jacobian loss is:0.06356076\n",
            "This is the iter 8760, the d1 loss is: 3093.6875, the d2 loss is: -3231.586, the g loss is: 3235.0938, the ae loss is: 0.0054749604, the jacobian loss is:0.085562065\n",
            "This is the iter 8761, the d1 loss is: 3047.4688, the d2 loss is: -3146.4531, the g loss is: 3133.2656, the ae loss is: 0.006210226, the jacobian loss is:0.14845096\n",
            "This is the iter 8762, the d1 loss is: 3133.3984, the d2 loss is: -3263.0469, the g loss is: 3241.711, the ae loss is: 0.0053717857, the jacobian loss is:0.09541324\n",
            "This is the iter 8763, the d1 loss is: 3022.5312, the d2 loss is: -3143.1406, the g loss is: 3067.4922, the ae loss is: 0.004575396, the jacobian loss is:0.19287048\n",
            "This is the iter 8764, the d1 loss is: 3053.4375, the d2 loss is: -3192.1094, the g loss is: 3209.7578, the ae loss is: 0.0073529035, the jacobian loss is:0.0919267\n",
            "This is the iter 8765, the d1 loss is: 3306.5547, the d2 loss is: -3411.586, the g loss is: 3450.3906, the ae loss is: 0.006007261, the jacobian loss is:0.09947069\n",
            "This is the iter 8766, the d1 loss is: 3181.8125, the d2 loss is: -3304.875, the g loss is: 3278.5156, the ae loss is: 0.006762919, the jacobian loss is:0.10476762\n",
            "This is the iter 8767, the d1 loss is: 3091.711, the d2 loss is: -3213.4688, the g loss is: 3179.5078, the ae loss is: 0.005603335, the jacobian loss is:0.09822467\n",
            "This is the iter 8768, the d1 loss is: 3129.5312, the d2 loss is: -3265.0625, the g loss is: 3215.6172, the ae loss is: 0.006177299, the jacobian loss is:0.097816184\n",
            "This is the iter 8769, the d1 loss is: 3050.2734, the d2 loss is: -3164.25, the g loss is: 3159.2969, the ae loss is: 0.006400206, the jacobian loss is:0.1102521\n",
            "This is the iter 8770, the d1 loss is: 3263.6094, the d2 loss is: -3397.3672, the g loss is: 3372.1953, the ae loss is: 0.00436475, the jacobian loss is:0.0841263\n",
            "This is the iter 8771, the d1 loss is: 3006.9531, the d2 loss is: -3129.6172, the g loss is: 3125.461, the ae loss is: 0.0048499526, the jacobian loss is:0.10142777\n",
            "This is the iter 8772, the d1 loss is: 3070.8906, the d2 loss is: -3179.9844, the g loss is: 3170.6484, the ae loss is: 0.0062287576, the jacobian loss is:0.0919693\n",
            "This is the iter 8773, the d1 loss is: 3008.6406, the d2 loss is: -3156.836, the g loss is: 3223.461, the ae loss is: 0.005206575, the jacobian loss is:0.13220325\n",
            "This is the iter 8774, the d1 loss is: 3243.625, the d2 loss is: -3360.3281, the g loss is: 3339.1797, the ae loss is: 0.006685287, the jacobian loss is:0.10112312\n",
            "This is the iter 8775, the d1 loss is: 3393.4922, the d2 loss is: -3500.539, the g loss is: 3509.5312, the ae loss is: 0.0070548374, the jacobian loss is:0.1109356\n",
            "This is the iter 8776, the d1 loss is: 2897.8984, the d2 loss is: -3009.8594, the g loss is: 3030.3281, the ae loss is: 0.005841672, the jacobian loss is:0.17115681\n",
            "This is the iter 8777, the d1 loss is: 3190.1875, the d2 loss is: -3299.914, the g loss is: 3235.5156, the ae loss is: 0.006623713, the jacobian loss is:0.13842784\n",
            "This is the iter 8778, the d1 loss is: 3114.9375, the d2 loss is: -3220.3516, the g loss is: 3263.4844, the ae loss is: 0.0056589325, the jacobian loss is:0.10245247\n",
            "This is the iter 8779, the d1 loss is: 2797.3125, the d2 loss is: -2902.2188, the g loss is: 2886.0156, the ae loss is: 0.0061351657, the jacobian loss is:0.10790826\n",
            "This is the iter 8780, the d1 loss is: 3093.4531, the d2 loss is: -3218.4297, the g loss is: 3236.2812, the ae loss is: 0.0047790436, the jacobian loss is:0.10653557\n",
            "This is the iter 8781, the d1 loss is: 3046.8906, the d2 loss is: -3137.9297, the g loss is: 3148.9375, the ae loss is: 0.0057415315, the jacobian loss is:0.089651875\n",
            "This is the iter 8782, the d1 loss is: 3175.1562, the d2 loss is: -3278.0, the g loss is: 3213.3281, the ae loss is: 0.005755092, the jacobian loss is:0.09647408\n",
            "This is the iter 8783, the d1 loss is: 3145.6875, the d2 loss is: -3258.3594, the g loss is: 3252.1094, the ae loss is: 0.007029852, the jacobian loss is:0.15048064\n",
            "This is the iter 8784, the d1 loss is: 3098.5469, the d2 loss is: -3224.086, the g loss is: 3207.7734, the ae loss is: 0.007649107, the jacobian loss is:0.11667355\n",
            "This is the iter 8785, the d1 loss is: 3222.9297, the d2 loss is: -3341.4219, the g loss is: 3352.1953, the ae loss is: 0.006207798, the jacobian loss is:0.10393324\n",
            "This is the iter 8786, the d1 loss is: 3121.7578, the d2 loss is: -3256.711, the g loss is: 3236.9375, the ae loss is: 0.0070235715, the jacobian loss is:0.10748671\n",
            "This is the iter 8787, the d1 loss is: 3059.4219, the d2 loss is: -3174.9844, the g loss is: 3191.0547, the ae loss is: 0.0063583725, the jacobian loss is:0.09205815\n",
            "This is the iter 8788, the d1 loss is: 3072.086, the d2 loss is: -3169.3906, the g loss is: 3166.1953, the ae loss is: 0.0068447255, the jacobian loss is:0.111298434\n",
            "This is the iter 8789, the d1 loss is: 3151.336, the d2 loss is: -3268.9219, the g loss is: 3233.5312, the ae loss is: 0.0048976364, the jacobian loss is:0.10463126\n",
            "This is the iter 8790, the d1 loss is: 3217.375, the d2 loss is: -3319.9219, the g loss is: 3350.6875, the ae loss is: 0.005809516, the jacobian loss is:0.12886293\n",
            "This is the iter 8791, the d1 loss is: 2949.5547, the d2 loss is: -3048.0, the g loss is: 2966.2656, the ae loss is: 0.0076228343, the jacobian loss is:0.15586618\n",
            "This is the iter 8792, the d1 loss is: 3119.875, the d2 loss is: -3225.5469, the g loss is: 3225.6172, the ae loss is: 0.005918492, the jacobian loss is:0.07740799\n",
            "This is the iter 8793, the d1 loss is: 3097.2812, the d2 loss is: -3216.8203, the g loss is: 3267.7188, the ae loss is: 0.0053122686, the jacobian loss is:0.10079652\n",
            "This is the iter 8794, the d1 loss is: 3100.7656, the d2 loss is: -3222.5703, the g loss is: 3216.3125, the ae loss is: 0.006094281, the jacobian loss is:0.076593824\n",
            "This is the iter 8795, the d1 loss is: 3042.1094, the d2 loss is: -3166.25, the g loss is: 3179.3203, the ae loss is: 0.0048120343, the jacobian loss is:0.09709274\n",
            "This is the iter 8796, the d1 loss is: 3058.7734, the d2 loss is: -3168.2188, the g loss is: 3164.0156, the ae loss is: 0.0060859113, the jacobian loss is:0.09902613\n",
            "This is the iter 8797, the d1 loss is: 3037.4688, the d2 loss is: -3163.8281, the g loss is: 3149.3438, the ae loss is: 0.0069362493, the jacobian loss is:0.11050686\n",
            "This is the iter 8798, the d1 loss is: 3220.1406, the d2 loss is: -3354.1016, the g loss is: 3314.7422, the ae loss is: 0.006606453, the jacobian loss is:0.078688405\n",
            "This is the iter 8799, the d1 loss is: 2833.4688, the d2 loss is: -2940.5, the g loss is: 2985.0, the ae loss is: 0.0054026716, the jacobian loss is:0.0780848\n",
            "This is the iter 8800, the d1 loss is: 3128.8984, the d2 loss is: -3226.914, the g loss is: 3244.9531, the ae loss is: 0.0060817096, the jacobian loss is:0.08672198\n",
            "0.27759424\n",
            "1.0634223\n",
            "This is the iter 8801, the d1 loss is: 3262.461, the d2 loss is: -3350.7031, the g loss is: 3415.586, the ae loss is: 0.00817682, the jacobian loss is:0.087674335\n",
            "This is the iter 8802, the d1 loss is: 3053.1094, the d2 loss is: -3157.4844, the g loss is: 3185.086, the ae loss is: 0.0073095416, the jacobian loss is:0.10323652\n",
            "This is the iter 8803, the d1 loss is: 3005.6953, the d2 loss is: -3128.9766, the g loss is: 3121.7969, the ae loss is: 0.007839338, the jacobian loss is:0.14127126\n",
            "This is the iter 8804, the d1 loss is: 3118.9531, the d2 loss is: -3235.4844, the g loss is: 3224.0781, the ae loss is: 0.0041950904, the jacobian loss is:0.08203907\n",
            "This is the iter 8805, the d1 loss is: 3063.586, the d2 loss is: -3181.9688, the g loss is: 3224.875, the ae loss is: 0.0075084493, the jacobian loss is:0.120299116\n",
            "This is the iter 8806, the d1 loss is: 3147.2031, the d2 loss is: -3260.9062, the g loss is: 3291.4766, the ae loss is: 0.0058136554, the jacobian loss is:0.1150925\n",
            "This is the iter 8807, the d1 loss is: 3072.461, the d2 loss is: -3199.3828, the g loss is: 3224.5625, the ae loss is: 0.0074001686, the jacobian loss is:0.1310548\n",
            "This is the iter 8808, the d1 loss is: 3089.625, the d2 loss is: -3232.1797, the g loss is: 3230.0781, the ae loss is: 0.006463064, the jacobian loss is:0.13171838\n",
            "This is the iter 8809, the d1 loss is: 3011.2266, the d2 loss is: -3136.0625, the g loss is: 3132.75, the ae loss is: 0.006532641, the jacobian loss is:0.13301979\n",
            "This is the iter 8810, the d1 loss is: 3117.5, the d2 loss is: -3218.3594, the g loss is: 3187.789, the ae loss is: 0.006815945, the jacobian loss is:0.0862886\n",
            "This is the iter 8811, the d1 loss is: 3015.3906, the d2 loss is: -3147.0, the g loss is: 3179.2344, the ae loss is: 0.0060176146, the jacobian loss is:0.10469358\n",
            "This is the iter 8812, the d1 loss is: 3116.7031, the d2 loss is: -3241.4062, the g loss is: 3185.1172, the ae loss is: 0.0059221108, the jacobian loss is:0.0859218\n",
            "This is the iter 8813, the d1 loss is: 3006.1875, the d2 loss is: -3130.25, the g loss is: 3163.039, the ae loss is: 0.005848871, the jacobian loss is:0.1352138\n",
            "This is the iter 8814, the d1 loss is: 3177.625, the d2 loss is: -3291.4688, the g loss is: 3259.2578, the ae loss is: 0.005269167, the jacobian loss is:0.10485923\n",
            "This is the iter 8815, the d1 loss is: 3128.3984, the d2 loss is: -3224.1953, the g loss is: 3194.8438, the ae loss is: 0.006396936, the jacobian loss is:0.110525616\n",
            "This is the iter 8816, the d1 loss is: 3174.4219, the d2 loss is: -3277.5, the g loss is: 3214.1797, the ae loss is: 0.007319828, the jacobian loss is:0.10366579\n",
            "This is the iter 8817, the d1 loss is: 3120.3281, the d2 loss is: -3247.3438, the g loss is: 3278.6562, the ae loss is: 0.007270352, the jacobian loss is:0.12571529\n",
            "This is the iter 8818, the d1 loss is: 2913.2812, the d2 loss is: -3020.4219, the g loss is: 3028.7656, the ae loss is: 0.00548001, the jacobian loss is:0.0971913\n",
            "This is the iter 8819, the d1 loss is: 3037.4766, the d2 loss is: -3142.3906, the g loss is: 3177.711, the ae loss is: 0.005516125, the jacobian loss is:0.10332275\n",
            "This is the iter 8820, the d1 loss is: 3109.6562, the d2 loss is: -3220.1172, the g loss is: 3202.9531, the ae loss is: 0.0050122, the jacobian loss is:0.096204534\n",
            "This is the iter 8821, the d1 loss is: 3023.0938, the d2 loss is: -3160.4297, the g loss is: 3172.9062, the ae loss is: 0.005319141, the jacobian loss is:0.08748891\n",
            "This is the iter 8822, the d1 loss is: 3010.6562, the d2 loss is: -3124.0938, the g loss is: 3090.0312, the ae loss is: 0.007431491, the jacobian loss is:0.122938804\n",
            "This is the iter 8823, the d1 loss is: 2957.3672, the d2 loss is: -3074.2812, the g loss is: 3182.4219, the ae loss is: 0.005560898, the jacobian loss is:0.139306\n",
            "This is the iter 8824, the d1 loss is: 2601.9844, the d2 loss is: -2705.1719, the g loss is: 2789.7812, the ae loss is: 0.004007389, the jacobian loss is:0.082519114\n",
            "This is the iter 8825, the d1 loss is: 2968.0156, the d2 loss is: -3097.2188, the g loss is: 3130.7188, the ae loss is: 0.006591147, the jacobian loss is:0.086880244\n",
            "This is the iter 8826, the d1 loss is: 3049.0234, the d2 loss is: -3165.4844, the g loss is: 3142.7344, the ae loss is: 0.006894177, the jacobian loss is:0.120180495\n",
            "This is the iter 8827, the d1 loss is: 2934.75, the d2 loss is: -3027.8594, the g loss is: 3014.7188, the ae loss is: 0.0061425124, the jacobian loss is:0.22804892\n",
            "This is the iter 8828, the d1 loss is: 3024.8047, the d2 loss is: -3143.7344, the g loss is: 3141.0469, the ae loss is: 0.0055737747, the jacobian loss is:0.10078\n",
            "This is the iter 8829, the d1 loss is: 2959.8906, the d2 loss is: -3076.3594, the g loss is: 3105.0625, the ae loss is: 0.0053668483, the jacobian loss is:0.11337799\n",
            "This is the iter 8830, the d1 loss is: 2943.3906, the d2 loss is: -3055.1953, the g loss is: 3048.125, the ae loss is: 0.0070115635, the jacobian loss is:0.080581345\n",
            "This is the iter 8831, the d1 loss is: 2856.2656, the d2 loss is: -2995.8906, the g loss is: 2954.7812, the ae loss is: 0.0070941774, the jacobian loss is:0.09496037\n",
            "This is the iter 8832, the d1 loss is: 3126.9375, the d2 loss is: -3264.0781, the g loss is: 3256.6016, the ae loss is: 0.0047988603, the jacobian loss is:0.09626838\n",
            "This is the iter 8833, the d1 loss is: 2926.75, the d2 loss is: -3027.5312, the g loss is: 3029.1797, the ae loss is: 0.004866168, the jacobian loss is:0.099224575\n",
            "This is the iter 8834, the d1 loss is: 3287.1094, the d2 loss is: -3432.25, the g loss is: 3363.0625, the ae loss is: 0.00703645, the jacobian loss is:0.11672378\n",
            "This is the iter 8835, the d1 loss is: 2807.2266, the d2 loss is: -2934.0703, the g loss is: 2930.4297, the ae loss is: 0.0060369726, the jacobian loss is:0.11636647\n",
            "This is the iter 8836, the d1 loss is: 3040.875, the d2 loss is: -3160.6406, the g loss is: 3074.7188, the ae loss is: 0.009092334, the jacobian loss is:0.13273835\n",
            "This is the iter 8837, the d1 loss is: 3095.1094, the d2 loss is: -3210.3281, the g loss is: 3224.2266, the ae loss is: 0.0041350434, the jacobian loss is:0.13706727\n",
            "This is the iter 8838, the d1 loss is: 2841.5312, the d2 loss is: -2954.8203, the g loss is: 2947.7188, the ae loss is: 0.005537889, the jacobian loss is:0.100773245\n",
            "This is the iter 8839, the d1 loss is: 3050.8828, the d2 loss is: -3153.4922, the g loss is: 3104.539, the ae loss is: 0.006986168, the jacobian loss is:0.135696\n",
            "This is the iter 8840, the d1 loss is: 3095.2031, the d2 loss is: -3202.4766, the g loss is: 3269.6875, the ae loss is: 0.0057829833, the jacobian loss is:0.13396145\n",
            "This is the iter 8841, the d1 loss is: 2782.0469, the d2 loss is: -2877.4922, the g loss is: 2867.164, the ae loss is: 0.0059536216, the jacobian loss is:0.09716005\n",
            "This is the iter 8842, the d1 loss is: 3123.836, the d2 loss is: -3245.0156, the g loss is: 3284.6953, the ae loss is: 0.006947103, the jacobian loss is:0.10174954\n",
            "This is the iter 8843, the d1 loss is: 2880.9922, the d2 loss is: -3002.211, the g loss is: 3007.4062, the ae loss is: 0.007380455, the jacobian loss is:0.09902579\n",
            "This is the iter 8844, the d1 loss is: 2958.4375, the d2 loss is: -3080.4688, the g loss is: 3129.1719, the ae loss is: 0.005923665, the jacobian loss is:0.094711564\n",
            "This is the iter 8845, the d1 loss is: 2900.164, the d2 loss is: -2990.4062, the g loss is: 3031.2344, the ae loss is: 0.005634451, the jacobian loss is:0.108163625\n",
            "This is the iter 8846, the d1 loss is: 3032.4531, the d2 loss is: -3149.539, the g loss is: 3076.1172, the ae loss is: 0.0058475737, the jacobian loss is:0.101068534\n",
            "This is the iter 8847, the d1 loss is: 2941.4219, the d2 loss is: -3058.8594, the g loss is: 3040.0469, the ae loss is: 0.005071248, the jacobian loss is:0.10685789\n",
            "This is the iter 8848, the d1 loss is: 3081.2656, the d2 loss is: -3185.3438, the g loss is: 3244.9297, the ae loss is: 0.005762295, the jacobian loss is:0.108226374\n",
            "This is the iter 8849, the d1 loss is: 2949.914, the d2 loss is: -3054.0938, the g loss is: 3060.5938, the ae loss is: 0.0072775306, the jacobian loss is:0.12958144\n",
            "This is the iter 8850, the d1 loss is: 3017.6016, the d2 loss is: -3105.0547, the g loss is: 3128.2734, the ae loss is: 0.0063225464, the jacobian loss is:0.13004477\n",
            "This is the iter 8851, the d1 loss is: 3027.6094, the d2 loss is: -3137.8281, the g loss is: 3167.0078, the ae loss is: 0.0065065846, the jacobian loss is:0.12674043\n",
            "This is the iter 8852, the d1 loss is: 3089.75, the d2 loss is: -3185.6797, the g loss is: 3227.5625, the ae loss is: 0.0050645117, the jacobian loss is:0.07321965\n",
            "This is the iter 8853, the d1 loss is: 2993.9062, the d2 loss is: -3084.4531, the g loss is: 3075.4062, the ae loss is: 0.004854098, the jacobian loss is:0.08414503\n",
            "This is the iter 8854, the d1 loss is: 2901.8672, the d2 loss is: -3034.0312, the g loss is: 3128.25, the ae loss is: 0.0053947885, the jacobian loss is:0.15320894\n",
            "This is the iter 8855, the d1 loss is: 3107.539, the d2 loss is: -3233.2031, the g loss is: 3198.3281, the ae loss is: 0.00637051, the jacobian loss is:0.14525384\n",
            "This is the iter 8856, the d1 loss is: 3050.9688, the d2 loss is: -3168.0078, the g loss is: 3153.5625, the ae loss is: 0.0070813578, the jacobian loss is:0.086792186\n",
            "This is the iter 8857, the d1 loss is: 2619.1562, the d2 loss is: -2727.0312, the g loss is: 2765.1562, the ae loss is: 0.006143743, the jacobian loss is:0.1357058\n",
            "This is the iter 8858, the d1 loss is: 3011.8828, the d2 loss is: -3136.0547, the g loss is: 3166.1484, the ae loss is: 0.0059565324, the jacobian loss is:0.10798532\n",
            "This is the iter 8859, the d1 loss is: 2943.4766, the d2 loss is: -3061.3281, the g loss is: 3083.3906, the ae loss is: 0.0055600647, the jacobian loss is:0.12385574\n",
            "This is the iter 8860, the d1 loss is: 3043.3438, the d2 loss is: -3128.8125, the g loss is: 3064.7812, the ae loss is: 0.0072728866, the jacobian loss is:0.089340866\n",
            "This is the iter 8861, the d1 loss is: 2975.9922, the d2 loss is: -3110.0469, the g loss is: 3112.414, the ae loss is: 0.0061747152, the jacobian loss is:0.11951382\n",
            "This is the iter 8862, the d1 loss is: 3025.0781, the d2 loss is: -3127.2656, the g loss is: 3111.1719, the ae loss is: 0.0072248094, the jacobian loss is:0.10376713\n",
            "This is the iter 8863, the d1 loss is: 3032.2656, the d2 loss is: -3153.3203, the g loss is: 3111.3672, the ae loss is: 0.007998097, the jacobian loss is:0.12444182\n",
            "This is the iter 8864, the d1 loss is: 3090.039, the d2 loss is: -3196.9531, the g loss is: 3170.7812, the ae loss is: 0.0081241885, the jacobian loss is:0.10943124\n",
            "This is the iter 8865, the d1 loss is: 3078.0547, the d2 loss is: -3171.1406, the g loss is: 3147.8438, the ae loss is: 0.0059867827, the jacobian loss is:0.18397687\n",
            "This is the iter 8866, the d1 loss is: 2980.625, the d2 loss is: -3087.625, the g loss is: 3084.9453, the ae loss is: 0.0073086633, the jacobian loss is:0.0818629\n",
            "This is the iter 8867, the d1 loss is: 3034.125, the d2 loss is: -3133.5312, the g loss is: 3138.9297, the ae loss is: 0.0038506142, the jacobian loss is:0.12348756\n",
            "This is the iter 8868, the d1 loss is: 2845.6094, the d2 loss is: -2974.3438, the g loss is: 2962.6094, the ae loss is: 0.0062424666, the jacobian loss is:0.093491375\n",
            "This is the iter 8869, the d1 loss is: 3118.7734, the d2 loss is: -3232.0156, the g loss is: 3254.0, the ae loss is: 0.0041210484, the jacobian loss is:0.09151342\n",
            "This is the iter 8870, the d1 loss is: 3312.2656, the d2 loss is: -3413.2734, the g loss is: 3421.6406, the ae loss is: 0.0044528875, the jacobian loss is:0.09921457\n",
            "This is the iter 8871, the d1 loss is: 3208.5547, the d2 loss is: -3342.7578, the g loss is: 3300.7969, the ae loss is: 0.0053783455, the jacobian loss is:0.09565313\n",
            "This is the iter 8872, the d1 loss is: 2965.1875, the d2 loss is: -3064.3438, the g loss is: 3168.125, the ae loss is: 0.004652845, the jacobian loss is:0.09027156\n",
            "This is the iter 8873, the d1 loss is: 3050.2031, the d2 loss is: -3164.3047, the g loss is: 3137.7188, the ae loss is: 0.006291854, the jacobian loss is:0.13542095\n",
            "This is the iter 8874, the d1 loss is: 3080.9844, the d2 loss is: -3211.5938, the g loss is: 3174.5312, the ae loss is: 0.004791951, the jacobian loss is:0.09463016\n",
            "This is the iter 8875, the d1 loss is: 3069.4766, the d2 loss is: -3169.3594, the g loss is: 3139.1484, the ae loss is: 0.0065349326, the jacobian loss is:0.097847186\n",
            "This is the iter 8876, the d1 loss is: 3129.2031, the d2 loss is: -3234.664, the g loss is: 3245.7266, the ae loss is: 0.0047096163, the jacobian loss is:0.10082625\n",
            "This is the iter 8877, the d1 loss is: 3027.4688, the d2 loss is: -3120.3516, the g loss is: 3141.8125, the ae loss is: 0.005363795, the jacobian loss is:0.08476343\n",
            "This is the iter 8878, the d1 loss is: 3078.8828, the d2 loss is: -3187.1406, the g loss is: 3161.4297, the ae loss is: 0.0049631204, the jacobian loss is:0.10487224\n",
            "This is the iter 8879, the d1 loss is: 3060.7656, the d2 loss is: -3168.9688, the g loss is: 3146.9062, the ae loss is: 0.0032609096, the jacobian loss is:0.12714651\n",
            "This is the iter 8880, the d1 loss is: 3212.8984, the d2 loss is: -3323.2344, the g loss is: 3360.1797, the ae loss is: 0.0065606143, the jacobian loss is:0.08323902\n",
            "This is the iter 8881, the d1 loss is: 2952.0703, the d2 loss is: -3055.1094, the g loss is: 3027.625, the ae loss is: 0.0047126664, the jacobian loss is:0.12456734\n",
            "This is the iter 8882, the d1 loss is: 3086.5625, the d2 loss is: -3211.8906, the g loss is: 3166.2188, the ae loss is: 0.0052297777, the jacobian loss is:0.08356368\n",
            "This is the iter 8883, the d1 loss is: 3102.8203, the d2 loss is: -3201.7734, the g loss is: 3217.5, the ae loss is: 0.0041172137, the jacobian loss is:0.12706456\n",
            "This is the iter 8884, the d1 loss is: 3265.9531, the d2 loss is: -3382.6875, the g loss is: 3385.9062, the ae loss is: 0.007382876, the jacobian loss is:0.11854525\n",
            "This is the iter 8885, the d1 loss is: 3205.7422, the d2 loss is: -3298.1328, the g loss is: 3281.586, the ae loss is: 0.006920662, the jacobian loss is:0.17491373\n",
            "This is the iter 8886, the d1 loss is: 2957.3594, the d2 loss is: -3081.9375, the g loss is: 3113.2969, the ae loss is: 0.0039479104, the jacobian loss is:0.06524621\n",
            "This is the iter 8887, the d1 loss is: 2820.5469, the d2 loss is: -2933.9453, the g loss is: 2918.5781, the ae loss is: 0.0050051343, the jacobian loss is:0.16024162\n",
            "This is the iter 8888, the d1 loss is: 3040.2656, the d2 loss is: -3154.125, the g loss is: 3124.6016, the ae loss is: 0.0059194826, the jacobian loss is:0.10281503\n",
            "This is the iter 8889, the d1 loss is: 2860.0156, the d2 loss is: -2988.3984, the g loss is: 2958.6875, the ae loss is: 0.0047238967, the jacobian loss is:0.09544448\n",
            "This is the iter 8890, the d1 loss is: 3074.5078, the d2 loss is: -3164.914, the g loss is: 3145.1094, the ae loss is: 0.00909671, the jacobian loss is:0.10418823\n",
            "This is the iter 8891, the d1 loss is: 2993.1094, the d2 loss is: -3110.914, the g loss is: 3086.6719, the ae loss is: 0.005900066, the jacobian loss is:0.10941483\n",
            "This is the iter 8892, the d1 loss is: 3105.789, the d2 loss is: -3222.8516, the g loss is: 3179.336, the ae loss is: 0.0047170497, the jacobian loss is:0.11316223\n",
            "This is the iter 8893, the d1 loss is: 3058.4922, the d2 loss is: -3173.914, the g loss is: 3136.25, the ae loss is: 0.0046740635, the jacobian loss is:0.07884287\n",
            "This is the iter 8894, the d1 loss is: 3218.0703, the d2 loss is: -3344.25, the g loss is: 3340.7578, the ae loss is: 0.0051299026, the jacobian loss is:0.0828974\n",
            "This is the iter 8895, the d1 loss is: 3097.3516, the d2 loss is: -3182.5625, the g loss is: 3137.6875, the ae loss is: 0.006595887, the jacobian loss is:0.1299776\n",
            "This is the iter 8896, the d1 loss is: 2915.8047, the d2 loss is: -3036.2031, the g loss is: 3038.9844, the ae loss is: 0.0041281315, the jacobian loss is:0.08803359\n",
            "This is the iter 8897, the d1 loss is: 3014.3438, the d2 loss is: -3126.0469, the g loss is: 3158.1328, the ae loss is: 0.00501189, the jacobian loss is:0.0975097\n",
            "This is the iter 8898, the d1 loss is: 3131.4453, the d2 loss is: -3241.2422, the g loss is: 3153.7422, the ae loss is: 0.004944628, the jacobian loss is:0.11191136\n",
            "This is the iter 8899, the d1 loss is: 3034.4766, the d2 loss is: -3150.1094, the g loss is: 3112.2656, the ae loss is: 0.0052138604, the jacobian loss is:0.13929483\n",
            "This is the iter 8900, the d1 loss is: 3005.1562, the d2 loss is: -3099.9219, the g loss is: 3138.6719, the ae loss is: 0.0054854155, the jacobian loss is:0.10041205\n",
            "0.27069116\n",
            "1.0173322\n",
            "This is the iter 8901, the d1 loss is: 3023.9375, the d2 loss is: -3141.2266, the g loss is: 3242.7812, the ae loss is: 0.0047797156, the jacobian loss is:0.09822367\n",
            "This is the iter 8902, the d1 loss is: 3186.8438, the d2 loss is: -3300.1016, the g loss is: 3333.7578, the ae loss is: 0.0057117012, the jacobian loss is:0.1023791\n",
            "This is the iter 8903, the d1 loss is: 2787.6875, the d2 loss is: -2906.6875, the g loss is: 2900.6328, the ae loss is: 0.004668674, the jacobian loss is:0.10420425\n",
            "This is the iter 8904, the d1 loss is: 3209.1484, the d2 loss is: -3318.4922, the g loss is: 3318.0312, the ae loss is: 0.0070278663, the jacobian loss is:0.08145526\n",
            "This is the iter 8905, the d1 loss is: 3105.8672, the d2 loss is: -3197.8281, the g loss is: 3202.9844, the ae loss is: 0.0061210375, the jacobian loss is:0.09880475\n",
            "This is the iter 8906, the d1 loss is: 3281.8516, the d2 loss is: -3377.0078, the g loss is: 3352.2188, the ae loss is: 0.0045458823, the jacobian loss is:0.07527234\n",
            "This is the iter 8907, the d1 loss is: 3032.6016, the d2 loss is: -3142.1719, the g loss is: 3119.5312, the ae loss is: 0.006148787, the jacobian loss is:0.11485765\n",
            "This is the iter 8908, the d1 loss is: 3218.3984, the d2 loss is: -3352.5078, the g loss is: 3355.7031, the ae loss is: 0.004661765, the jacobian loss is:0.08548786\n",
            "This is the iter 8909, the d1 loss is: 3078.4844, the d2 loss is: -3192.8594, the g loss is: 3130.3828, the ae loss is: 0.004670416, the jacobian loss is:0.11328491\n",
            "This is the iter 8910, the d1 loss is: 3362.039, the d2 loss is: -3482.4688, the g loss is: 3482.0703, the ae loss is: 0.006266266, the jacobian loss is:0.072493926\n",
            "This is the iter 8911, the d1 loss is: 2971.125, the d2 loss is: -3066.8828, the g loss is: 3061.914, the ae loss is: 0.0061017377, the jacobian loss is:0.11328212\n",
            "This is the iter 8912, the d1 loss is: 3111.9375, the d2 loss is: -3217.7812, the g loss is: 3161.0234, the ae loss is: 0.008893424, the jacobian loss is:0.073572606\n",
            "This is the iter 8913, the d1 loss is: 3043.0547, the d2 loss is: -3164.4297, the g loss is: 3154.1875, the ae loss is: 0.004363106, the jacobian loss is:0.0872215\n",
            "This is the iter 8914, the d1 loss is: 3279.461, the d2 loss is: -3395.9219, the g loss is: 3341.6719, the ae loss is: 0.0065221707, the jacobian loss is:0.06785803\n",
            "This is the iter 8915, the d1 loss is: 2898.414, the d2 loss is: -3021.4844, the g loss is: 2993.9922, the ae loss is: 0.0043232455, the jacobian loss is:0.15772308\n",
            "This is the iter 8916, the d1 loss is: 2891.461, the d2 loss is: -2991.9062, the g loss is: 2967.6484, the ae loss is: 0.005429607, the jacobian loss is:0.06941449\n",
            "This is the iter 8917, the d1 loss is: 3049.8828, the d2 loss is: -3165.3516, the g loss is: 3129.3281, the ae loss is: 0.0068099806, the jacobian loss is:0.10443689\n",
            "This is the iter 8918, the d1 loss is: 3071.586, the d2 loss is: -3203.0625, the g loss is: 3163.8984, the ae loss is: 0.0064425785, the jacobian loss is:0.07240944\n",
            "This is the iter 8919, the d1 loss is: 2978.3594, the d2 loss is: -3103.9688, the g loss is: 3106.5938, the ae loss is: 0.004460951, the jacobian loss is:0.08971001\n",
            "This is the iter 8920, the d1 loss is: 3352.5156, the d2 loss is: -3460.2734, the g loss is: 3497.2812, the ae loss is: 0.0069851447, the jacobian loss is:0.0772992\n",
            "This is the iter 8921, the d1 loss is: 3102.4922, the d2 loss is: -3207.9766, the g loss is: 3190.625, the ae loss is: 0.007993052, the jacobian loss is:0.102707095\n",
            "This is the iter 8922, the d1 loss is: 3129.7031, the d2 loss is: -3238.5156, the g loss is: 3203.6094, the ae loss is: 0.0041122357, the jacobian loss is:0.061463375\n",
            "This is the iter 8923, the d1 loss is: 3379.9375, the d2 loss is: -3478.7969, the g loss is: 3433.8438, the ae loss is: 0.0065222066, the jacobian loss is:0.13944604\n",
            "This is the iter 8924, the d1 loss is: 3044.8516, the d2 loss is: -3129.414, the g loss is: 3090.2188, the ae loss is: 0.006600404, the jacobian loss is:0.08888513\n",
            "This is the iter 8925, the d1 loss is: 3131.6406, the d2 loss is: -3245.3672, the g loss is: 3243.086, the ae loss is: 0.005284465, the jacobian loss is:0.09114393\n",
            "This is the iter 8926, the d1 loss is: 3114.875, the d2 loss is: -3207.9922, the g loss is: 3246.8672, the ae loss is: 0.008125175, the jacobian loss is:0.07857696\n",
            "This is the iter 8927, the d1 loss is: 3180.8828, the d2 loss is: -3265.4688, the g loss is: 3180.3906, the ae loss is: 0.006700806, the jacobian loss is:0.17263775\n",
            "This is the iter 8928, the d1 loss is: 3172.8438, the d2 loss is: -3295.1406, the g loss is: 3269.211, the ae loss is: 0.005914192, the jacobian loss is:0.07634691\n",
            "This is the iter 8929, the d1 loss is: 3183.414, the d2 loss is: -3290.125, the g loss is: 3312.6094, the ae loss is: 0.005300804, the jacobian loss is:0.10345538\n",
            "This is the iter 8930, the d1 loss is: 3066.7031, the d2 loss is: -3188.3516, the g loss is: 3178.2734, the ae loss is: 0.0051369267, the jacobian loss is:0.07337567\n",
            "This is the iter 8931, the d1 loss is: 3057.5156, the d2 loss is: -3169.9062, the g loss is: 3153.1406, the ae loss is: 0.0058441646, the jacobian loss is:0.13528827\n",
            "This is the iter 8932, the d1 loss is: 3111.8594, the d2 loss is: -3196.789, the g loss is: 3234.4531, the ae loss is: 0.0064280154, the jacobian loss is:0.11721553\n",
            "This is the iter 8933, the d1 loss is: 3098.3594, the d2 loss is: -3214.414, the g loss is: 3229.8906, the ae loss is: 0.005177968, the jacobian loss is:0.14382285\n",
            "This is the iter 8934, the d1 loss is: 3122.9375, the d2 loss is: -3236.3125, the g loss is: 3247.7188, the ae loss is: 0.0072690994, the jacobian loss is:0.096940026\n",
            "This is the iter 8935, the d1 loss is: 3130.789, the d2 loss is: -3258.4453, the g loss is: 3220.9297, the ae loss is: 0.006216514, the jacobian loss is:0.11391653\n",
            "This is the iter 8936, the d1 loss is: 3141.586, the d2 loss is: -3248.7031, the g loss is: 3237.625, the ae loss is: 0.0059856623, the jacobian loss is:0.09513787\n",
            "This is the iter 8937, the d1 loss is: 3090.6562, the d2 loss is: -3211.7969, the g loss is: 3196.3203, the ae loss is: 0.0051742448, the jacobian loss is:0.08837031\n",
            "This is the iter 8938, the d1 loss is: 3409.1094, the d2 loss is: -3489.1328, the g loss is: 3459.4766, the ae loss is: 0.005614457, the jacobian loss is:0.098566666\n",
            "This is the iter 8939, the d1 loss is: 3037.1797, the d2 loss is: -3157.9688, the g loss is: 3131.1875, the ae loss is: 0.005746466, the jacobian loss is:0.07062275\n",
            "This is the iter 8940, the d1 loss is: 3201.0312, the d2 loss is: -3309.3828, the g loss is: 3262.2734, the ae loss is: 0.0074331285, the jacobian loss is:0.10897882\n",
            "This is the iter 8941, the d1 loss is: 3063.375, the d2 loss is: -3196.5234, the g loss is: 3168.3125, the ae loss is: 0.0048052133, the jacobian loss is:0.2097897\n",
            "This is the iter 8942, the d1 loss is: 3108.0547, the d2 loss is: -3241.1875, the g loss is: 3185.6797, the ae loss is: 0.0049583963, the jacobian loss is:0.09176558\n",
            "This is the iter 8943, the d1 loss is: 3126.2188, the d2 loss is: -3251.1875, the g loss is: 3234.9375, the ae loss is: 0.004982888, the jacobian loss is:0.10027748\n",
            "This is the iter 8944, the d1 loss is: 3053.836, the d2 loss is: -3178.25, the g loss is: 3116.0469, the ae loss is: 0.007264517, the jacobian loss is:0.1566581\n",
            "This is the iter 8945, the d1 loss is: 3047.4766, the d2 loss is: -3155.9375, the g loss is: 3195.5312, the ae loss is: 0.00627229, the jacobian loss is:0.069846556\n",
            "This is the iter 8946, the d1 loss is: 3122.4766, the d2 loss is: -3274.6953, the g loss is: 3284.0781, the ae loss is: 0.0061712805, the jacobian loss is:0.103376076\n",
            "This is the iter 8947, the d1 loss is: 3093.125, the d2 loss is: -3211.836, the g loss is: 3184.0, the ae loss is: 0.0049808724, the jacobian loss is:0.07862915\n",
            "This is the iter 8948, the d1 loss is: 3085.5469, the d2 loss is: -3216.25, the g loss is: 3235.75, the ae loss is: 0.0059115095, the jacobian loss is:0.081377015\n",
            "This is the iter 8949, the d1 loss is: 2872.9688, the d2 loss is: -2974.25, the g loss is: 2945.25, the ae loss is: 0.005250394, the jacobian loss is:0.071213655\n",
            "This is the iter 8950, the d1 loss is: 3064.0, the d2 loss is: -3162.6875, the g loss is: 3247.414, the ae loss is: 0.0054227025, the jacobian loss is:0.06867739\n",
            "This is the iter 8951, the d1 loss is: 3109.2969, the d2 loss is: -3248.4844, the g loss is: 3238.5703, the ae loss is: 0.006297935, the jacobian loss is:0.07275341\n",
            "This is the iter 8952, the d1 loss is: 2958.6797, the d2 loss is: -3069.5234, the g loss is: 3052.0469, the ae loss is: 0.0057503204, the jacobian loss is:0.08770904\n",
            "This is the iter 8953, the d1 loss is: 3039.586, the d2 loss is: -3138.586, the g loss is: 3167.9297, the ae loss is: 0.00564326, the jacobian loss is:0.14788853\n",
            "This is the iter 8954, the d1 loss is: 3124.5234, the d2 loss is: -3250.4062, the g loss is: 3236.1719, the ae loss is: 0.0063767913, the jacobian loss is:0.111330874\n",
            "This is the iter 8955, the d1 loss is: 3009.586, the d2 loss is: -3114.5938, the g loss is: 3123.7578, the ae loss is: 0.009468671, the jacobian loss is:0.08260886\n",
            "This is the iter 8956, the d1 loss is: 3266.4219, the d2 loss is: -3387.3438, the g loss is: 3400.0, the ae loss is: 0.0073542977, the jacobian loss is:0.118745975\n",
            "This is the iter 8957, the d1 loss is: 3021.8203, the d2 loss is: -3152.7031, the g loss is: 3129.8672, the ae loss is: 0.007331532, the jacobian loss is:0.16015935\n",
            "This is the iter 8958, the d1 loss is: 3013.711, the d2 loss is: -3134.1797, the g loss is: 3158.4844, the ae loss is: 0.0055342773, the jacobian loss is:0.08005299\n",
            "This is the iter 8959, the d1 loss is: 3090.0078, the d2 loss is: -3183.4766, the g loss is: 3164.1562, the ae loss is: 0.005935922, the jacobian loss is:0.12430037\n",
            "This is the iter 8960, the d1 loss is: 3058.336, the d2 loss is: -3177.6953, the g loss is: 3113.1406, the ae loss is: 0.0065555926, the jacobian loss is:0.13384767\n",
            "This is the iter 8961, the d1 loss is: 3148.4219, the d2 loss is: -3253.5547, the g loss is: 3285.3672, the ae loss is: 0.0066651353, the jacobian loss is:0.093275525\n",
            "This is the iter 8962, the d1 loss is: 2925.336, the d2 loss is: -3028.7031, the g loss is: 3057.625, the ae loss is: 0.0060057756, the jacobian loss is:0.08815276\n",
            "This is the iter 8963, the d1 loss is: 3241.6719, the d2 loss is: -3367.1875, the g loss is: 3357.289, the ae loss is: 0.006086602, the jacobian loss is:0.121057786\n",
            "This is the iter 8964, the d1 loss is: 3279.2656, the d2 loss is: -3398.2188, the g loss is: 3389.836, the ae loss is: 0.007451873, the jacobian loss is:0.13751915\n",
            "This is the iter 8965, the d1 loss is: 3045.9375, the d2 loss is: -3152.8203, the g loss is: 3209.0547, the ae loss is: 0.0054266164, the jacobian loss is:0.15368377\n",
            "This is the iter 8966, the d1 loss is: 2982.4688, the d2 loss is: -3077.711, the g loss is: 3084.086, the ae loss is: 0.008985706, the jacobian loss is:0.081653036\n",
            "This is the iter 8967, the d1 loss is: 3081.3906, the d2 loss is: -3200.3125, the g loss is: 3179.4844, the ae loss is: 0.0057197222, the jacobian loss is:0.12342907\n",
            "This is the iter 8968, the d1 loss is: 3132.3906, the d2 loss is: -3231.1875, the g loss is: 3295.0781, the ae loss is: 0.0079802405, the jacobian loss is:0.09236863\n",
            "This is the iter 8969, the d1 loss is: 3156.5625, the d2 loss is: -3278.3125, the g loss is: 3291.3984, the ae loss is: 0.0045071687, the jacobian loss is:0.13647757\n",
            "This is the iter 8970, the d1 loss is: 2956.3906, the d2 loss is: -3051.2812, the g loss is: 3073.289, the ae loss is: 0.0062381206, the jacobian loss is:0.09089602\n",
            "This is the iter 8971, the d1 loss is: 3061.3906, the d2 loss is: -3165.4531, the g loss is: 3087.6406, the ae loss is: 0.007886683, the jacobian loss is:0.11426243\n",
            "This is the iter 8972, the d1 loss is: 3061.2344, the d2 loss is: -3184.664, the g loss is: 3172.5625, the ae loss is: 0.006438546, the jacobian loss is:0.078722894\n",
            "This is the iter 8973, the d1 loss is: 3013.5938, the d2 loss is: -3118.5, the g loss is: 3101.1797, the ae loss is: 0.0070224633, the jacobian loss is:0.09333045\n",
            "This is the iter 8974, the d1 loss is: 3029.5312, the d2 loss is: -3150.6484, the g loss is: 3148.0, the ae loss is: 0.005291854, the jacobian loss is:0.09662896\n",
            "This is the iter 8975, the d1 loss is: 3039.3984, the d2 loss is: -3132.9531, the g loss is: 3184.4453, the ae loss is: 0.0045754947, the jacobian loss is:0.14054686\n",
            "This is the iter 8976, the d1 loss is: 3069.3281, the d2 loss is: -3180.7344, the g loss is: 3315.2578, the ae loss is: 0.005716775, the jacobian loss is:0.074709125\n",
            "This is the iter 8977, the d1 loss is: 3104.086, the d2 loss is: -3224.9531, the g loss is: 3212.1875, the ae loss is: 0.0048778336, the jacobian loss is:0.11048676\n",
            "This is the iter 8978, the d1 loss is: 3399.9062, the d2 loss is: -3515.6406, the g loss is: 3488.9453, the ae loss is: 0.0074366718, the jacobian loss is:0.09593277\n",
            "This is the iter 8979, the d1 loss is: 2791.414, the d2 loss is: -2917.2031, the g loss is: 2875.6953, the ae loss is: 0.006016944, the jacobian loss is:0.08800514\n",
            "This is the iter 8980, the d1 loss is: 3142.6953, the d2 loss is: -3253.125, the g loss is: 3210.25, the ae loss is: 0.007677807, the jacobian loss is:0.087119885\n",
            "This is the iter 8981, the d1 loss is: 2985.3906, the d2 loss is: -3133.2266, the g loss is: 3133.9844, the ae loss is: 0.005204303, the jacobian loss is:0.118563354\n",
            "This is the iter 8982, the d1 loss is: 3075.1953, the d2 loss is: -3192.2969, the g loss is: 3226.0781, the ae loss is: 0.0067782896, the jacobian loss is:0.08145056\n",
            "This is the iter 8983, the d1 loss is: 3173.1562, the d2 loss is: -3277.5312, the g loss is: 3247.3281, the ae loss is: 0.006556951, the jacobian loss is:0.103884615\n",
            "This is the iter 8984, the d1 loss is: 3287.9922, the d2 loss is: -3400.0, the g loss is: 3364.1562, the ae loss is: 0.0070574638, the jacobian loss is:0.099810384\n",
            "This is the iter 8985, the d1 loss is: 3023.1328, the d2 loss is: -3161.25, the g loss is: 3122.5938, the ae loss is: 0.0065957885, the jacobian loss is:0.14564042\n",
            "This is the iter 8986, the d1 loss is: 3105.211, the d2 loss is: -3200.164, the g loss is: 3209.8438, the ae loss is: 0.0059112674, the jacobian loss is:0.06706447\n",
            "This is the iter 8987, the d1 loss is: 3201.7422, the d2 loss is: -3308.914, the g loss is: 3412.5547, the ae loss is: 0.0055496907, the jacobian loss is:0.10550459\n",
            "This is the iter 8988, the d1 loss is: 3068.3125, the d2 loss is: -3165.9062, the g loss is: 3211.836, the ae loss is: 0.005196034, the jacobian loss is:0.08261203\n",
            "This is the iter 8989, the d1 loss is: 3010.3594, the d2 loss is: -3109.1953, the g loss is: 3142.539, the ae loss is: 0.0037829597, the jacobian loss is:0.13329794\n",
            "This is the iter 8990, the d1 loss is: 3098.625, the d2 loss is: -3214.664, the g loss is: 3230.3828, the ae loss is: 0.0069348337, the jacobian loss is:0.07438122\n",
            "This is the iter 8991, the d1 loss is: 3016.875, the d2 loss is: -3129.3281, the g loss is: 3137.9688, the ae loss is: 0.0058717383, the jacobian loss is:0.19169821\n",
            "This is the iter 8992, the d1 loss is: 3106.8281, the d2 loss is: -3228.586, the g loss is: 3242.2266, the ae loss is: 0.005482519, the jacobian loss is:0.0837676\n",
            "This is the iter 8993, the d1 loss is: 2974.3203, the d2 loss is: -3075.8828, the g loss is: 3161.961, the ae loss is: 0.005187394, the jacobian loss is:0.09251682\n",
            "This is the iter 8994, the d1 loss is: 3072.3984, the d2 loss is: -3199.2344, the g loss is: 3206.2578, the ae loss is: 0.004938637, the jacobian loss is:0.104681045\n",
            "This is the iter 8995, the d1 loss is: 2938.3125, the d2 loss is: -3033.3281, the g loss is: 3082.7969, the ae loss is: 0.004331302, the jacobian loss is:0.102147736\n",
            "This is the iter 8996, the d1 loss is: 3098.6406, the d2 loss is: -3211.9844, the g loss is: 3229.2734, the ae loss is: 0.0053244648, the jacobian loss is:0.08363829\n",
            "This is the iter 8997, the d1 loss is: 2744.6875, the d2 loss is: -2870.664, the g loss is: 2902.9375, the ae loss is: 0.0036221342, the jacobian loss is:0.098207735\n",
            "This is the iter 8998, the d1 loss is: 3151.9219, the d2 loss is: -3263.789, the g loss is: 3271.0938, the ae loss is: 0.008647348, the jacobian loss is:0.079545856\n",
            "This is the iter 8999, the d1 loss is: 2919.5938, the d2 loss is: -3008.9766, the g loss is: 2983.1094, the ae loss is: 0.0057053636, the jacobian loss is:0.07047018\n",
            "This is the iter 9000, the d1 loss is: 3335.5234, the d2 loss is: -3476.6328, the g loss is: 3402.8047, the ae loss is: 0.006874581, the jacobian loss is:0.1179786\n",
            "0.26969653\n",
            "1.0106093\n",
            "This is the iter 9001, the d1 loss is: 2952.6953, the d2 loss is: -3054.039, the g loss is: 3119.5312, the ae loss is: 0.005849543, the jacobian loss is:0.13443601\n",
            "This is the iter 9002, the d1 loss is: 3098.4766, the d2 loss is: -3238.9844, the g loss is: 3191.6875, the ae loss is: 0.0076516103, the jacobian loss is:0.08934333\n",
            "This is the iter 9003, the d1 loss is: 2986.5234, the d2 loss is: -3098.9453, the g loss is: 3110.6094, the ae loss is: 0.004816345, the jacobian loss is:0.08163895\n",
            "This is the iter 9004, the d1 loss is: 3077.0469, the d2 loss is: -3176.3281, the g loss is: 3154.6406, the ae loss is: 0.0050905095, the jacobian loss is:0.08309292\n",
            "This is the iter 9005, the d1 loss is: 3033.3125, the d2 loss is: -3139.5, the g loss is: 3144.1953, the ae loss is: 0.00517922, the jacobian loss is:0.10325676\n",
            "This is the iter 9006, the d1 loss is: 2934.5781, the d2 loss is: -3054.3828, the g loss is: 3067.125, the ae loss is: 0.007127017, the jacobian loss is:0.08312379\n",
            "This is the iter 9007, the d1 loss is: 2919.4531, the d2 loss is: -3000.8516, the g loss is: 3066.289, the ae loss is: 0.0054040113, the jacobian loss is:0.10451093\n",
            "This is the iter 9008, the d1 loss is: 2951.664, the d2 loss is: -3074.2031, the g loss is: 3060.625, the ae loss is: 0.005824767, the jacobian loss is:0.082283214\n",
            "This is the iter 9009, the d1 loss is: 2830.0078, the d2 loss is: -2943.6328, the g loss is: 2966.0781, the ae loss is: 0.006228455, the jacobian loss is:0.09399267\n",
            "This is the iter 9010, the d1 loss is: 3144.6172, the d2 loss is: -3241.7656, the g loss is: 3194.1719, the ae loss is: 0.007775342, the jacobian loss is:0.09389815\n",
            "This is the iter 9011, the d1 loss is: 3053.9531, the d2 loss is: -3166.5938, the g loss is: 3083.5156, the ae loss is: 0.005883088, the jacobian loss is:0.09185312\n",
            "This is the iter 9012, the d1 loss is: 3182.1406, the d2 loss is: -3301.0, the g loss is: 3279.3906, the ae loss is: 0.006106469, the jacobian loss is:0.089394175\n",
            "This is the iter 9013, the d1 loss is: 2838.2422, the d2 loss is: -2951.5469, the g loss is: 2984.9375, the ae loss is: 0.0073842183, the jacobian loss is:0.11127624\n",
            "This is the iter 9014, the d1 loss is: 3254.0312, the d2 loss is: -3365.9766, the g loss is: 3347.961, the ae loss is: 0.006813914, the jacobian loss is:0.085833035\n",
            "This is the iter 9015, the d1 loss is: 2795.6797, the d2 loss is: -2917.7266, the g loss is: 2893.211, the ae loss is: 0.008667301, the jacobian loss is:0.11067431\n",
            "This is the iter 9016, the d1 loss is: 3070.0312, the d2 loss is: -3177.9062, the g loss is: 3146.5156, the ae loss is: 0.0055931127, the jacobian loss is:0.06907269\n",
            "This is the iter 9017, the d1 loss is: 2950.2969, the d2 loss is: -3076.9688, the g loss is: 3065.1094, the ae loss is: 0.005965991, the jacobian loss is:0.15882702\n",
            "This is the iter 9018, the d1 loss is: 2930.5938, the d2 loss is: -3018.1016, the g loss is: 3031.9531, the ae loss is: 0.007146485, the jacobian loss is:0.10472537\n",
            "This is the iter 9019, the d1 loss is: 2812.0, the d2 loss is: -2915.5312, the g loss is: 2891.5156, the ae loss is: 0.006876888, the jacobian loss is:0.114560015\n",
            "This is the iter 9020, the d1 loss is: 3069.7031, the d2 loss is: -3192.7734, the g loss is: 3170.75, the ae loss is: 0.0059905783, the jacobian loss is:0.11544386\n",
            "This is the iter 9021, the d1 loss is: 2988.9453, the d2 loss is: -3092.6016, the g loss is: 3112.7422, the ae loss is: 0.00601821, the jacobian loss is:0.15511015\n",
            "This is the iter 9022, the d1 loss is: 3075.5625, the d2 loss is: -3191.7422, the g loss is: 3212.1719, the ae loss is: 0.005565566, the jacobian loss is:0.13576828\n",
            "This is the iter 9023, the d1 loss is: 2740.711, the d2 loss is: -2862.8672, the g loss is: 2852.6094, the ae loss is: 0.0065783467, the jacobian loss is:0.12781033\n",
            "This is the iter 9024, the d1 loss is: 3043.5469, the d2 loss is: -3156.3906, the g loss is: 3133.5312, the ae loss is: 0.006380391, the jacobian loss is:0.06464008\n",
            "This is the iter 9025, the d1 loss is: 3059.1328, the d2 loss is: -3163.8516, the g loss is: 3146.8828, the ae loss is: 0.0043990044, the jacobian loss is:0.13863434\n",
            "This is the iter 9026, the d1 loss is: 3100.9062, the d2 loss is: -3230.086, the g loss is: 3240.9219, the ae loss is: 0.005718766, the jacobian loss is:0.07601156\n",
            "This is the iter 9027, the d1 loss is: 3250.6875, the d2 loss is: -3375.1562, the g loss is: 3426.125, the ae loss is: 0.004113486, the jacobian loss is:0.16496252\n",
            "This is the iter 9028, the d1 loss is: 3095.4531, the d2 loss is: -3195.125, the g loss is: 3268.1484, the ae loss is: 0.0046281293, the jacobian loss is:0.084505856\n",
            "This is the iter 9029, the d1 loss is: 3004.4297, the d2 loss is: -3116.789, the g loss is: 3152.7578, the ae loss is: 0.0053678313, the jacobian loss is:0.09398968\n",
            "This is the iter 9030, the d1 loss is: 3060.1562, the d2 loss is: -3146.1406, the g loss is: 3143.414, the ae loss is: 0.007353178, the jacobian loss is:0.090986274\n",
            "This is the iter 9031, the d1 loss is: 2987.0469, the d2 loss is: -3078.8828, the g loss is: 3135.2969, the ae loss is: 0.004525072, the jacobian loss is:0.10114495\n",
            "This is the iter 9032, the d1 loss is: 3016.3906, the d2 loss is: -3125.2422, the g loss is: 3126.4844, the ae loss is: 0.0049633817, the jacobian loss is:0.08696316\n",
            "This is the iter 9033, the d1 loss is: 3013.2578, the d2 loss is: -3128.6875, the g loss is: 3120.8438, the ae loss is: 0.0050252387, the jacobian loss is:0.11389483\n",
            "This is the iter 9034, the d1 loss is: 3212.2812, the d2 loss is: -3335.914, the g loss is: 3318.9062, the ae loss is: 0.008055476, the jacobian loss is:0.07022479\n",
            "This is the iter 9035, the d1 loss is: 3019.6406, the d2 loss is: -3123.5938, the g loss is: 3108.5469, the ae loss is: 0.004615319, the jacobian loss is:0.12132003\n",
            "This is the iter 9036, the d1 loss is: 2842.5781, the d2 loss is: -2961.789, the g loss is: 2925.9531, the ae loss is: 0.0040078512, the jacobian loss is:0.10187193\n",
            "This is the iter 9037, the d1 loss is: 3106.3516, the d2 loss is: -3206.3047, the g loss is: 3150.875, the ae loss is: 0.005600104, the jacobian loss is:0.14454383\n",
            "This is the iter 9038, the d1 loss is: 3076.1953, the d2 loss is: -3171.3516, the g loss is: 3203.664, the ae loss is: 0.0067635933, the jacobian loss is:0.10390141\n",
            "This is the iter 9039, the d1 loss is: 2825.2812, the d2 loss is: -2928.3047, the g loss is: 2924.4844, the ae loss is: 0.0042983554, the jacobian loss is:0.15438384\n",
            "This is the iter 9040, the d1 loss is: 3112.5625, the d2 loss is: -3220.4219, the g loss is: 3205.7266, the ae loss is: 0.004874847, the jacobian loss is:0.078545965\n",
            "This is the iter 9041, the d1 loss is: 2992.5938, the d2 loss is: -3091.7422, the g loss is: 3150.2969, the ae loss is: 0.0044826064, the jacobian loss is:0.10676242\n",
            "This is the iter 9042, the d1 loss is: 2816.961, the d2 loss is: -2933.1719, the g loss is: 2906.836, the ae loss is: 0.006104014, the jacobian loss is:0.093854174\n",
            "This is the iter 9043, the d1 loss is: 3075.3516, the d2 loss is: -3200.3203, the g loss is: 3214.2969, the ae loss is: 0.0046748687, the jacobian loss is:0.09557971\n",
            "This is the iter 9044, the d1 loss is: 2978.2031, the d2 loss is: -3095.0156, the g loss is: 3108.2969, the ae loss is: 0.0047819996, the jacobian loss is:0.07356063\n",
            "This is the iter 9045, the d1 loss is: 2633.836, the d2 loss is: -2743.7734, the g loss is: 2826.8516, the ae loss is: 0.0048787436, the jacobian loss is:0.14987554\n",
            "This is the iter 9046, the d1 loss is: 3200.664, the d2 loss is: -3309.9922, the g loss is: 3319.1406, the ae loss is: 0.0047488725, the jacobian loss is:0.08949512\n",
            "This is the iter 9047, the d1 loss is: 2841.0703, the d2 loss is: -2943.3594, the g loss is: 2937.6406, the ae loss is: 0.005927889, the jacobian loss is:0.13053647\n",
            "This is the iter 9048, the d1 loss is: 3228.8125, the d2 loss is: -3346.2969, the g loss is: 3368.5312, the ae loss is: 0.005445808, the jacobian loss is:0.08626631\n",
            "This is the iter 9049, the d1 loss is: 2982.711, the d2 loss is: -3066.5938, the g loss is: 3058.5234, the ae loss is: 0.0041012713, the jacobian loss is:0.08350229\n",
            "This is the iter 9050, the d1 loss is: 3114.9922, the d2 loss is: -3217.5156, the g loss is: 3274.8203, the ae loss is: 0.00788757, the jacobian loss is:0.12427168\n",
            "This is the iter 9051, the d1 loss is: 3066.5547, the d2 loss is: -3177.1953, the g loss is: 3100.6094, the ae loss is: 0.0043429146, the jacobian loss is:0.1273741\n",
            "This is the iter 9052, the d1 loss is: 3273.9766, the d2 loss is: -3404.2969, the g loss is: 3399.7031, the ae loss is: 0.005945859, the jacobian loss is:0.1292701\n",
            "This is the iter 9053, the d1 loss is: 3200.2734, the d2 loss is: -3299.2578, the g loss is: 3245.1406, the ae loss is: 0.005747522, the jacobian loss is:0.13373858\n",
            "This is the iter 9054, the d1 loss is: 3081.3125, the d2 loss is: -3209.3438, the g loss is: 3174.9219, the ae loss is: 0.0070703127, the jacobian loss is:0.10470223\n",
            "This is the iter 9055, the d1 loss is: 3064.5547, the d2 loss is: -3188.375, the g loss is: 3159.3828, the ae loss is: 0.004560368, the jacobian loss is:0.15374438\n",
            "This is the iter 9056, the d1 loss is: 3087.7422, the d2 loss is: -3193.1406, the g loss is: 3121.7812, the ae loss is: 0.008283066, the jacobian loss is:0.10052007\n",
            "This is the iter 9057, the d1 loss is: 3073.4219, the d2 loss is: -3166.8828, the g loss is: 3106.4531, the ae loss is: 0.0062651085, the jacobian loss is:0.0990245\n",
            "This is the iter 9058, the d1 loss is: 3101.5547, the d2 loss is: -3207.1562, the g loss is: 3227.75, the ae loss is: 0.0038656937, the jacobian loss is:0.12090189\n",
            "This is the iter 9059, the d1 loss is: 3082.711, the d2 loss is: -3165.8594, the g loss is: 3165.2031, the ae loss is: 0.003913958, the jacobian loss is:0.15449873\n",
            "This is the iter 9060, the d1 loss is: 3006.9531, the d2 loss is: -3123.5, the g loss is: 3130.4531, the ae loss is: 0.0074780406, the jacobian loss is:0.09046787\n",
            "This is the iter 9061, the d1 loss is: 3342.1562, the d2 loss is: -3468.1328, the g loss is: 3430.211, the ae loss is: 0.005819196, the jacobian loss is:0.11853678\n",
            "This is the iter 9062, the d1 loss is: 3005.1953, the d2 loss is: -3099.5625, the g loss is: 3071.8594, the ae loss is: 0.008351599, the jacobian loss is:0.14447805\n",
            "This is the iter 9063, the d1 loss is: 2971.8047, the d2 loss is: -3098.7734, the g loss is: 3160.125, the ae loss is: 0.0046344213, the jacobian loss is:0.13638942\n",
            "This is the iter 9064, the d1 loss is: 3003.0, the d2 loss is: -3117.9922, the g loss is: 3105.875, the ae loss is: 0.00622556, the jacobian loss is:0.090946384\n",
            "This is the iter 9065, the d1 loss is: 2997.5938, the d2 loss is: -3107.7578, the g loss is: 3115.4922, the ae loss is: 0.005634659, the jacobian loss is:0.12824653\n",
            "This is the iter 9066, the d1 loss is: 3342.0625, the d2 loss is: -3441.711, the g loss is: 3373.461, the ae loss is: 0.009021376, the jacobian loss is:0.114988916\n",
            "This is the iter 9067, the d1 loss is: 3077.336, the d2 loss is: -3191.5234, the g loss is: 3238.4688, the ae loss is: 0.006671859, the jacobian loss is:0.17998181\n",
            "This is the iter 9068, the d1 loss is: 3112.9062, the d2 loss is: -3253.4375, the g loss is: 3266.125, the ae loss is: 0.005185481, the jacobian loss is:0.10035017\n",
            "This is the iter 9069, the d1 loss is: 3171.6797, the d2 loss is: -3288.9453, the g loss is: 3285.914, the ae loss is: 0.004324344, the jacobian loss is:0.12318198\n",
            "This is the iter 9070, the d1 loss is: 2997.461, the d2 loss is: -3130.9062, the g loss is: 3153.5, the ae loss is: 0.0067791087, the jacobian loss is:0.12022146\n",
            "This is the iter 9071, the d1 loss is: 3000.9453, the d2 loss is: -3109.789, the g loss is: 3093.0625, the ae loss is: 0.004962184, the jacobian loss is:0.15611161\n",
            "This is the iter 9072, the d1 loss is: 3091.6094, the d2 loss is: -3176.211, the g loss is: 3182.0156, the ae loss is: 0.005762456, the jacobian loss is:0.11093628\n",
            "This is the iter 9073, the d1 loss is: 3038.0156, the d2 loss is: -3174.414, the g loss is: 3165.0625, the ae loss is: 0.004402628, the jacobian loss is:0.1538454\n",
            "This is the iter 9074, the d1 loss is: 3085.289, the d2 loss is: -3205.0, the g loss is: 3205.5078, the ae loss is: 0.007613305, the jacobian loss is:0.11042281\n",
            "This is the iter 9075, the d1 loss is: 3014.0781, the d2 loss is: -3132.164, the g loss is: 3150.5781, the ae loss is: 0.0055549126, the jacobian loss is:0.13629363\n",
            "This is the iter 9076, the d1 loss is: 2968.289, the d2 loss is: -3097.7344, the g loss is: 3092.5156, the ae loss is: 0.004750748, the jacobian loss is:0.12392997\n",
            "This is the iter 9077, the d1 loss is: 2966.4922, the d2 loss is: -3071.3984, the g loss is: 3043.6016, the ae loss is: 0.008577873, the jacobian loss is:0.12765867\n",
            "This is the iter 9078, the d1 loss is: 3144.1875, the d2 loss is: -3255.6875, the g loss is: 3241.1406, the ae loss is: 0.007960869, the jacobian loss is:0.09857565\n",
            "This is the iter 9079, the d1 loss is: 3135.5156, the d2 loss is: -3236.7812, the g loss is: 3268.7188, the ae loss is: 0.0057584075, the jacobian loss is:0.12869735\n",
            "This is the iter 9080, the d1 loss is: 3094.836, the d2 loss is: -3210.8281, the g loss is: 3217.0156, the ae loss is: 0.005950164, the jacobian loss is:0.117383555\n",
            "This is the iter 9081, the d1 loss is: 3072.1016, the d2 loss is: -3164.8906, the g loss is: 3159.3281, the ae loss is: 0.0064256405, the jacobian loss is:0.10812057\n",
            "This is the iter 9082, the d1 loss is: 2980.4688, the d2 loss is: -3071.75, the g loss is: 3052.5234, the ae loss is: 0.00600113, the jacobian loss is:0.07895297\n",
            "This is the iter 9083, the d1 loss is: 3090.3047, the d2 loss is: -3179.3672, the g loss is: 3199.7812, the ae loss is: 0.005132614, the jacobian loss is:0.13447258\n",
            "This is the iter 9084, the d1 loss is: 2884.2812, the d2 loss is: -2987.664, the g loss is: 2938.4531, the ae loss is: 0.009224768, the jacobian loss is:0.10020584\n",
            "This is the iter 9085, the d1 loss is: 3017.9062, the d2 loss is: -3133.0469, the g loss is: 3105.7656, the ae loss is: 0.0073084775, the jacobian loss is:0.115964465\n",
            "This is the iter 9086, the d1 loss is: 3062.1562, the d2 loss is: -3176.8828, the g loss is: 3208.5156, the ae loss is: 0.0065606833, the jacobian loss is:0.105659015\n",
            "This is the iter 9087, the d1 loss is: 2999.3672, the d2 loss is: -3108.9922, the g loss is: 3054.1094, the ae loss is: 0.00849353, the jacobian loss is:0.1623649\n",
            "This is the iter 9088, the d1 loss is: 3039.875, the d2 loss is: -3160.8281, the g loss is: 3096.414, the ae loss is: 0.007443805, the jacobian loss is:0.101217784\n",
            "This is the iter 9089, the d1 loss is: 2994.8125, the d2 loss is: -3124.8438, the g loss is: 3153.8672, the ae loss is: 0.006807663, the jacobian loss is:0.10785444\n",
            "This is the iter 9090, the d1 loss is: 3058.6328, the d2 loss is: -3164.3828, the g loss is: 3127.8984, the ae loss is: 0.0054148533, the jacobian loss is:0.12456438\n",
            "This is the iter 9091, the d1 loss is: 2869.9297, the d2 loss is: -2988.6484, the g loss is: 3082.1719, the ae loss is: 0.0063694087, the jacobian loss is:0.08560359\n",
            "This is the iter 9092, the d1 loss is: 2982.3672, the d2 loss is: -3075.8281, the g loss is: 3056.6719, the ae loss is: 0.008748884, the jacobian loss is:0.07271612\n",
            "This is the iter 9093, the d1 loss is: 2960.414, the d2 loss is: -3071.8047, the g loss is: 3085.7188, the ae loss is: 0.009002896, the jacobian loss is:0.12415493\n",
            "This is the iter 9094, the d1 loss is: 3130.2266, the d2 loss is: -3242.6875, the g loss is: 3226.1562, the ae loss is: 0.0059874915, the jacobian loss is:0.14868617\n",
            "This is the iter 9095, the d1 loss is: 3136.789, the d2 loss is: -3263.789, the g loss is: 3285.8125, the ae loss is: 0.005100685, the jacobian loss is:0.12158763\n",
            "This is the iter 9096, the d1 loss is: 2917.4688, the d2 loss is: -3064.0312, the g loss is: 3036.8672, the ae loss is: 0.0056963903, the jacobian loss is:0.142113\n",
            "This is the iter 9097, the d1 loss is: 3022.5078, the d2 loss is: -3137.5781, the g loss is: 3130.3672, the ae loss is: 0.005161994, the jacobian loss is:0.093285434\n",
            "This is the iter 9098, the d1 loss is: 2943.2656, the d2 loss is: -3050.6484, the g loss is: 3045.3438, the ae loss is: 0.008389821, the jacobian loss is:0.100295536\n",
            "This is the iter 9099, the d1 loss is: 3017.4688, the d2 loss is: -3143.0781, the g loss is: 3137.4297, the ae loss is: 0.007073141, the jacobian loss is:0.09891478\n",
            "This is the iter 9100, the d1 loss is: 2924.5703, the d2 loss is: -3025.1875, the g loss is: 3061.8047, the ae loss is: 0.0042709783, the jacobian loss is:0.09641818\n",
            "0.25564834\n",
            "0.94694114\n",
            "This is the iter 9101, the d1 loss is: 2896.539, the d2 loss is: -3025.4922, the g loss is: 3018.5312, the ae loss is: 0.0056081847, the jacobian loss is:0.1656239\n",
            "This is the iter 9102, the d1 loss is: 3077.6562, the d2 loss is: -3192.8672, the g loss is: 3201.2422, the ae loss is: 0.005925038, the jacobian loss is:0.15069781\n",
            "This is the iter 9103, the d1 loss is: 3248.4453, the d2 loss is: -3355.4297, the g loss is: 3383.4297, the ae loss is: 0.007445987, the jacobian loss is:0.0893813\n",
            "This is the iter 9104, the d1 loss is: 3048.8984, the d2 loss is: -3148.5078, the g loss is: 3201.9219, the ae loss is: 0.0073131584, the jacobian loss is:0.12461668\n",
            "This is the iter 9105, the d1 loss is: 3030.0312, the d2 loss is: -3135.2969, the g loss is: 3130.5156, the ae loss is: 0.0068548936, the jacobian loss is:0.11093981\n",
            "This is the iter 9106, the d1 loss is: 3035.9297, the d2 loss is: -3123.7422, the g loss is: 3127.664, the ae loss is: 0.0058567408, the jacobian loss is:0.11812788\n",
            "This is the iter 9107, the d1 loss is: 2821.8125, the d2 loss is: -2939.836, the g loss is: 2953.3125, the ae loss is: 0.009331632, the jacobian loss is:0.101852685\n",
            "This is the iter 9108, the d1 loss is: 2898.6719, the d2 loss is: -3016.3125, the g loss is: 2999.9688, the ae loss is: 0.005721192, the jacobian loss is:0.07272398\n",
            "This is the iter 9109, the d1 loss is: 2978.3281, the d2 loss is: -3068.1953, the g loss is: 3151.8516, the ae loss is: 0.006203157, the jacobian loss is:0.09177248\n",
            "This is the iter 9110, the d1 loss is: 2975.3828, the d2 loss is: -3110.4297, the g loss is: 3135.4062, the ae loss is: 0.0050421595, the jacobian loss is:0.06970768\n",
            "This is the iter 9111, the d1 loss is: 2937.1875, the d2 loss is: -3051.2422, the g loss is: 3110.1719, the ae loss is: 0.006298874, the jacobian loss is:0.074601024\n",
            "This is the iter 9112, the d1 loss is: 3252.5234, the d2 loss is: -3386.8594, the g loss is: 3375.8906, the ae loss is: 0.005324327, the jacobian loss is:0.10365297\n",
            "This is the iter 9113, the d1 loss is: 2982.3438, the d2 loss is: -3118.6719, the g loss is: 3118.2969, the ae loss is: 0.0059210313, the jacobian loss is:0.1244959\n",
            "This is the iter 9114, the d1 loss is: 3090.1719, the d2 loss is: -3188.4297, the g loss is: 3128.6172, the ae loss is: 0.0051367166, the jacobian loss is:0.0707526\n",
            "This is the iter 9115, the d1 loss is: 3391.039, the d2 loss is: -3496.5938, the g loss is: 3478.0938, the ae loss is: 0.0055912533, the jacobian loss is:0.12115727\n",
            "This is the iter 9116, the d1 loss is: 2854.4453, the d2 loss is: -2971.9688, the g loss is: 2934.3281, the ae loss is: 0.007029889, the jacobian loss is:0.0918107\n",
            "This is the iter 9117, the d1 loss is: 2940.461, the d2 loss is: -3066.0078, the g loss is: 3075.5469, the ae loss is: 0.0050637675, the jacobian loss is:0.08830732\n",
            "This is the iter 9118, the d1 loss is: 3320.7812, the d2 loss is: -3414.1406, the g loss is: 3385.7812, the ae loss is: 0.0059202733, the jacobian loss is:0.0808594\n",
            "This is the iter 9119, the d1 loss is: 3194.7812, the d2 loss is: -3318.2812, the g loss is: 3307.789, the ae loss is: 0.0049472717, the jacobian loss is:0.07965851\n",
            "This is the iter 9120, the d1 loss is: 2857.7656, the d2 loss is: -2982.3125, the g loss is: 2966.7734, the ae loss is: 0.004298737, the jacobian loss is:0.12022975\n",
            "This is the iter 9121, the d1 loss is: 2892.586, the d2 loss is: -2977.5, the g loss is: 3020.5156, the ae loss is: 0.007905617, the jacobian loss is:0.13115735\n",
            "This is the iter 9122, the d1 loss is: 3087.5156, the d2 loss is: -3198.9531, the g loss is: 3141.1953, the ae loss is: 0.009175188, the jacobian loss is:0.10574001\n",
            "This is the iter 9123, the d1 loss is: 2972.6875, the d2 loss is: -3074.2656, the g loss is: 3113.586, the ae loss is: 0.0054414966, the jacobian loss is:0.08936943\n",
            "This is the iter 9124, the d1 loss is: 2781.3594, the d2 loss is: -2906.0625, the g loss is: 2867.6719, the ae loss is: 0.008177962, the jacobian loss is:0.09290434\n",
            "This is the iter 9125, the d1 loss is: 3174.4922, the d2 loss is: -3272.4766, the g loss is: 3255.75, the ae loss is: 0.0057907333, the jacobian loss is:0.11717111\n",
            "This is the iter 9126, the d1 loss is: 2948.1562, the d2 loss is: -3086.75, the g loss is: 3039.3125, the ae loss is: 0.0044535417, the jacobian loss is:0.08095772\n",
            "This is the iter 9127, the d1 loss is: 3197.9688, the d2 loss is: -3312.8281, the g loss is: 3357.4922, the ae loss is: 0.006215328, the jacobian loss is:0.11036617\n",
            "This is the iter 9128, the d1 loss is: 3160.9531, the d2 loss is: -3234.4922, the g loss is: 3229.1875, the ae loss is: 0.0072634025, the jacobian loss is:0.11627719\n",
            "This is the iter 9129, the d1 loss is: 3048.5703, the d2 loss is: -3179.539, the g loss is: 3163.211, the ae loss is: 0.0051015983, the jacobian loss is:0.113253966\n",
            "This is the iter 9130, the d1 loss is: 3280.3281, the d2 loss is: -3392.9688, the g loss is: 3420.7188, the ae loss is: 0.0073798886, the jacobian loss is:0.10614803\n",
            "This is the iter 9131, the d1 loss is: 3154.7031, the d2 loss is: -3259.8906, the g loss is: 3243.7969, the ae loss is: 0.0050109597, the jacobian loss is:0.13139084\n",
            "This is the iter 9132, the d1 loss is: 2934.9297, the d2 loss is: -3068.7656, the g loss is: 3077.7344, the ae loss is: 0.0076925643, the jacobian loss is:0.11397276\n",
            "This is the iter 9133, the d1 loss is: 3012.0078, the d2 loss is: -3135.9297, the g loss is: 3112.1328, the ae loss is: 0.005773656, the jacobian loss is:0.101092175\n",
            "This is the iter 9134, the d1 loss is: 2983.5938, the d2 loss is: -3118.9062, the g loss is: 3136.4688, the ae loss is: 0.0072375075, the jacobian loss is:0.105829485\n",
            "This is the iter 9135, the d1 loss is: 3041.0938, the d2 loss is: -3140.2578, the g loss is: 3164.9297, the ae loss is: 0.0047694407, the jacobian loss is:0.09304518\n",
            "This is the iter 9136, the d1 loss is: 3284.6406, the d2 loss is: -3402.2344, the g loss is: 3397.2656, the ae loss is: 0.0071265595, the jacobian loss is:0.11115554\n",
            "This is the iter 9137, the d1 loss is: 3122.6328, the d2 loss is: -3239.8203, the g loss is: 3246.6484, the ae loss is: 0.003959905, the jacobian loss is:0.09099595\n",
            "This is the iter 9138, the d1 loss is: 2892.5938, the d2 loss is: -3015.7031, the g loss is: 2991.875, the ae loss is: 0.0055494662, the jacobian loss is:0.13092403\n",
            "This is the iter 9139, the d1 loss is: 3273.914, the d2 loss is: -3385.9219, the g loss is: 3356.1562, the ae loss is: 0.005336659, the jacobian loss is:0.098366655\n",
            "This is the iter 9140, the d1 loss is: 3057.7266, the d2 loss is: -3184.1875, the g loss is: 3233.5312, the ae loss is: 0.004843493, the jacobian loss is:0.07419937\n",
            "This is the iter 9141, the d1 loss is: 3153.336, the d2 loss is: -3257.3281, the g loss is: 3386.8438, the ae loss is: 0.0059377467, the jacobian loss is:0.12956479\n",
            "This is the iter 9142, the d1 loss is: 2833.4062, the d2 loss is: -2966.4062, the g loss is: 2914.0703, the ae loss is: 0.006120447, the jacobian loss is:0.16099468\n",
            "This is the iter 9143, the d1 loss is: 2759.6094, the d2 loss is: -2868.9766, the g loss is: 2927.3906, the ae loss is: 0.003886321, the jacobian loss is:0.121349335\n",
            "This is the iter 9144, the d1 loss is: 2887.2734, the d2 loss is: -3007.9844, the g loss is: 3026.461, the ae loss is: 0.0070990333, the jacobian loss is:0.070622616\n",
            "This is the iter 9145, the d1 loss is: 3236.1953, the d2 loss is: -3355.9219, the g loss is: 3324.1719, the ae loss is: 0.005609742, the jacobian loss is:0.1069292\n",
            "This is the iter 9146, the d1 loss is: 2892.4062, the d2 loss is: -3035.3516, the g loss is: 3057.1875, the ae loss is: 0.00668864, the jacobian loss is:0.09169346\n",
            "This is the iter 9147, the d1 loss is: 2701.9531, the d2 loss is: -2817.9219, the g loss is: 2815.7031, the ae loss is: 0.005710753, the jacobian loss is:0.094002776\n",
            "This is the iter 9148, the d1 loss is: 2747.9062, the d2 loss is: -2851.4531, the g loss is: 2930.1562, the ae loss is: 0.004692179, the jacobian loss is:0.09861168\n",
            "This is the iter 9149, the d1 loss is: 2817.4531, the d2 loss is: -2945.5156, the g loss is: 2974.039, the ae loss is: 0.00683567, the jacobian loss is:0.12087294\n",
            "This is the iter 9150, the d1 loss is: 2964.7656, the d2 loss is: -3048.3047, the g loss is: 3038.8281, the ae loss is: 0.0060891313, the jacobian loss is:0.091517136\n",
            "This is the iter 9151, the d1 loss is: 3037.7812, the d2 loss is: -3142.9297, the g loss is: 3220.4219, the ae loss is: 0.0045712143, the jacobian loss is:0.07860281\n",
            "This is the iter 9152, the d1 loss is: 2970.7734, the d2 loss is: -3078.8125, the g loss is: 3065.4844, the ae loss is: 0.0063806237, the jacobian loss is:0.14866672\n",
            "This is the iter 9153, the d1 loss is: 3138.4531, the d2 loss is: -3239.8203, the g loss is: 3263.6719, the ae loss is: 0.004257912, the jacobian loss is:0.105942816\n",
            "This is the iter 9154, the d1 loss is: 3177.6562, the d2 loss is: -3306.9844, the g loss is: 3270.3984, the ae loss is: 0.007470226, the jacobian loss is:0.13370661\n",
            "This is the iter 9155, the d1 loss is: 2977.6172, the d2 loss is: -3081.0, the g loss is: 3080.6484, the ae loss is: 0.0051872004, the jacobian loss is:0.11743706\n",
            "This is the iter 9156, the d1 loss is: 3074.9375, the d2 loss is: -3180.289, the g loss is: 3180.1875, the ae loss is: 0.005285479, the jacobian loss is:0.067314625\n",
            "This is the iter 9157, the d1 loss is: 2875.3906, the d2 loss is: -2986.0469, the g loss is: 2940.4375, the ae loss is: 0.006584673, the jacobian loss is:0.13196114\n",
            "This is the iter 9158, the d1 loss is: 3008.1484, the d2 loss is: -3108.4375, the g loss is: 3034.3125, the ae loss is: 0.00616973, the jacobian loss is:0.12918726\n",
            "This is the iter 9159, the d1 loss is: 2950.711, the d2 loss is: -3056.5781, the g loss is: 3023.0156, the ae loss is: 0.0050026122, the jacobian loss is:0.14286183\n",
            "This is the iter 9160, the d1 loss is: 2695.0938, the d2 loss is: -2811.4766, the g loss is: 2778.5234, the ae loss is: 0.0057031238, the jacobian loss is:0.087424055\n",
            "This is the iter 9161, the d1 loss is: 2907.2578, the d2 loss is: -3004.3438, the g loss is: 3040.4297, the ae loss is: 0.007927896, the jacobian loss is:0.092937745\n",
            "This is the iter 9162, the d1 loss is: 3118.1328, the d2 loss is: -3207.6875, the g loss is: 3157.375, the ae loss is: 0.005439313, the jacobian loss is:0.13160804\n",
            "This is the iter 9163, the d1 loss is: 2974.1562, the d2 loss is: -3116.4766, the g loss is: 3067.6719, the ae loss is: 0.005157488, the jacobian loss is:0.07584925\n",
            "This is the iter 9164, the d1 loss is: 2993.6797, the d2 loss is: -3098.5781, the g loss is: 3065.7266, the ae loss is: 0.007464666, the jacobian loss is:0.14947985\n",
            "This is the iter 9165, the d1 loss is: 3065.2344, the d2 loss is: -3177.8281, the g loss is: 3190.6328, the ae loss is: 0.0062858686, the jacobian loss is:0.0818334\n",
            "This is the iter 9166, the d1 loss is: 3110.211, the d2 loss is: -3215.1484, the g loss is: 3187.9844, the ae loss is: 0.0060610287, the jacobian loss is:0.10367801\n",
            "This is the iter 9167, the d1 loss is: 2930.2734, the d2 loss is: -3038.8594, the g loss is: 3125.164, the ae loss is: 0.0053202473, the jacobian loss is:0.06848827\n",
            "This is the iter 9168, the d1 loss is: 2967.3672, the d2 loss is: -3063.6094, the g loss is: 3128.625, the ae loss is: 0.0049531003, the jacobian loss is:0.08192449\n",
            "This is the iter 9169, the d1 loss is: 2963.0156, the d2 loss is: -3053.3125, the g loss is: 3067.3047, the ae loss is: 0.007954326, the jacobian loss is:0.09223896\n",
            "This is the iter 9170, the d1 loss is: 2988.039, the d2 loss is: -3100.6406, the g loss is: 3106.3594, the ae loss is: 0.005121204, the jacobian loss is:0.10452519\n",
            "This is the iter 9171, the d1 loss is: 3133.9375, the d2 loss is: -3229.0547, the g loss is: 3256.6172, the ae loss is: 0.004389053, the jacobian loss is:0.09952018\n",
            "This is the iter 9172, the d1 loss is: 3151.8203, the d2 loss is: -3260.961, the g loss is: 3278.711, the ae loss is: 0.008208678, the jacobian loss is:0.08840424\n",
            "This is the iter 9173, the d1 loss is: 2948.4062, the d2 loss is: -3075.8516, the g loss is: 3065.8594, the ae loss is: 0.006674247, the jacobian loss is:0.07774445\n",
            "This is the iter 9174, the d1 loss is: 2907.5078, the d2 loss is: -3014.3125, the g loss is: 2991.375, the ae loss is: 0.0057435194, the jacobian loss is:0.06903357\n",
            "This is the iter 9175, the d1 loss is: 2803.2578, the d2 loss is: -2909.6953, the g loss is: 2891.2188, the ae loss is: 0.0041894866, the jacobian loss is:0.09359633\n",
            "This is the iter 9176, the d1 loss is: 3079.6719, the d2 loss is: -3183.9375, the g loss is: 3135.6562, the ae loss is: 0.006007822, the jacobian loss is:0.11678493\n",
            "This is the iter 9177, the d1 loss is: 2947.7031, the d2 loss is: -3057.9219, the g loss is: 3039.1562, the ae loss is: 0.0073423306, the jacobian loss is:0.14512075\n",
            "This is the iter 9178, the d1 loss is: 2849.1875, the d2 loss is: -2962.0469, the g loss is: 2928.414, the ae loss is: 0.007393221, the jacobian loss is:0.07938879\n",
            "This is the iter 9179, the d1 loss is: 3155.75, the d2 loss is: -3283.0547, the g loss is: 3264.9922, the ae loss is: 0.0051322025, the jacobian loss is:0.112602256\n",
            "This is the iter 9180, the d1 loss is: 2990.5469, the d2 loss is: -3110.3906, the g loss is: 3103.0781, the ae loss is: 0.0053139236, the jacobian loss is:0.08957395\n",
            "This is the iter 9181, the d1 loss is: 2958.6172, the d2 loss is: -3063.2969, the g loss is: 3012.1172, the ae loss is: 0.0049629146, the jacobian loss is:0.120931625\n",
            "This is the iter 9182, the d1 loss is: 3273.0938, the d2 loss is: -3383.211, the g loss is: 3421.3125, the ae loss is: 0.004630563, the jacobian loss is:0.08947538\n",
            "This is the iter 9183, the d1 loss is: 2748.8438, the d2 loss is: -2867.086, the g loss is: 2902.3594, the ae loss is: 0.0070875594, the jacobian loss is:0.08864085\n",
            "This is the iter 9184, the d1 loss is: 3241.1797, the d2 loss is: -3340.125, the g loss is: 3384.8516, the ae loss is: 0.0052153924, the jacobian loss is:0.06648763\n",
            "This is the iter 9185, the d1 loss is: 3103.2656, the d2 loss is: -3203.9219, the g loss is: 3193.961, the ae loss is: 0.00443026, the jacobian loss is:0.09923643\n",
            "This is the iter 9186, the d1 loss is: 3107.336, the d2 loss is: -3213.2578, the g loss is: 3262.75, the ae loss is: 0.0073294444, the jacobian loss is:0.11737917\n",
            "This is the iter 9187, the d1 loss is: 2883.2969, the d2 loss is: -3010.2344, the g loss is: 3012.6172, the ae loss is: 0.004740549, the jacobian loss is:0.09642259\n",
            "This is the iter 9188, the d1 loss is: 3070.9062, the d2 loss is: -3182.2422, the g loss is: 3160.164, the ae loss is: 0.0057975147, the jacobian loss is:0.06274216\n",
            "This is the iter 9189, the d1 loss is: 2956.461, the d2 loss is: -3066.2422, the g loss is: 3059.8438, the ae loss is: 0.005707141, the jacobian loss is:0.08149876\n",
            "This is the iter 9190, the d1 loss is: 3022.4219, the d2 loss is: -3132.8203, the g loss is: 3141.336, the ae loss is: 0.0055492087, the jacobian loss is:0.104792476\n",
            "This is the iter 9191, the d1 loss is: 2895.4375, the d2 loss is: -2990.3047, the g loss is: 3044.7969, the ae loss is: 0.007175613, the jacobian loss is:0.1473764\n",
            "This is the iter 9192, the d1 loss is: 3024.3906, the d2 loss is: -3157.9922, the g loss is: 3164.6172, the ae loss is: 0.007508939, the jacobian loss is:0.08864097\n",
            "This is the iter 9193, the d1 loss is: 3037.625, the d2 loss is: -3157.6953, the g loss is: 3148.1797, the ae loss is: 0.0049099075, the jacobian loss is:0.09749687\n",
            "This is the iter 9194, the d1 loss is: 2981.0156, the d2 loss is: -3088.6875, the g loss is: 3106.7578, the ae loss is: 0.005199814, the jacobian loss is:0.07722021\n",
            "This is the iter 9195, the d1 loss is: 3062.8125, the d2 loss is: -3178.5, the g loss is: 3128.0938, the ae loss is: 0.0052062557, the jacobian loss is:0.06752731\n",
            "This is the iter 9196, the d1 loss is: 3171.7656, the d2 loss is: -3268.3906, the g loss is: 3304.039, the ae loss is: 0.005873153, the jacobian loss is:0.09998273\n",
            "This is the iter 9197, the d1 loss is: 2994.8203, the d2 loss is: -3109.0469, the g loss is: 3114.7266, the ae loss is: 0.0068278005, the jacobian loss is:0.067222685\n",
            "This is the iter 9198, the d1 loss is: 2926.8516, the d2 loss is: -3048.2578, the g loss is: 3026.3438, the ae loss is: 0.0062323897, the jacobian loss is:0.06428197\n",
            "This is the iter 9199, the d1 loss is: 2989.6875, the d2 loss is: -3076.9062, the g loss is: 3072.9922, the ae loss is: 0.005908739, the jacobian loss is:0.07174125\n",
            "This is the iter 9200, the d1 loss is: 3013.5781, the d2 loss is: -3127.5469, the g loss is: 3104.211, the ae loss is: 0.00991411, the jacobian loss is:0.09993643\n",
            "0.25214517\n",
            "0.92559403\n",
            "This is the iter 9201, the d1 loss is: 3007.664, the d2 loss is: -3114.8828, the g loss is: 3091.7031, the ae loss is: 0.0065271114, the jacobian loss is:0.08040293\n",
            "This is the iter 9202, the d1 loss is: 3087.4688, the d2 loss is: -3216.125, the g loss is: 3194.6875, the ae loss is: 0.0066749705, the jacobian loss is:0.10342124\n",
            "This is the iter 9203, the d1 loss is: 3055.4219, the d2 loss is: -3172.586, the g loss is: 3151.0703, the ae loss is: 0.0042724987, the jacobian loss is:0.084322125\n",
            "This is the iter 9204, the d1 loss is: 2976.7656, the d2 loss is: -3098.461, the g loss is: 3139.9844, the ae loss is: 0.007437577, the jacobian loss is:0.07264964\n",
            "This is the iter 9205, the d1 loss is: 3019.039, the d2 loss is: -3134.25, the g loss is: 3180.6953, the ae loss is: 0.0059818393, the jacobian loss is:0.06880394\n",
            "This is the iter 9206, the d1 loss is: 3032.7188, the d2 loss is: -3150.0781, the g loss is: 3142.1562, the ae loss is: 0.006019881, the jacobian loss is:0.06235723\n",
            "This is the iter 9207, the d1 loss is: 2951.086, the d2 loss is: -3067.0469, the g loss is: 3042.6719, the ae loss is: 0.00587816, the jacobian loss is:0.081347026\n",
            "This is the iter 9208, the d1 loss is: 2994.6562, the d2 loss is: -3125.7422, the g loss is: 3135.7578, the ae loss is: 0.004765873, the jacobian loss is:0.08890299\n",
            "This is the iter 9209, the d1 loss is: 2991.039, the d2 loss is: -3098.9844, the g loss is: 3165.2812, the ae loss is: 0.006917061, the jacobian loss is:0.11904714\n",
            "This is the iter 9210, the d1 loss is: 3062.3125, the d2 loss is: -3167.5, the g loss is: 3284.9062, the ae loss is: 0.0060728285, the jacobian loss is:0.06674622\n",
            "This is the iter 9211, the d1 loss is: 2823.2266, the d2 loss is: -2950.3984, the g loss is: 2957.9922, the ae loss is: 0.007767897, the jacobian loss is:0.101644896\n",
            "This is the iter 9212, the d1 loss is: 3083.9688, the d2 loss is: -3184.6328, the g loss is: 3164.4844, the ae loss is: 0.007324987, the jacobian loss is:0.05634832\n",
            "This is the iter 9213, the d1 loss is: 2886.2031, the d2 loss is: -3023.625, the g loss is: 3106.4375, the ae loss is: 0.0069983117, the jacobian loss is:0.10962488\n",
            "This is the iter 9214, the d1 loss is: 3103.6875, the d2 loss is: -3195.8984, the g loss is: 3153.0078, the ae loss is: 0.0065172813, the jacobian loss is:0.07753325\n",
            "This is the iter 9215, the d1 loss is: 3088.4219, the d2 loss is: -3212.9688, the g loss is: 3169.7422, the ae loss is: 0.005435389, the jacobian loss is:0.0799369\n",
            "This is the iter 9216, the d1 loss is: 2805.6172, the d2 loss is: -2931.1562, the g loss is: 2927.5, the ae loss is: 0.008406552, the jacobian loss is:0.065823354\n",
            "This is the iter 9217, the d1 loss is: 3093.4922, the d2 loss is: -3202.7656, the g loss is: 3211.9219, the ae loss is: 0.0048394185, the jacobian loss is:0.09409354\n",
            "This is the iter 9218, the d1 loss is: 3015.711, the d2 loss is: -3121.6406, the g loss is: 3120.4219, the ae loss is: 0.003999634, the jacobian loss is:0.060758643\n",
            "This is the iter 9219, the d1 loss is: 3030.1719, the d2 loss is: -3123.375, the g loss is: 3071.9844, the ae loss is: 0.007819397, the jacobian loss is:0.0958788\n",
            "This is the iter 9220, the d1 loss is: 2843.4062, the d2 loss is: -2955.586, the g loss is: 2919.4453, the ae loss is: 0.0068956157, the jacobian loss is:0.10737762\n",
            "This is the iter 9221, the d1 loss is: 2976.125, the d2 loss is: -3107.8438, the g loss is: 3054.0781, the ae loss is: 0.0056147752, the jacobian loss is:0.07587465\n",
            "This is the iter 9222, the d1 loss is: 2993.461, the d2 loss is: -3117.6875, the g loss is: 3108.5781, the ae loss is: 0.0045911083, the jacobian loss is:0.09764052\n",
            "This is the iter 9223, the d1 loss is: 3176.1094, the d2 loss is: -3258.625, the g loss is: 3249.1406, the ae loss is: 0.005311194, the jacobian loss is:0.09068985\n",
            "This is the iter 9224, the d1 loss is: 2872.5781, the d2 loss is: -3002.6562, the g loss is: 3005.2734, the ae loss is: 0.0064786663, the jacobian loss is:0.09690726\n",
            "This is the iter 9225, the d1 loss is: 2953.7734, the d2 loss is: -3055.4375, the g loss is: 3053.1562, the ae loss is: 0.004888871, the jacobian loss is:0.07419975\n",
            "This is the iter 9226, the d1 loss is: 2985.0156, the d2 loss is: -3064.1719, the g loss is: 3127.8281, the ae loss is: 0.0058507184, the jacobian loss is:0.10216504\n",
            "This is the iter 9227, the d1 loss is: 3137.5469, the d2 loss is: -3227.4766, the g loss is: 3243.6719, the ae loss is: 0.0055438597, the jacobian loss is:0.06827787\n",
            "This is the iter 9228, the d1 loss is: 3007.1172, the d2 loss is: -3121.5625, the g loss is: 3115.9922, the ae loss is: 0.004838213, the jacobian loss is:0.07943927\n",
            "This is the iter 9229, the d1 loss is: 2932.7266, the d2 loss is: -3038.8438, the g loss is: 3040.1797, the ae loss is: 0.007909927, the jacobian loss is:0.13485105\n",
            "This is the iter 9230, the d1 loss is: 3020.1328, the d2 loss is: -3121.0625, the g loss is: 3094.9688, the ae loss is: 0.0055790963, the jacobian loss is:0.08620294\n",
            "This is the iter 9231, the d1 loss is: 3189.2344, the d2 loss is: -3310.0781, the g loss is: 3302.3203, the ae loss is: 0.005314739, the jacobian loss is:0.10103849\n",
            "This is the iter 9232, the d1 loss is: 3044.8047, the d2 loss is: -3148.8594, the g loss is: 3125.3906, the ae loss is: 0.008098012, the jacobian loss is:0.10850652\n",
            "This is the iter 9233, the d1 loss is: 3241.4766, the d2 loss is: -3362.5234, the g loss is: 3326.5625, the ae loss is: 0.004525489, the jacobian loss is:0.07508455\n",
            "This is the iter 9234, the d1 loss is: 2939.0234, the d2 loss is: -3051.7812, the g loss is: 3034.5312, the ae loss is: 0.0049836924, the jacobian loss is:0.08769534\n",
            "This is the iter 9235, the d1 loss is: 2982.3438, the d2 loss is: -3082.8516, the g loss is: 3121.8984, the ae loss is: 0.007625215, the jacobian loss is:0.08175414\n",
            "This is the iter 9236, the d1 loss is: 3030.0469, the d2 loss is: -3133.3906, the g loss is: 3134.4688, the ae loss is: 0.005242978, the jacobian loss is:0.06561621\n",
            "This is the iter 9237, the d1 loss is: 2987.9531, the d2 loss is: -3095.2969, the g loss is: 3070.2344, the ae loss is: 0.006860667, the jacobian loss is:0.114462756\n",
            "This is the iter 9238, the d1 loss is: 2928.3516, the d2 loss is: -3023.3125, the g loss is: 2990.836, the ae loss is: 0.0064741964, the jacobian loss is:0.20891552\n",
            "This is the iter 9239, the d1 loss is: 3134.3047, the d2 loss is: -3222.3984, the g loss is: 3232.0781, the ae loss is: 0.008366793, the jacobian loss is:0.10664547\n",
            "This is the iter 9240, the d1 loss is: 3173.6875, the d2 loss is: -3280.8203, the g loss is: 3272.4688, the ae loss is: 0.006071088, the jacobian loss is:0.066418394\n",
            "This is the iter 9241, the d1 loss is: 3130.8516, the d2 loss is: -3252.7812, the g loss is: 3251.75, the ae loss is: 0.0072140284, the jacobian loss is:0.09621214\n",
            "This is the iter 9242, the d1 loss is: 2753.1797, the d2 loss is: -2850.3906, the g loss is: 2923.6016, the ae loss is: 0.0058652237, the jacobian loss is:0.07838589\n",
            "This is the iter 9243, the d1 loss is: 2942.7344, the d2 loss is: -3052.1953, the g loss is: 3051.164, the ae loss is: 0.006877453, the jacobian loss is:0.11429952\n",
            "This is the iter 9244, the d1 loss is: 3000.3125, the d2 loss is: -3110.461, the g loss is: 3155.5938, the ae loss is: 0.005936633, the jacobian loss is:0.082395814\n",
            "This is the iter 9245, the d1 loss is: 2951.461, the d2 loss is: -3064.7266, the g loss is: 3116.9688, the ae loss is: 0.0054547153, the jacobian loss is:0.070670724\n",
            "This is the iter 9246, the d1 loss is: 2867.25, the d2 loss is: -2973.789, the g loss is: 2975.9531, the ae loss is: 0.0056346096, the jacobian loss is:0.09870182\n",
            "This is the iter 9247, the d1 loss is: 2968.7344, the d2 loss is: -3067.461, the g loss is: 3013.2578, the ae loss is: 0.005020883, the jacobian loss is:0.07625187\n",
            "This is the iter 9248, the d1 loss is: 3007.6562, the d2 loss is: -3134.6328, the g loss is: 3083.0625, the ae loss is: 0.006313594, the jacobian loss is:0.082921155\n",
            "This is the iter 9249, the d1 loss is: 2880.586, the d2 loss is: -2993.2422, the g loss is: 3026.8906, the ae loss is: 0.005596758, the jacobian loss is:0.09437144\n",
            "This is the iter 9250, the d1 loss is: 2816.1953, the d2 loss is: -2925.8047, the g loss is: 2922.5469, the ae loss is: 0.0054970016, the jacobian loss is:0.09527077\n",
            "This is the iter 9251, the d1 loss is: 2646.539, the d2 loss is: -2739.9062, the g loss is: 2748.9297, the ae loss is: 0.0057432503, the jacobian loss is:0.07654782\n",
            "This is the iter 9252, the d1 loss is: 2804.9688, the d2 loss is: -2898.1406, the g loss is: 2841.6562, the ae loss is: 0.0044049923, the jacobian loss is:0.1592039\n",
            "This is the iter 9253, the d1 loss is: 3187.5703, the d2 loss is: -3285.9531, the g loss is: 3294.2266, the ae loss is: 0.0073643494, the jacobian loss is:0.08547155\n",
            "This is the iter 9254, the d1 loss is: 3080.9844, the d2 loss is: -3162.8203, the g loss is: 3184.125, the ae loss is: 0.0068203174, the jacobian loss is:0.07417196\n",
            "This is the iter 9255, the d1 loss is: 2916.3203, the d2 loss is: -3005.1719, the g loss is: 3037.0781, the ae loss is: 0.0069150617, the jacobian loss is:0.15912423\n",
            "This is the iter 9256, the d1 loss is: 2901.3047, the d2 loss is: -3031.2344, the g loss is: 3064.5, the ae loss is: 0.0055893874, the jacobian loss is:0.07298883\n",
            "This is the iter 9257, the d1 loss is: 2890.8047, the d2 loss is: -2990.7344, the g loss is: 2901.0234, the ae loss is: 0.009171491, the jacobian loss is:0.0986695\n",
            "This is the iter 9258, the d1 loss is: 2963.9062, the d2 loss is: -3067.914, the g loss is: 3028.5703, the ae loss is: 0.0054340092, the jacobian loss is:0.08885132\n",
            "This is the iter 9259, the d1 loss is: 2692.6484, the d2 loss is: -2804.0703, the g loss is: 2789.7344, the ae loss is: 0.006354328, the jacobian loss is:0.0859105\n",
            "This is the iter 9260, the d1 loss is: 2903.8516, the d2 loss is: -3021.3281, the g loss is: 3019.539, the ae loss is: 0.0063341833, the jacobian loss is:0.086818285\n",
            "This is the iter 9261, the d1 loss is: 3031.1094, the d2 loss is: -3163.9766, the g loss is: 3111.6953, the ae loss is: 0.012790439, the jacobian loss is:0.13086918\n",
            "This is the iter 9262, the d1 loss is: 2929.8203, the d2 loss is: -3026.0234, the g loss is: 3032.0156, the ae loss is: 0.0059905373, the jacobian loss is:0.11048175\n",
            "This is the iter 9263, the d1 loss is: 2975.211, the d2 loss is: -3084.0156, the g loss is: 3008.961, the ae loss is: 0.009243256, the jacobian loss is:0.12384274\n",
            "This is the iter 9264, the d1 loss is: 3035.9375, the d2 loss is: -3134.3828, the g loss is: 3131.2969, the ae loss is: 0.00837015, the jacobian loss is:0.09991669\n",
            "This is the iter 9265, the d1 loss is: 3142.164, the d2 loss is: -3238.789, the g loss is: 3214.6797, the ae loss is: 0.009065703, the jacobian loss is:0.06984531\n",
            "This is the iter 9266, the d1 loss is: 2880.4062, the d2 loss is: -2963.1875, the g loss is: 2952.5156, the ae loss is: 0.0054058526, the jacobian loss is:0.12178611\n",
            "This is the iter 9267, the d1 loss is: 2948.0156, the d2 loss is: -3054.2266, the g loss is: 3043.2188, the ae loss is: 0.0064169234, the jacobian loss is:0.07104619\n",
            "This is the iter 9268, the d1 loss is: 2951.414, the d2 loss is: -3045.8828, the g loss is: 3011.0703, the ae loss is: 0.0077490527, the jacobian loss is:0.123786874\n",
            "This is the iter 9269, the d1 loss is: 2827.8125, the d2 loss is: -2926.8047, the g loss is: 2987.3281, the ae loss is: 0.0077568553, the jacobian loss is:0.088111475\n",
            "This is the iter 9270, the d1 loss is: 3060.8203, the d2 loss is: -3137.2422, the g loss is: 3150.9375, the ae loss is: 0.0047517377, the jacobian loss is:0.090657495\n",
            "This is the iter 9271, the d1 loss is: 2898.6797, the d2 loss is: -2997.7344, the g loss is: 3128.3828, the ae loss is: 0.0064843423, the jacobian loss is:0.08864067\n",
            "This is the iter 9272, the d1 loss is: 2901.9062, the d2 loss is: -3014.4531, the g loss is: 2996.6406, the ae loss is: 0.0056130183, the jacobian loss is:0.07444932\n",
            "This is the iter 9273, the d1 loss is: 2938.1484, the d2 loss is: -3068.1953, the g loss is: 3083.7344, the ae loss is: 0.006134101, the jacobian loss is:0.1264627\n",
            "This is the iter 9274, the d1 loss is: 2858.7188, the d2 loss is: -2979.0703, the g loss is: 3009.8984, the ae loss is: 0.0074142125, the jacobian loss is:0.11687948\n",
            "This is the iter 9275, the d1 loss is: 2765.2656, the d2 loss is: -2860.8828, the g loss is: 2824.7812, the ae loss is: 0.0064653764, the jacobian loss is:0.05396117\n",
            "This is the iter 9276, the d1 loss is: 3252.7656, the d2 loss is: -3356.2344, the g loss is: 3325.2656, the ae loss is: 0.0046271547, the jacobian loss is:0.08786373\n",
            "This is the iter 9277, the d1 loss is: 3016.6484, the d2 loss is: -3104.8594, the g loss is: 3051.4766, the ae loss is: 0.0060314895, the jacobian loss is:0.09230963\n",
            "This is the iter 9278, the d1 loss is: 2981.4062, the d2 loss is: -3098.5, the g loss is: 3034.664, the ae loss is: 0.007661486, the jacobian loss is:0.11500895\n",
            "This is the iter 9279, the d1 loss is: 3146.9766, the d2 loss is: -3233.8125, the g loss is: 3177.1406, the ae loss is: 0.008447344, the jacobian loss is:0.08837903\n",
            "This is the iter 9280, the d1 loss is: 3023.8203, the d2 loss is: -3132.7422, the g loss is: 3145.3438, the ae loss is: 0.00583471, the jacobian loss is:0.07505006\n",
            "This is the iter 9281, the d1 loss is: 3213.8516, the d2 loss is: -3302.4688, the g loss is: 3251.8516, the ae loss is: 0.006384341, the jacobian loss is:0.14502588\n",
            "This is the iter 9282, the d1 loss is: 3012.6719, the d2 loss is: -3134.2812, the g loss is: 3115.961, the ae loss is: 0.006734584, the jacobian loss is:0.08593576\n",
            "This is the iter 9283, the d1 loss is: 3141.3125, the d2 loss is: -3209.2812, the g loss is: 3253.5, the ae loss is: 0.007260857, the jacobian loss is:0.10191038\n",
            "This is the iter 9284, the d1 loss is: 3104.125, the d2 loss is: -3194.0547, the g loss is: 3215.4219, the ae loss is: 0.006081707, the jacobian loss is:0.10594314\n",
            "This is the iter 9285, the d1 loss is: 2959.3828, the d2 loss is: -3028.1328, the g loss is: 3113.3984, the ae loss is: 0.006822968, the jacobian loss is:0.09149238\n",
            "This is the iter 9286, the d1 loss is: 3067.586, the d2 loss is: -3166.8828, the g loss is: 3181.75, the ae loss is: 0.006109738, the jacobian loss is:0.0845474\n",
            "This is the iter 9287, the d1 loss is: 3020.0469, the d2 loss is: -3141.4688, the g loss is: 3146.1328, the ae loss is: 0.0056327386, the jacobian loss is:0.11742736\n",
            "This is the iter 9288, the d1 loss is: 3000.1953, the d2 loss is: -3077.5781, the g loss is: 3058.8047, the ae loss is: 0.0057491343, the jacobian loss is:0.10622189\n",
            "This is the iter 9289, the d1 loss is: 3092.7422, the d2 loss is: -3199.6562, the g loss is: 3295.5156, the ae loss is: 0.006661895, the jacobian loss is:0.07309032\n",
            "This is the iter 9290, the d1 loss is: 3015.6562, the d2 loss is: -3118.0234, the g loss is: 3062.1953, the ae loss is: 0.008797368, the jacobian loss is:0.110609405\n",
            "This is the iter 9291, the d1 loss is: 3230.6562, the d2 loss is: -3352.0312, the g loss is: 3318.7031, the ae loss is: 0.0059035607, the jacobian loss is:0.11778879\n",
            "This is the iter 9292, the d1 loss is: 3060.8203, the d2 loss is: -3165.164, the g loss is: 3144.6562, the ae loss is: 0.005584202, the jacobian loss is:0.1117306\n",
            "This is the iter 9293, the d1 loss is: 3023.9062, the d2 loss is: -3126.8906, the g loss is: 3094.164, the ae loss is: 0.0045611686, the jacobian loss is:0.098682985\n",
            "This is the iter 9294, the d1 loss is: 3059.8438, the d2 loss is: -3166.3672, the g loss is: 3154.3516, the ae loss is: 0.0042106127, the jacobian loss is:0.06680944\n",
            "This is the iter 9295, the d1 loss is: 3039.1328, the d2 loss is: -3141.0703, the g loss is: 3149.3594, the ae loss is: 0.0069821062, the jacobian loss is:0.07519273\n",
            "This is the iter 9296, the d1 loss is: 3029.4531, the d2 loss is: -3123.875, the g loss is: 3119.1484, the ae loss is: 0.00573026, the jacobian loss is:0.178186\n",
            "This is the iter 9297, the d1 loss is: 3041.9766, the d2 loss is: -3145.0547, the g loss is: 3121.711, the ae loss is: 0.005499386, the jacobian loss is:0.11680359\n",
            "This is the iter 9298, the d1 loss is: 3091.0156, the d2 loss is: -3208.4297, the g loss is: 3216.7344, the ae loss is: 0.007406154, the jacobian loss is:0.08572454\n",
            "This is the iter 9299, the d1 loss is: 2967.9531, the d2 loss is: -3074.0156, the g loss is: 3077.586, the ae loss is: 0.005889955, the jacobian loss is:0.07894764\n",
            "This is the iter 9300, the d1 loss is: 3072.6562, the d2 loss is: -3159.914, the g loss is: 3197.0, the ae loss is: 0.005717327, the jacobian loss is:0.09573093\n",
            "0.2563726\n",
            "0.94134355\n",
            "This is the iter 9301, the d1 loss is: 2915.0, the d2 loss is: -3000.3516, the g loss is: 3045.2578, the ae loss is: 0.007751609, the jacobian loss is:0.099078014\n",
            "This is the iter 9302, the d1 loss is: 3089.539, the d2 loss is: -3188.8438, the g loss is: 3159.6719, the ae loss is: 0.008632646, the jacobian loss is:0.11306995\n",
            "This is the iter 9303, the d1 loss is: 2842.461, the d2 loss is: -2948.7656, the g loss is: 3018.9219, the ae loss is: 0.006268316, the jacobian loss is:0.07607936\n",
            "This is the iter 9304, the d1 loss is: 3252.0, the d2 loss is: -3354.4531, the g loss is: 3321.0469, the ae loss is: 0.0069696642, the jacobian loss is:0.104680985\n",
            "This is the iter 9305, the d1 loss is: 3147.5078, the d2 loss is: -3259.6172, the g loss is: 3259.7031, the ae loss is: 0.005187216, the jacobian loss is:0.097854584\n",
            "This is the iter 9306, the d1 loss is: 2698.3594, the d2 loss is: -2807.5703, the g loss is: 2813.1328, the ae loss is: 0.007557297, the jacobian loss is:0.117932044\n",
            "This is the iter 9307, the d1 loss is: 3249.4688, the d2 loss is: -3366.625, the g loss is: 3318.8906, the ae loss is: 0.006459063, the jacobian loss is:0.13126738\n",
            "This is the iter 9308, the d1 loss is: 2974.6562, the d2 loss is: -3093.1406, the g loss is: 3112.086, the ae loss is: 0.0055227526, the jacobian loss is:0.1182452\n",
            "This is the iter 9309, the d1 loss is: 2773.8438, the d2 loss is: -2885.3516, the g loss is: 2837.7188, the ae loss is: 0.005208829, the jacobian loss is:0.1214202\n",
            "This is the iter 9310, the d1 loss is: 3130.539, the d2 loss is: -3223.5547, the g loss is: 3219.3281, the ae loss is: 0.006368139, the jacobian loss is:0.09964115\n",
            "This is the iter 9311, the d1 loss is: 2807.9453, the d2 loss is: -2918.8281, the g loss is: 2926.0078, the ae loss is: 0.007664663, the jacobian loss is:0.07169879\n",
            "This is the iter 9312, the d1 loss is: 2879.8438, the d2 loss is: -2990.789, the g loss is: 2948.8594, the ae loss is: 0.006975161, the jacobian loss is:0.10368386\n",
            "This is the iter 9313, the d1 loss is: 3182.7969, the d2 loss is: -3295.2031, the g loss is: 3272.539, the ae loss is: 0.002861022, the jacobian loss is:0.086945176\n",
            "This is the iter 9314, the d1 loss is: 2985.8281, the d2 loss is: -3098.0781, the g loss is: 3074.7344, the ae loss is: 0.0043839025, the jacobian loss is:0.077416256\n",
            "This is the iter 9315, the d1 loss is: 3018.625, the d2 loss is: -3117.25, the g loss is: 3131.7969, the ae loss is: 0.0059906603, the jacobian loss is:0.08677297\n",
            "This is the iter 9316, the d1 loss is: 3079.4531, the d2 loss is: -3185.7188, the g loss is: 3127.7969, the ae loss is: 0.0074530556, the jacobian loss is:0.10500444\n",
            "This is the iter 9317, the d1 loss is: 2879.2656, the d2 loss is: -2987.3516, the g loss is: 2962.1797, the ae loss is: 0.004850286, the jacobian loss is:0.082784116\n",
            "This is the iter 9318, the d1 loss is: 3010.0234, the d2 loss is: -3131.3125, the g loss is: 3063.9297, the ae loss is: 0.0075293407, the jacobian loss is:0.18075664\n",
            "This is the iter 9319, the d1 loss is: 2988.3672, the d2 loss is: -3086.0234, the g loss is: 3087.3047, the ae loss is: 0.008495616, the jacobian loss is:0.09083685\n",
            "This is the iter 9320, the d1 loss is: 2994.6484, the d2 loss is: -3093.664, the g loss is: 3105.1562, the ae loss is: 0.008363151, the jacobian loss is:0.12549426\n",
            "This is the iter 9321, the d1 loss is: 3169.711, the d2 loss is: -3260.2188, the g loss is: 3248.1562, the ae loss is: 0.006145812, the jacobian loss is:0.06016717\n",
            "This is the iter 9322, the d1 loss is: 2847.8906, the d2 loss is: -2973.6172, the g loss is: 2939.539, the ae loss is: 0.0052899616, the jacobian loss is:0.08944708\n",
            "This is the iter 9323, the d1 loss is: 3223.5, the d2 loss is: -3333.711, the g loss is: 3346.875, the ae loss is: 0.003995061, the jacobian loss is:0.055750333\n",
            "This is the iter 9324, the d1 loss is: 2810.75, the d2 loss is: -2900.0, the g loss is: 2958.7656, the ae loss is: 0.0069607864, the jacobian loss is:0.098889224\n",
            "This is the iter 9325, the d1 loss is: 2792.6875, the d2 loss is: -2898.3125, the g loss is: 2902.4375, the ae loss is: 0.0072339633, the jacobian loss is:0.07504788\n",
            "This is the iter 9326, the d1 loss is: 3019.6562, the d2 loss is: -3126.9453, the g loss is: 3086.086, the ae loss is: 0.0044918554, the jacobian loss is:0.10044853\n",
            "This is the iter 9327, the d1 loss is: 2813.4219, the d2 loss is: -2918.9453, the g loss is: 2916.1484, the ae loss is: 0.0048904656, the jacobian loss is:0.09178532\n",
            "This is the iter 9328, the d1 loss is: 3028.9453, the d2 loss is: -3154.3906, the g loss is: 3126.1719, the ae loss is: 0.0074525606, the jacobian loss is:0.2133212\n",
            "This is the iter 9329, the d1 loss is: 3140.5547, the d2 loss is: -3220.961, the g loss is: 3273.8438, the ae loss is: 0.006168816, the jacobian loss is:0.10857459\n",
            "This is the iter 9330, the d1 loss is: 2949.7656, the d2 loss is: -3026.0547, the g loss is: 3033.039, the ae loss is: 0.0068540913, the jacobian loss is:0.13304235\n",
            "This is the iter 9331, the d1 loss is: 2867.0234, the d2 loss is: -2996.6094, the g loss is: 2989.75, the ae loss is: 0.0067873755, the jacobian loss is:0.18485026\n",
            "This is the iter 9332, the d1 loss is: 2978.0781, the d2 loss is: -3088.5312, the g loss is: 3065.3672, the ae loss is: 0.0065734354, the jacobian loss is:0.17006706\n",
            "This is the iter 9333, the d1 loss is: 3034.7344, the d2 loss is: -3127.2031, the g loss is: 3105.664, the ae loss is: 0.0068186643, the jacobian loss is:0.12640776\n",
            "This is the iter 9334, the d1 loss is: 2825.961, the d2 loss is: -2935.8047, the g loss is: 2932.3984, the ae loss is: 0.0042086057, the jacobian loss is:0.096261\n",
            "This is the iter 9335, the d1 loss is: 2838.0078, the d2 loss is: -2951.461, the g loss is: 3002.9219, the ae loss is: 0.0038649892, the jacobian loss is:0.082999356\n",
            "This is the iter 9336, the d1 loss is: 2639.4297, the d2 loss is: -2748.3047, the g loss is: 2735.9062, the ae loss is: 0.0057013915, the jacobian loss is:0.11019884\n",
            "This is the iter 9337, the d1 loss is: 3042.2734, the d2 loss is: -3136.836, the g loss is: 3196.6094, the ae loss is: 0.0075777518, the jacobian loss is:0.10461408\n",
            "This is the iter 9338, the d1 loss is: 3224.789, the d2 loss is: -3339.3047, the g loss is: 3344.75, the ae loss is: 0.0055302894, the jacobian loss is:0.09453907\n",
            "This is the iter 9339, the d1 loss is: 3154.5781, the d2 loss is: -3242.7969, the g loss is: 3268.5312, the ae loss is: 0.005830899, the jacobian loss is:0.075398825\n",
            "This is the iter 9340, the d1 loss is: 2959.0234, the d2 loss is: -3071.7344, the g loss is: 3023.9922, the ae loss is: 0.0052972836, the jacobian loss is:0.1016961\n",
            "This is the iter 9341, the d1 loss is: 2939.4688, the d2 loss is: -3047.5625, the g loss is: 3128.9688, the ae loss is: 0.00596305, the jacobian loss is:0.095523626\n",
            "This is the iter 9342, the d1 loss is: 2999.0469, the d2 loss is: -3097.086, the g loss is: 3092.289, the ae loss is: 0.005956891, the jacobian loss is:0.12649697\n",
            "This is the iter 9343, the d1 loss is: 2989.7344, the d2 loss is: -3094.2266, the g loss is: 3076.789, the ae loss is: 0.0066506388, the jacobian loss is:0.15755197\n",
            "This is the iter 9344, the d1 loss is: 3002.0625, the d2 loss is: -3104.0078, the g loss is: 3082.5469, the ae loss is: 0.0072018495, the jacobian loss is:0.08173112\n",
            "This is the iter 9345, the d1 loss is: 3088.9375, the d2 loss is: -3187.2812, the g loss is: 3169.4375, the ae loss is: 0.007670841, the jacobian loss is:0.19860308\n",
            "This is the iter 9346, the d1 loss is: 2793.7969, the d2 loss is: -2892.6484, the g loss is: 2875.5312, the ae loss is: 0.009022944, the jacobian loss is:0.15700544\n",
            "This is the iter 9347, the d1 loss is: 2839.125, the d2 loss is: -2943.4219, the g loss is: 2930.586, the ae loss is: 0.0068826317, the jacobian loss is:0.08040839\n",
            "This is the iter 9348, the d1 loss is: 2863.9531, the d2 loss is: -2963.461, the g loss is: 2929.3047, the ae loss is: 0.0051859464, the jacobian loss is:0.10564537\n",
            "This is the iter 9349, the d1 loss is: 2980.9922, the d2 loss is: -3067.0938, the g loss is: 3044.0469, the ae loss is: 0.0063040783, the jacobian loss is:0.12738164\n",
            "This is the iter 9350, the d1 loss is: 3006.9297, the d2 loss is: -3113.1094, the g loss is: 3002.2969, the ae loss is: 0.0065593193, the jacobian loss is:0.13187432\n",
            "This is the iter 9351, the d1 loss is: 2826.3047, the d2 loss is: -2929.7734, the g loss is: 2933.461, the ae loss is: 0.007253526, the jacobian loss is:0.12379926\n",
            "This is the iter 9352, the d1 loss is: 3248.914, the d2 loss is: -3342.414, the g loss is: 3333.8672, the ae loss is: 0.0059139784, the jacobian loss is:0.11412622\n",
            "This is the iter 9353, the d1 loss is: 2992.9297, the d2 loss is: -3110.7734, the g loss is: 3141.5, the ae loss is: 0.007132944, the jacobian loss is:0.10231637\n",
            "This is the iter 9354, the d1 loss is: 3073.3125, the d2 loss is: -3159.8438, the g loss is: 3157.0469, the ae loss is: 0.008179485, the jacobian loss is:0.11249469\n",
            "This is the iter 9355, the d1 loss is: 3244.3281, the d2 loss is: -3341.6953, the g loss is: 3372.2031, the ae loss is: 0.007077547, the jacobian loss is:0.13415962\n",
            "This is the iter 9356, the d1 loss is: 2970.8125, the d2 loss is: -3081.5234, the g loss is: 3090.1562, the ae loss is: 0.0072526275, the jacobian loss is:0.12018278\n",
            "This is the iter 9357, the d1 loss is: 3139.0938, the d2 loss is: -3232.625, the g loss is: 3229.336, the ae loss is: 0.004851709, the jacobian loss is:0.15655178\n",
            "This is the iter 9358, the d1 loss is: 3017.9766, the d2 loss is: -3118.5547, the g loss is: 3121.9219, the ae loss is: 0.005085047, the jacobian loss is:0.14737\n",
            "This is the iter 9359, the d1 loss is: 2985.461, the d2 loss is: -3095.0469, the g loss is: 3076.3125, the ae loss is: 0.009488491, the jacobian loss is:0.14554647\n",
            "This is the iter 9360, the d1 loss is: 2973.7969, the d2 loss is: -3081.9766, the g loss is: 3129.7344, the ae loss is: 0.009705579, the jacobian loss is:0.18782362\n",
            "This is the iter 9361, the d1 loss is: 2856.6406, the d2 loss is: -2935.1406, the g loss is: 2947.1406, the ae loss is: 0.0054598385, the jacobian loss is:0.13111244\n",
            "This is the iter 9362, the d1 loss is: 2910.4531, the d2 loss is: -2990.3203, the g loss is: 2986.0, the ae loss is: 0.0048567564, the jacobian loss is:0.088849224\n",
            "This is the iter 9363, the d1 loss is: 2988.875, the d2 loss is: -3078.5547, the g loss is: 3082.5781, the ae loss is: 0.006956283, the jacobian loss is:0.10076801\n",
            "This is the iter 9364, the d1 loss is: 3020.8594, the d2 loss is: -3119.1406, the g loss is: 3122.7656, the ae loss is: 0.005912005, the jacobian loss is:0.12420871\n",
            "This is the iter 9365, the d1 loss is: 2947.2344, the d2 loss is: -3049.289, the g loss is: 2985.2031, the ae loss is: 0.0073389015, the jacobian loss is:0.11583472\n",
            "This is the iter 9366, the d1 loss is: 3214.5703, the d2 loss is: -3317.7969, the g loss is: 3361.375, the ae loss is: 0.0069681765, the jacobian loss is:0.09186932\n",
            "This is the iter 9367, the d1 loss is: 2881.9375, the d2 loss is: -2980.2344, the g loss is: 3009.7188, the ae loss is: 0.0049794074, the jacobian loss is:0.10110928\n",
            "This is the iter 9368, the d1 loss is: 2889.0781, the d2 loss is: -2983.6406, the g loss is: 2966.7812, the ae loss is: 0.0054172454, the jacobian loss is:0.10238866\n",
            "This is the iter 9369, the d1 loss is: 3105.5938, the d2 loss is: -3201.586, the g loss is: 3245.4922, the ae loss is: 0.005785876, the jacobian loss is:0.111950986\n",
            "This is the iter 9370, the d1 loss is: 2617.7969, the d2 loss is: -2707.6172, the g loss is: 2684.2422, the ae loss is: 0.0071032597, the jacobian loss is:0.11610563\n",
            "This is the iter 9371, the d1 loss is: 2902.6328, the d2 loss is: -3000.0469, the g loss is: 2978.6953, the ae loss is: 0.006389055, the jacobian loss is:0.086361565\n",
            "This is the iter 9372, the d1 loss is: 2866.336, the d2 loss is: -2948.2266, the g loss is: 2948.5781, the ae loss is: 0.006239013, the jacobian loss is:0.089408726\n",
            "This is the iter 9373, the d1 loss is: 2939.1562, the d2 loss is: -3044.8125, the g loss is: 3076.9375, the ae loss is: 0.0057237456, the jacobian loss is:0.10745531\n",
            "This is the iter 9374, the d1 loss is: 3003.4922, the d2 loss is: -3098.6562, the g loss is: 3045.7578, the ae loss is: 0.0064216945, the jacobian loss is:0.12061093\n",
            "This is the iter 9375, the d1 loss is: 2921.0938, the d2 loss is: -3010.9922, the g loss is: 3015.9219, the ae loss is: 0.007733097, the jacobian loss is:0.112427436\n",
            "This is the iter 9376, the d1 loss is: 2992.7969, the d2 loss is: -3106.1172, the g loss is: 3107.8594, the ae loss is: 0.007984999, the jacobian loss is:0.1202381\n",
            "This is the iter 9377, the d1 loss is: 3132.9922, the d2 loss is: -3243.6797, the g loss is: 3223.6172, the ae loss is: 0.007462403, the jacobian loss is:0.13289551\n",
            "This is the iter 9378, the d1 loss is: 3076.211, the d2 loss is: -3181.211, the g loss is: 3144.0625, the ae loss is: 0.009176649, the jacobian loss is:0.16442166\n",
            "This is the iter 9379, the d1 loss is: 3016.6094, the d2 loss is: -3132.0703, the g loss is: 3151.7031, the ae loss is: 0.007127366, the jacobian loss is:0.1151667\n",
            "This is the iter 9380, the d1 loss is: 2916.7656, the d2 loss is: -3034.2344, the g loss is: 3057.625, the ae loss is: 0.006657473, the jacobian loss is:0.25020215\n",
            "This is the iter 9381, the d1 loss is: 2954.1094, the d2 loss is: -3064.25, the g loss is: 3009.4375, the ae loss is: 0.004705445, the jacobian loss is:0.12694587\n",
            "This is the iter 9382, the d1 loss is: 3007.4531, the d2 loss is: -3095.4766, the g loss is: 3090.961, the ae loss is: 0.006521174, the jacobian loss is:0.13971023\n",
            "This is the iter 9383, the d1 loss is: 3172.8984, the d2 loss is: -3250.789, the g loss is: 3261.2578, the ae loss is: 0.0056510367, the jacobian loss is:0.10865361\n",
            "This is the iter 9384, the d1 loss is: 3026.625, the d2 loss is: -3110.8281, the g loss is: 3193.4219, the ae loss is: 0.0061697, the jacobian loss is:0.11446702\n",
            "This is the iter 9385, the d1 loss is: 2832.75, the d2 loss is: -2904.9062, the g loss is: 2888.7266, the ae loss is: 0.0059615374, the jacobian loss is:0.08414117\n",
            "This is the iter 9386, the d1 loss is: 3034.164, the d2 loss is: -3127.9531, the g loss is: 3099.6172, the ae loss is: 0.005677565, the jacobian loss is:0.11622544\n",
            "This is the iter 9387, the d1 loss is: 2805.1406, the d2 loss is: -2907.5156, the g loss is: 2877.4062, the ae loss is: 0.0073978305, the jacobian loss is:0.123292886\n",
            "This is the iter 9388, the d1 loss is: 3064.2188, the d2 loss is: -3176.3125, the g loss is: 3184.5078, the ae loss is: 0.0059746187, the jacobian loss is:0.101877995\n",
            "This is the iter 9389, the d1 loss is: 2912.7656, the d2 loss is: -3026.3984, the g loss is: 2967.9844, the ae loss is: 0.006886702, the jacobian loss is:0.12352775\n",
            "This is the iter 9390, the d1 loss is: 3049.6094, the d2 loss is: -3142.7656, the g loss is: 3135.7031, the ae loss is: 0.0058302972, the jacobian loss is:0.12645894\n",
            "This is the iter 9391, the d1 loss is: 2968.8281, the d2 loss is: -3061.1016, the g loss is: 3067.8906, the ae loss is: 0.0074576563, the jacobian loss is:0.13972893\n",
            "This is the iter 9392, the d1 loss is: 3024.789, the d2 loss is: -3147.9766, the g loss is: 3143.6016, the ae loss is: 0.006121292, the jacobian loss is:0.11676996\n",
            "This is the iter 9393, the d1 loss is: 2785.8594, the d2 loss is: -2867.5312, the g loss is: 2922.3438, the ae loss is: 0.004873598, the jacobian loss is:0.10511801\n",
            "This is the iter 9394, the d1 loss is: 2976.6328, the d2 loss is: -3091.5234, the g loss is: 3057.3984, the ae loss is: 0.007329124, the jacobian loss is:0.13562573\n",
            "This is the iter 9395, the d1 loss is: 2908.2812, the d2 loss is: -2997.7031, the g loss is: 2937.7734, the ae loss is: 0.009404242, the jacobian loss is:0.10920541\n",
            "This is the iter 9396, the d1 loss is: 3075.2812, the d2 loss is: -3174.2969, the g loss is: 3139.2266, the ae loss is: 0.006815782, the jacobian loss is:0.10473179\n",
            "This is the iter 9397, the d1 loss is: 2967.5938, the d2 loss is: -3078.3672, the g loss is: 3063.25, the ae loss is: 0.0074305106, the jacobian loss is:0.10188183\n",
            "This is the iter 9398, the d1 loss is: 2952.5469, the d2 loss is: -3053.4219, the g loss is: 3074.211, the ae loss is: 0.004325032, the jacobian loss is:0.114478916\n",
            "This is the iter 9399, the d1 loss is: 3014.7031, the d2 loss is: -3115.7578, the g loss is: 3099.586, the ae loss is: 0.008654025, the jacobian loss is:0.15340304\n",
            "This is the iter 9400, the d1 loss is: 3101.5, the d2 loss is: -3225.7344, the g loss is: 3196.8281, the ae loss is: 0.008995952, the jacobian loss is:0.11648394\n",
            "0.2599123\n",
            "0.9580782\n",
            "This is the iter 9401, the d1 loss is: 2934.0938, the d2 loss is: -3047.0, the g loss is: 3042.625, the ae loss is: 0.005976279, the jacobian loss is:0.12088839\n",
            "This is the iter 9402, the d1 loss is: 3011.1328, the d2 loss is: -3086.7656, the g loss is: 3070.4531, the ae loss is: 0.0074355127, the jacobian loss is:0.13284263\n",
            "This is the iter 9403, the d1 loss is: 2981.3125, the d2 loss is: -3085.7266, the g loss is: 3085.711, the ae loss is: 0.0042306716, the jacobian loss is:0.11494821\n",
            "This is the iter 9404, the d1 loss is: 3151.4375, the d2 loss is: -3242.1484, the g loss is: 3267.0156, the ae loss is: 0.0074389474, the jacobian loss is:0.10935911\n",
            "This is the iter 9405, the d1 loss is: 2940.5234, the d2 loss is: -3017.7734, the g loss is: 3071.1484, the ae loss is: 0.006733938, the jacobian loss is:0.10315513\n",
            "This is the iter 9406, the d1 loss is: 3015.0625, the d2 loss is: -3118.4688, the g loss is: 3100.9766, the ae loss is: 0.0060698926, the jacobian loss is:0.09223663\n",
            "This is the iter 9407, the d1 loss is: 2966.086, the d2 loss is: -3063.0, the g loss is: 3111.75, the ae loss is: 0.0069634807, the jacobian loss is:0.11284025\n",
            "This is the iter 9408, the d1 loss is: 2917.5938, the d2 loss is: -3019.625, the g loss is: 3019.711, the ae loss is: 0.005199656, the jacobian loss is:0.1094287\n",
            "This is the iter 9409, the d1 loss is: 3019.8672, the d2 loss is: -3126.0234, the g loss is: 3046.4219, the ae loss is: 0.0051477314, the jacobian loss is:0.117037304\n",
            "This is the iter 9410, the d1 loss is: 2922.6562, the d2 loss is: -3004.8906, the g loss is: 3085.125, the ae loss is: 0.0069813766, the jacobian loss is:0.14641502\n",
            "This is the iter 9411, the d1 loss is: 2842.8125, the d2 loss is: -2929.8594, the g loss is: 2988.4375, the ae loss is: 0.007223128, the jacobian loss is:0.15252537\n",
            "This is the iter 9412, the d1 loss is: 2842.1797, the d2 loss is: -2930.0156, the g loss is: 2937.1875, the ae loss is: 0.0069229947, the jacobian loss is:0.11046209\n",
            "This is the iter 9413, the d1 loss is: 2949.6484, the d2 loss is: -3042.164, the g loss is: 3050.414, the ae loss is: 0.006763978, the jacobian loss is:0.120855205\n",
            "This is the iter 9414, the d1 loss is: 2898.875, the d2 loss is: -2988.5312, the g loss is: 3083.4062, the ae loss is: 0.005608433, the jacobian loss is:0.124321334\n",
            "This is the iter 9415, the d1 loss is: 3097.5781, the d2 loss is: -3208.6328, the g loss is: 3222.3516, the ae loss is: 0.0057814913, the jacobian loss is:0.08257158\n",
            "This is the iter 9416, the d1 loss is: 3025.125, the d2 loss is: -3121.125, the g loss is: 3160.4219, the ae loss is: 0.006553754, the jacobian loss is:0.13866696\n",
            "This is the iter 9417, the d1 loss is: 2843.8672, the d2 loss is: -2927.7734, the g loss is: 2913.8594, the ae loss is: 0.008952759, the jacobian loss is:0.1462677\n",
            "This is the iter 9418, the d1 loss is: 2767.836, the d2 loss is: -2872.2812, the g loss is: 2851.6406, the ae loss is: 0.008301472, the jacobian loss is:0.10175168\n",
            "This is the iter 9419, the d1 loss is: 2969.2969, the d2 loss is: -3079.9688, the g loss is: 3093.75, the ae loss is: 0.0056903968, the jacobian loss is:0.13199899\n",
            "This is the iter 9420, the d1 loss is: 2865.0078, the d2 loss is: -2974.3438, the g loss is: 2948.7266, the ae loss is: 0.0047456417, the jacobian loss is:0.11288683\n",
            "This is the iter 9421, the d1 loss is: 2768.5, the d2 loss is: -2859.8516, the g loss is: 2841.586, the ae loss is: 0.006264462, the jacobian loss is:0.11969374\n",
            "This is the iter 9422, the d1 loss is: 2926.461, the d2 loss is: -3037.6172, the g loss is: 3013.4062, the ae loss is: 0.007920508, the jacobian loss is:0.115124136\n",
            "This is the iter 9423, the d1 loss is: 2944.1719, the d2 loss is: -3047.375, the g loss is: 2998.75, the ae loss is: 0.0063379672, the jacobian loss is:0.1022865\n",
            "This is the iter 9424, the d1 loss is: 2987.461, the d2 loss is: -3089.1172, the g loss is: 3114.0156, the ae loss is: 0.006549651, the jacobian loss is:0.124950536\n",
            "This is the iter 9425, the d1 loss is: 2900.3828, the d2 loss is: -3020.625, the g loss is: 3024.9297, the ae loss is: 0.0074344664, the jacobian loss is:0.09985932\n",
            "This is the iter 9426, the d1 loss is: 2988.1172, the d2 loss is: -3072.039, the g loss is: 3074.664, the ae loss is: 0.006148276, the jacobian loss is:0.11956356\n",
            "This is the iter 9427, the d1 loss is: 2842.6328, the d2 loss is: -2950.1562, the g loss is: 2969.961, the ae loss is: 0.0080873985, the jacobian loss is:0.109610766\n",
            "This is the iter 9428, the d1 loss is: 3017.7031, the d2 loss is: -3122.6719, the g loss is: 3080.7031, the ae loss is: 0.0070480295, the jacobian loss is:0.11481036\n",
            "This is the iter 9429, the d1 loss is: 3008.8281, the d2 loss is: -3096.125, the g loss is: 3057.7578, the ae loss is: 0.0070702415, the jacobian loss is:0.14441203\n",
            "This is the iter 9430, the d1 loss is: 3134.5234, the d2 loss is: -3245.6328, the g loss is: 3246.6094, the ae loss is: 0.0060255136, the jacobian loss is:0.10200466\n",
            "This is the iter 9431, the d1 loss is: 2887.8281, the d2 loss is: -2979.75, the g loss is: 2962.1094, the ae loss is: 0.009036757, the jacobian loss is:0.14642072\n",
            "This is the iter 9432, the d1 loss is: 2761.6797, the d2 loss is: -2844.9688, the g loss is: 2905.4297, the ae loss is: 0.009078901, the jacobian loss is:0.11189673\n",
            "This is the iter 9433, the d1 loss is: 2847.125, the d2 loss is: -2946.6406, the g loss is: 2998.2031, the ae loss is: 0.006535867, the jacobian loss is:0.08970103\n",
            "This is the iter 9434, the d1 loss is: 2924.5938, the d2 loss is: -3029.7656, the g loss is: 3046.9531, the ae loss is: 0.00726604, the jacobian loss is:0.13816114\n",
            "This is the iter 9435, the d1 loss is: 2771.2969, the d2 loss is: -2882.461, the g loss is: 2857.9922, the ae loss is: 0.007417338, the jacobian loss is:0.10181405\n",
            "This is the iter 9436, the d1 loss is: 3044.6875, the d2 loss is: -3153.539, the g loss is: 3107.1328, the ae loss is: 0.007722271, the jacobian loss is:0.13015482\n",
            "This is the iter 9437, the d1 loss is: 2914.2266, the d2 loss is: -2995.9688, the g loss is: 2979.8438, the ae loss is: 0.008657492, the jacobian loss is:0.09226779\n",
            "This is the iter 9438, the d1 loss is: 2842.2344, the d2 loss is: -2922.2344, the g loss is: 2925.1875, the ae loss is: 0.0052117766, the jacobian loss is:0.111653134\n",
            "This is the iter 9439, the d1 loss is: 3080.7812, the d2 loss is: -3202.7734, the g loss is: 3198.4219, the ae loss is: 0.0071019265, the jacobian loss is:0.13163666\n",
            "This is the iter 9440, the d1 loss is: 3011.5469, the d2 loss is: -3124.3125, the g loss is: 3072.3438, the ae loss is: 0.006636683, the jacobian loss is:0.107952476\n",
            "This is the iter 9441, the d1 loss is: 2963.8906, the d2 loss is: -3064.75, the g loss is: 3102.5625, the ae loss is: 0.006613721, the jacobian loss is:0.109876744\n",
            "This is the iter 9442, the d1 loss is: 2938.4531, the d2 loss is: -3038.5703, the g loss is: 3046.6484, the ae loss is: 0.008677534, the jacobian loss is:0.097763516\n",
            "This is the iter 9443, the d1 loss is: 2919.8047, the d2 loss is: -3029.4375, the g loss is: 3030.1094, the ae loss is: 0.005701463, the jacobian loss is:0.09821068\n",
            "This is the iter 9444, the d1 loss is: 2670.0234, the d2 loss is: -2791.1094, the g loss is: 2804.125, the ae loss is: 0.008153247, the jacobian loss is:0.10541504\n",
            "This is the iter 9445, the d1 loss is: 2974.9453, the d2 loss is: -3072.1172, the g loss is: 3044.4531, the ae loss is: 0.006741181, the jacobian loss is:0.12127272\n",
            "This is the iter 9446, the d1 loss is: 2926.4531, the d2 loss is: -3021.7578, the g loss is: 3000.4219, the ae loss is: 0.008599177, the jacobian loss is:0.10431084\n",
            "This is the iter 9447, the d1 loss is: 3062.1484, the d2 loss is: -3173.6562, the g loss is: 3193.9062, the ae loss is: 0.0069886255, the jacobian loss is:0.12647688\n",
            "This is the iter 9448, the d1 loss is: 2818.8594, the d2 loss is: -2916.1406, the g loss is: 2918.0156, the ae loss is: 0.006851009, the jacobian loss is:0.08437461\n",
            "This is the iter 9449, the d1 loss is: 2707.6562, the d2 loss is: -2813.2188, the g loss is: 2815.2656, the ae loss is: 0.0056425715, the jacobian loss is:0.11577828\n",
            "This is the iter 9450, the d1 loss is: 3131.8594, the d2 loss is: -3220.1953, the g loss is: 3221.5156, the ae loss is: 0.0051464257, the jacobian loss is:0.12884808\n",
            "This is the iter 9451, the d1 loss is: 2812.4375, the d2 loss is: -2886.5781, the g loss is: 2841.7656, the ae loss is: 0.006325693, the jacobian loss is:0.12687102\n",
            "This is the iter 9452, the d1 loss is: 2959.1016, the d2 loss is: -3091.9922, the g loss is: 3124.8125, the ae loss is: 0.0062579126, the jacobian loss is:0.12099537\n",
            "This is the iter 9453, the d1 loss is: 2882.125, the d2 loss is: -2979.5078, the g loss is: 2956.5547, the ae loss is: 0.007455079, the jacobian loss is:0.09208872\n",
            "This is the iter 9454, the d1 loss is: 2942.9531, the d2 loss is: -3023.9844, the g loss is: 3082.9375, the ae loss is: 0.0063713267, the jacobian loss is:0.10169166\n",
            "This is the iter 9455, the d1 loss is: 2939.0078, the d2 loss is: -3059.0781, the g loss is: 3056.914, the ae loss is: 0.0088825645, the jacobian loss is:0.14109038\n",
            "This is the iter 9456, the d1 loss is: 3117.8594, the d2 loss is: -3226.6016, the g loss is: 3241.9062, the ae loss is: 0.008252546, the jacobian loss is:0.10470914\n",
            "This is the iter 9457, the d1 loss is: 2865.7578, the d2 loss is: -2963.375, the g loss is: 2964.0312, the ae loss is: 0.0067176046, the jacobian loss is:0.12522876\n",
            "This is the iter 9458, the d1 loss is: 2930.461, the d2 loss is: -3031.2656, the g loss is: 2895.5703, the ae loss is: 0.0070444914, the jacobian loss is:0.12537152\n",
            "This is the iter 9459, the d1 loss is: 2943.6953, the d2 loss is: -3049.1875, the g loss is: 3031.4219, the ae loss is: 0.0056812866, the jacobian loss is:0.1332356\n",
            "This is the iter 9460, the d1 loss is: 2806.1562, the d2 loss is: -2893.7188, the g loss is: 2936.4219, the ae loss is: 0.007361174, the jacobian loss is:0.09587039\n",
            "This is the iter 9461, the d1 loss is: 3226.7734, the d2 loss is: -3322.9375, the g loss is: 3308.3906, the ae loss is: 0.007336218, the jacobian loss is:0.13038515\n",
            "This is the iter 9462, the d1 loss is: 3073.5, the d2 loss is: -3155.1406, the g loss is: 3141.086, the ae loss is: 0.0056407675, the jacobian loss is:0.11085208\n",
            "This is the iter 9463, the d1 loss is: 3166.7734, the d2 loss is: -3264.0781, the g loss is: 3238.0469, the ae loss is: 0.0071729203, the jacobian loss is:0.106068596\n",
            "This is the iter 9464, the d1 loss is: 2657.8438, the d2 loss is: -2749.6484, the g loss is: 2827.1094, the ae loss is: 0.0071946625, the jacobian loss is:0.093846075\n",
            "This is the iter 9465, the d1 loss is: 3003.539, the d2 loss is: -3103.289, the g loss is: 3116.3516, the ae loss is: 0.0052232053, the jacobian loss is:0.07687929\n",
            "This is the iter 9466, the d1 loss is: 2985.0938, the d2 loss is: -3086.711, the g loss is: 3022.8281, the ae loss is: 0.0067231255, the jacobian loss is:0.08104805\n",
            "This is the iter 9467, the d1 loss is: 3137.1406, the d2 loss is: -3222.0547, the g loss is: 3236.039, the ae loss is: 0.005575531, the jacobian loss is:0.10564254\n",
            "This is the iter 9468, the d1 loss is: 3143.039, the d2 loss is: -3238.7266, the g loss is: 3212.7578, the ae loss is: 0.00430659, the jacobian loss is:0.09121237\n",
            "This is the iter 9469, the d1 loss is: 2926.1094, the d2 loss is: -3008.2188, the g loss is: 3039.6172, the ae loss is: 0.004583814, the jacobian loss is:0.13805789\n",
            "This is the iter 9470, the d1 loss is: 3130.5312, the d2 loss is: -3233.2344, the g loss is: 3246.6328, the ae loss is: 0.004789704, the jacobian loss is:0.086918816\n",
            "This is the iter 9471, the d1 loss is: 2933.9844, the d2 loss is: -3033.3828, the g loss is: 3066.8672, the ae loss is: 0.006727844, the jacobian loss is:0.09704179\n",
            "This is the iter 9472, the d1 loss is: 3277.539, the d2 loss is: -3377.5938, the g loss is: 3316.1484, the ae loss is: 0.0058905105, the jacobian loss is:0.12914751\n",
            "This is the iter 9473, the d1 loss is: 3118.1484, the d2 loss is: -3220.5625, the g loss is: 3235.5156, the ae loss is: 0.008008561, the jacobian loss is:0.1244861\n",
            "This is the iter 9474, the d1 loss is: 2762.9375, the d2 loss is: -2851.4844, the g loss is: 2894.6797, the ae loss is: 0.005799346, the jacobian loss is:0.10475903\n",
            "This is the iter 9475, the d1 loss is: 2744.211, the d2 loss is: -2843.7031, the g loss is: 2846.9219, the ae loss is: 0.005928361, the jacobian loss is:0.123341486\n",
            "This is the iter 9476, the d1 loss is: 3108.375, the d2 loss is: -3175.5234, the g loss is: 3182.875, the ae loss is: 0.006690726, the jacobian loss is:0.08613623\n",
            "This is the iter 9477, the d1 loss is: 3149.1875, the d2 loss is: -3242.5234, the g loss is: 3246.5938, the ae loss is: 0.007413609, the jacobian loss is:0.10828211\n",
            "This is the iter 9478, the d1 loss is: 2932.3594, the d2 loss is: -3020.1797, the g loss is: 3059.7656, the ae loss is: 0.007703555, the jacobian loss is:0.14388806\n",
            "This is the iter 9479, the d1 loss is: 3010.2656, the d2 loss is: -3100.6016, the g loss is: 3139.914, the ae loss is: 0.0053842906, the jacobian loss is:0.095042385\n",
            "This is the iter 9480, the d1 loss is: 2817.8438, the d2 loss is: -2913.6094, the g loss is: 2924.6719, the ae loss is: 0.0051129707, the jacobian loss is:0.0953178\n",
            "This is the iter 9481, the d1 loss is: 3162.6562, the d2 loss is: -3248.9531, the g loss is: 3364.7969, the ae loss is: 0.0073767146, the jacobian loss is:0.08288739\n",
            "This is the iter 9482, the d1 loss is: 3135.4219, the d2 loss is: -3250.7969, the g loss is: 3216.1562, the ae loss is: 0.007587868, the jacobian loss is:0.12427428\n",
            "This is the iter 9483, the d1 loss is: 2927.1406, the d2 loss is: -3011.2188, the g loss is: 3098.3828, the ae loss is: 0.0061141374, the jacobian loss is:0.11039416\n",
            "This is the iter 9484, the d1 loss is: 2808.4844, the d2 loss is: -2898.7812, the g loss is: 2866.6484, the ae loss is: 0.005417997, the jacobian loss is:0.11541068\n",
            "This is the iter 9485, the d1 loss is: 2835.4219, the d2 loss is: -2939.2812, the g loss is: 2899.5469, the ae loss is: 0.0074639847, the jacobian loss is:0.09333116\n",
            "This is the iter 9486, the d1 loss is: 3230.625, the d2 loss is: -3332.7344, the g loss is: 3283.9844, the ae loss is: 0.010235045, the jacobian loss is:0.107506305\n",
            "This is the iter 9487, the d1 loss is: 2730.6016, the d2 loss is: -2821.1562, the g loss is: 2806.6094, the ae loss is: 0.006082915, the jacobian loss is:0.10506596\n",
            "This is the iter 9488, the d1 loss is: 2881.6094, the d2 loss is: -2977.9531, the g loss is: 2960.4844, the ae loss is: 0.009853549, the jacobian loss is:0.0882206\n",
            "This is the iter 9489, the d1 loss is: 3117.6797, the d2 loss is: -3200.0938, the g loss is: 3195.5, the ae loss is: 0.0058177216, the jacobian loss is:0.12350703\n",
            "This is the iter 9490, the d1 loss is: 2997.2188, the d2 loss is: -3107.8828, the g loss is: 3082.414, the ae loss is: 0.0079079345, the jacobian loss is:0.101358354\n",
            "This is the iter 9491, the d1 loss is: 2926.164, the d2 loss is: -3035.2812, the g loss is: 3038.8125, the ae loss is: 0.0053838943, the jacobian loss is:0.09638867\n",
            "This is the iter 9492, the d1 loss is: 3009.789, the d2 loss is: -3107.3594, the g loss is: 3111.0938, the ae loss is: 0.0067263492, the jacobian loss is:0.09035635\n",
            "This is the iter 9493, the d1 loss is: 3159.5156, the d2 loss is: -3267.1094, the g loss is: 3221.2266, the ae loss is: 0.006700041, the jacobian loss is:0.21265191\n",
            "This is the iter 9494, the d1 loss is: 2769.8125, the d2 loss is: -2838.6719, the g loss is: 2847.914, the ae loss is: 0.004165179, the jacobian loss is:0.082167424\n",
            "This is the iter 9495, the d1 loss is: 3133.4219, the d2 loss is: -3225.4688, the g loss is: 3241.5938, the ae loss is: 0.00728303, the jacobian loss is:0.12333361\n",
            "This is the iter 9496, the d1 loss is: 3091.5234, the d2 loss is: -3178.4766, the g loss is: 3164.539, the ae loss is: 0.0050588883, the jacobian loss is:0.093694136\n",
            "This is the iter 9497, the d1 loss is: 2969.2188, the d2 loss is: -3091.2422, the g loss is: 3075.7031, the ae loss is: 0.006804315, the jacobian loss is:0.10993707\n",
            "This is the iter 9498, the d1 loss is: 2983.0625, the d2 loss is: -3077.1406, the g loss is: 3122.7578, the ae loss is: 0.008156484, the jacobian loss is:0.09677345\n",
            "This is the iter 9499, the d1 loss is: 2960.711, the d2 loss is: -3052.4297, the g loss is: 3032.25, the ae loss is: 0.006817642, the jacobian loss is:0.094429374\n",
            "This is the iter 9500, the d1 loss is: 3214.6875, the d2 loss is: -3325.7969, the g loss is: 3323.3672, the ae loss is: 0.008307493, the jacobian loss is:0.09313419\n",
            "0.26150286\n",
            "0.9651488\n",
            "This is the iter 9501, the d1 loss is: 2932.7188, the d2 loss is: -3052.5312, the g loss is: 3064.4375, the ae loss is: 0.0076967813, the jacobian loss is:0.10736558\n",
            "This is the iter 9502, the d1 loss is: 3258.25, the d2 loss is: -3358.711, the g loss is: 3350.5312, the ae loss is: 0.006262795, the jacobian loss is:0.092826955\n",
            "This is the iter 9503, the d1 loss is: 2936.2031, the d2 loss is: -3052.5, the g loss is: 3017.3828, the ae loss is: 0.006277576, the jacobian loss is:0.09360102\n",
            "This is the iter 9504, the d1 loss is: 3066.7734, the d2 loss is: -3153.1953, the g loss is: 3092.7656, the ae loss is: 0.006475786, the jacobian loss is:0.09348948\n",
            "This is the iter 9505, the d1 loss is: 2947.5078, the d2 loss is: -3061.0938, the g loss is: 3009.3906, the ae loss is: 0.011808077, the jacobian loss is:0.14369439\n",
            "This is the iter 9506, the d1 loss is: 3170.539, the d2 loss is: -3263.1094, the g loss is: 3255.164, the ae loss is: 0.008676215, the jacobian loss is:0.10239548\n",
            "This is the iter 9507, the d1 loss is: 3221.5469, the d2 loss is: -3301.8516, the g loss is: 3277.8906, the ae loss is: 0.006420674, the jacobian loss is:0.10355005\n",
            "This is the iter 9508, the d1 loss is: 3024.0781, the d2 loss is: -3135.0938, the g loss is: 3120.4375, the ae loss is: 0.006503321, the jacobian loss is:0.08420802\n",
            "This is the iter 9509, the d1 loss is: 3073.8594, the d2 loss is: -3168.0938, the g loss is: 3099.711, the ae loss is: 0.009527569, the jacobian loss is:0.14002827\n",
            "This is the iter 9510, the d1 loss is: 3103.25, the d2 loss is: -3204.875, the g loss is: 3178.2422, the ae loss is: 0.007434108, the jacobian loss is:0.0898837\n",
            "This is the iter 9511, the d1 loss is: 2959.625, the d2 loss is: -3067.3438, the g loss is: 3076.3125, the ae loss is: 0.004975997, the jacobian loss is:0.08392479\n",
            "This is the iter 9512, the d1 loss is: 3143.3125, the d2 loss is: -3231.5625, the g loss is: 3226.8438, the ae loss is: 0.006398499, the jacobian loss is:0.087549716\n",
            "This is the iter 9513, the d1 loss is: 2800.1953, the d2 loss is: -2880.1406, the g loss is: 2839.8906, the ae loss is: 0.0066499948, the jacobian loss is:0.10599772\n",
            "This is the iter 9514, the d1 loss is: 3082.8438, the d2 loss is: -3158.4766, the g loss is: 3141.4844, the ae loss is: 0.007314281, the jacobian loss is:0.10072939\n",
            "This is the iter 9515, the d1 loss is: 2926.5469, the d2 loss is: -3022.5547, the g loss is: 3049.6172, the ae loss is: 0.0037613655, the jacobian loss is:0.085010625\n",
            "This is the iter 9516, the d1 loss is: 3257.3828, the d2 loss is: -3342.1406, the g loss is: 3330.3047, the ae loss is: 0.0078714695, the jacobian loss is:0.09967025\n",
            "This is the iter 9517, the d1 loss is: 3175.961, the d2 loss is: -3267.8672, the g loss is: 3257.0156, the ae loss is: 0.0057362043, the jacobian loss is:0.16771935\n",
            "This is the iter 9518, the d1 loss is: 3081.9297, the d2 loss is: -3167.5781, the g loss is: 3254.75, the ae loss is: 0.006479988, the jacobian loss is:0.070957005\n",
            "This is the iter 9519, the d1 loss is: 2995.8828, the d2 loss is: -3090.3594, the g loss is: 3100.6953, the ae loss is: 0.0055715656, the jacobian loss is:0.09736474\n",
            "This is the iter 9520, the d1 loss is: 3003.8047, the d2 loss is: -3078.8672, the g loss is: 3047.1172, the ae loss is: 0.009227714, the jacobian loss is:0.08784225\n",
            "This is the iter 9521, the d1 loss is: 3316.0234, the d2 loss is: -3398.8906, the g loss is: 3378.2031, the ae loss is: 0.0071311165, the jacobian loss is:0.0842017\n",
            "This is the iter 9522, the d1 loss is: 2818.3984, the d2 loss is: -2920.0938, the g loss is: 2908.7344, the ae loss is: 0.0056674103, the jacobian loss is:0.08336164\n",
            "This is the iter 9523, the d1 loss is: 3040.1406, the d2 loss is: -3145.9844, the g loss is: 3128.3438, the ae loss is: 0.00852496, the jacobian loss is:0.10931104\n",
            "This is the iter 9524, the d1 loss is: 2957.5, the d2 loss is: -3044.5156, the g loss is: 3074.6797, the ae loss is: 0.007999891, the jacobian loss is:0.114723794\n",
            "This is the iter 9525, the d1 loss is: 3088.1562, the d2 loss is: -3198.2812, the g loss is: 3158.539, the ae loss is: 0.00607043, the jacobian loss is:0.10937117\n",
            "This is the iter 9526, the d1 loss is: 3065.6562, the d2 loss is: -3156.539, the g loss is: 3162.3281, the ae loss is: 0.0061269687, the jacobian loss is:0.08295397\n",
            "This is the iter 9527, the d1 loss is: 3044.711, the d2 loss is: -3128.0547, the g loss is: 3127.6562, the ae loss is: 0.0064063207, the jacobian loss is:0.115711085\n",
            "This is the iter 9528, the d1 loss is: 3054.8438, the d2 loss is: -3160.3125, the g loss is: 3186.1406, the ae loss is: 0.006499799, the jacobian loss is:0.07327146\n",
            "This is the iter 9529, the d1 loss is: 3022.0781, the d2 loss is: -3107.9922, the g loss is: 3130.0234, the ae loss is: 0.005801858, the jacobian loss is:0.08544481\n",
            "This is the iter 9530, the d1 loss is: 3354.3047, the d2 loss is: -3436.6094, the g loss is: 3416.9297, the ae loss is: 0.006343287, the jacobian loss is:0.11003196\n",
            "This is the iter 9531, the d1 loss is: 2943.125, the d2 loss is: -3043.7656, the g loss is: 3078.0234, the ae loss is: 0.008352693, the jacobian loss is:0.09745177\n",
            "This is the iter 9532, the d1 loss is: 3116.9297, the d2 loss is: -3233.3203, the g loss is: 3215.0547, the ae loss is: 0.0075627174, the jacobian loss is:0.104956694\n",
            "This is the iter 9533, the d1 loss is: 2863.4844, the d2 loss is: -2955.6406, the g loss is: 2985.6016, the ae loss is: 0.0071217455, the jacobian loss is:0.07359186\n",
            "This is the iter 9534, the d1 loss is: 2815.5, the d2 loss is: -2905.5078, the g loss is: 2920.8594, the ae loss is: 0.0065288506, the jacobian loss is:0.09409625\n",
            "This is the iter 9535, the d1 loss is: 3082.8125, the d2 loss is: -3164.1094, the g loss is: 3133.1484, the ae loss is: 0.0072253356, the jacobian loss is:0.07107273\n",
            "This is the iter 9536, the d1 loss is: 3049.8594, the d2 loss is: -3135.586, the g loss is: 3149.3125, the ae loss is: 0.008667851, the jacobian loss is:0.08187169\n",
            "This is the iter 9537, the d1 loss is: 3009.7812, the d2 loss is: -3089.8906, the g loss is: 3094.5234, the ae loss is: 0.0046955813, the jacobian loss is:0.1368059\n",
            "This is the iter 9538, the d1 loss is: 3093.9688, the d2 loss is: -3168.1484, the g loss is: 3129.211, the ae loss is: 0.0062975404, the jacobian loss is:0.17255437\n",
            "This is the iter 9539, the d1 loss is: 2916.4766, the d2 loss is: -2999.211, the g loss is: 2977.3281, the ae loss is: 0.0060264524, the jacobian loss is:0.08607031\n",
            "This is the iter 9540, the d1 loss is: 3167.5547, the d2 loss is: -3263.4062, the g loss is: 3252.7344, the ae loss is: 0.006935874, the jacobian loss is:0.07196429\n",
            "This is the iter 9541, the d1 loss is: 2806.5625, the d2 loss is: -2902.375, the g loss is: 2873.4922, the ae loss is: 0.0063156374, the jacobian loss is:0.08777858\n",
            "This is the iter 9542, the d1 loss is: 2834.0469, the d2 loss is: -2926.4531, the g loss is: 2927.789, the ae loss is: 0.0053319614, the jacobian loss is:0.08212818\n",
            "This is the iter 9543, the d1 loss is: 3102.289, the d2 loss is: -3194.211, the g loss is: 3280.6875, the ae loss is: 0.006273628, the jacobian loss is:0.08722288\n",
            "This is the iter 9544, the d1 loss is: 2981.6484, the d2 loss is: -3084.3281, the g loss is: 3097.5078, the ae loss is: 0.0075365743, the jacobian loss is:0.0954007\n",
            "This is the iter 9545, the d1 loss is: 2768.4297, the d2 loss is: -2868.7734, the g loss is: 2900.211, the ae loss is: 0.0052903923, the jacobian loss is:0.10599959\n",
            "This is the iter 9546, the d1 loss is: 3073.1562, the d2 loss is: -3144.4375, the g loss is: 3184.8203, the ae loss is: 0.007653338, the jacobian loss is:0.08720161\n",
            "This is the iter 9547, the d1 loss is: 3051.414, the d2 loss is: -3164.4375, the g loss is: 3118.461, the ae loss is: 0.005573099, the jacobian loss is:0.087462686\n",
            "This is the iter 9548, the d1 loss is: 3118.8281, the d2 loss is: -3233.1016, the g loss is: 3256.375, the ae loss is: 0.005279125, the jacobian loss is:0.09141269\n",
            "This is the iter 9549, the d1 loss is: 2934.789, the d2 loss is: -2997.3594, the g loss is: 2980.4375, the ae loss is: 0.0045429454, the jacobian loss is:0.07922622\n",
            "This is the iter 9550, the d1 loss is: 3081.5703, the d2 loss is: -3171.1094, the g loss is: 3179.1719, the ae loss is: 0.0065242243, the jacobian loss is:0.08852415\n",
            "This is the iter 9551, the d1 loss is: 3110.6719, the d2 loss is: -3188.2031, the g loss is: 3177.0625, the ae loss is: 0.0063162767, the jacobian loss is:0.077557266\n",
            "This is the iter 9552, the d1 loss is: 2953.5938, the d2 loss is: -3040.6797, the g loss is: 2991.9219, the ae loss is: 0.006774836, the jacobian loss is:0.10369981\n",
            "This is the iter 9553, the d1 loss is: 3001.4688, the d2 loss is: -3113.8125, the g loss is: 3050.8906, the ae loss is: 0.008864639, the jacobian loss is:0.11050469\n",
            "This is the iter 9554, the d1 loss is: 2679.5938, the d2 loss is: -2773.5625, the g loss is: 2783.9219, the ae loss is: 0.0073695593, the jacobian loss is:0.08609264\n",
            "This is the iter 9555, the d1 loss is: 2767.414, the d2 loss is: -2826.9531, the g loss is: 2914.2656, the ae loss is: 0.0062025385, the jacobian loss is:0.07151759\n",
            "This is the iter 9556, the d1 loss is: 2939.25, the d2 loss is: -3031.9375, the g loss is: 3068.6953, the ae loss is: 0.0064924713, the jacobian loss is:0.091539755\n",
            "This is the iter 9557, the d1 loss is: 2821.875, the d2 loss is: -2898.6406, the g loss is: 2862.8594, the ae loss is: 0.004165527, the jacobian loss is:0.07140195\n",
            "This is the iter 9558, the d1 loss is: 3141.5625, the d2 loss is: -3231.1406, the g loss is: 3310.6484, the ae loss is: 0.006073188, the jacobian loss is:0.083282\n",
            "This is the iter 9559, the d1 loss is: 3142.9297, the d2 loss is: -3247.2969, the g loss is: 3217.0156, the ae loss is: 0.0069509824, the jacobian loss is:0.0962098\n",
            "This is the iter 9560, the d1 loss is: 2808.6094, the d2 loss is: -2926.7969, the g loss is: 2955.8125, the ae loss is: 0.0069056237, the jacobian loss is:0.094741195\n",
            "This is the iter 9561, the d1 loss is: 2937.7188, the d2 loss is: -3025.1094, the g loss is: 3035.664, the ae loss is: 0.0051932316, the jacobian loss is:0.08846572\n",
            "This is the iter 9562, the d1 loss is: 2966.3828, the d2 loss is: -3065.7734, the g loss is: 3063.9375, the ae loss is: 0.0068650963, the jacobian loss is:0.08664865\n",
            "This is the iter 9563, the d1 loss is: 2957.5, the d2 loss is: -3048.2188, the g loss is: 3037.289, the ae loss is: 0.0059907115, the jacobian loss is:0.1165368\n",
            "This is the iter 9564, the d1 loss is: 2972.6719, the d2 loss is: -3079.5469, the g loss is: 3091.4844, the ae loss is: 0.004547124, the jacobian loss is:0.11870231\n",
            "This is the iter 9565, the d1 loss is: 2855.9844, the d2 loss is: -2960.6016, the g loss is: 2994.3438, the ae loss is: 0.005630851, the jacobian loss is:0.08317595\n",
            "This is the iter 9566, the d1 loss is: 2950.4688, the d2 loss is: -3052.4922, the g loss is: 2982.8125, the ae loss is: 0.007957258, the jacobian loss is:0.1005101\n",
            "This is the iter 9567, the d1 loss is: 3183.1328, the d2 loss is: -3289.2969, the g loss is: 3245.211, the ae loss is: 0.00620183, the jacobian loss is:0.072570555\n",
            "This is the iter 9568, the d1 loss is: 2953.625, the d2 loss is: -3049.3203, the g loss is: 3041.8984, the ae loss is: 0.0045859604, the jacobian loss is:0.07065163\n",
            "This is the iter 9569, the d1 loss is: 2926.5703, the d2 loss is: -3039.8281, the g loss is: 3060.1562, the ae loss is: 0.0065328814, the jacobian loss is:0.08705538\n",
            "This is the iter 9570, the d1 loss is: 2899.1719, the d2 loss is: -2981.5234, the g loss is: 2949.7031, the ae loss is: 0.007700231, the jacobian loss is:0.09491044\n",
            "This is the iter 9571, the d1 loss is: 2967.3125, the d2 loss is: -3046.7734, the g loss is: 3018.8125, the ae loss is: 0.003948747, the jacobian loss is:0.14359966\n",
            "This is the iter 9572, the d1 loss is: 2980.2578, the d2 loss is: -3095.4531, the g loss is: 3107.664, the ae loss is: 0.006136922, the jacobian loss is:0.078323655\n",
            "This is the iter 9573, the d1 loss is: 2939.914, the d2 loss is: -3036.8906, the g loss is: 3040.7656, the ae loss is: 0.006249128, the jacobian loss is:0.07728965\n",
            "This is the iter 9574, the d1 loss is: 2903.9531, the d2 loss is: -2992.5469, the g loss is: 3064.8828, the ae loss is: 0.0067270896, the jacobian loss is:0.15459843\n",
            "This is the iter 9575, the d1 loss is: 2909.7422, the d2 loss is: -2978.2812, the g loss is: 3005.0312, the ae loss is: 0.006262122, the jacobian loss is:0.076537475\n",
            "This is the iter 9576, the d1 loss is: 3059.1406, the d2 loss is: -3158.375, the g loss is: 3182.1875, the ae loss is: 0.006108347, the jacobian loss is:0.09139457\n",
            "This is the iter 9577, the d1 loss is: 2970.9297, the d2 loss is: -3063.6797, the g loss is: 3033.1797, the ae loss is: 0.008286006, the jacobian loss is:0.068023\n",
            "This is the iter 9578, the d1 loss is: 2773.3438, the d2 loss is: -2867.9062, the g loss is: 2876.5469, the ae loss is: 0.0064476165, the jacobian loss is:0.08641556\n",
            "This is the iter 9579, the d1 loss is: 3057.4688, the d2 loss is: -3155.3125, the g loss is: 3124.961, the ae loss is: 0.0071864333, the jacobian loss is:0.08856461\n",
            "This is the iter 9580, the d1 loss is: 2984.7656, the d2 loss is: -3063.7031, the g loss is: 3118.3516, the ae loss is: 0.005685946, the jacobian loss is:0.098529525\n",
            "This is the iter 9581, the d1 loss is: 2884.1953, the d2 loss is: -2977.8984, the g loss is: 2988.7656, the ae loss is: 0.0080288, the jacobian loss is:0.08787085\n",
            "This is the iter 9582, the d1 loss is: 2906.3984, the d2 loss is: -2989.2656, the g loss is: 3020.2656, the ae loss is: 0.0058067413, the jacobian loss is:0.06781185\n",
            "This is the iter 9583, the d1 loss is: 2861.586, the d2 loss is: -2936.7812, the g loss is: 3008.9688, the ae loss is: 0.004396751, the jacobian loss is:0.11686048\n",
            "This is the iter 9584, the d1 loss is: 2944.3672, the d2 loss is: -3029.125, the g loss is: 2996.3438, the ae loss is: 0.006464179, the jacobian loss is:0.09740116\n",
            "This is the iter 9585, the d1 loss is: 2889.4922, the d2 loss is: -2984.1562, the g loss is: 2963.1094, the ae loss is: 0.006818692, the jacobian loss is:0.12246376\n",
            "This is the iter 9586, the d1 loss is: 2871.164, the d2 loss is: -2969.4062, the g loss is: 2954.836, the ae loss is: 0.0063910447, the jacobian loss is:0.08963625\n",
            "This is the iter 9587, the d1 loss is: 3186.5781, the d2 loss is: -3293.7422, the g loss is: 3293.4688, the ae loss is: 0.009639562, the jacobian loss is:0.07172695\n",
            "This is the iter 9588, the d1 loss is: 2862.5625, the d2 loss is: -2977.8906, the g loss is: 2957.0156, the ae loss is: 0.0074923085, the jacobian loss is:0.13014114\n",
            "This is the iter 9589, the d1 loss is: 3043.75, the d2 loss is: -3142.1797, the g loss is: 3183.0312, the ae loss is: 0.0047058994, the jacobian loss is:0.08303562\n",
            "This is the iter 9590, the d1 loss is: 2941.0156, the d2 loss is: -3033.7656, the g loss is: 3008.1172, the ae loss is: 0.005894482, the jacobian loss is:0.1100938\n",
            "This is the iter 9591, the d1 loss is: 2808.375, the d2 loss is: -2905.289, the g loss is: 2871.4922, the ae loss is: 0.00696179, the jacobian loss is:0.10304892\n",
            "This is the iter 9592, the d1 loss is: 3083.2344, the d2 loss is: -3162.3047, the g loss is: 3164.9219, the ae loss is: 0.008773867, the jacobian loss is:0.12296371\n",
            "This is the iter 9593, the d1 loss is: 3113.1016, the d2 loss is: -3201.9688, the g loss is: 3185.4453, the ae loss is: 0.006974633, the jacobian loss is:0.10258047\n",
            "This is the iter 9594, the d1 loss is: 3034.1406, the d2 loss is: -3109.1016, the g loss is: 3082.7578, the ae loss is: 0.007525202, the jacobian loss is:0.10869482\n",
            "This is the iter 9595, the d1 loss is: 2939.3203, the d2 loss is: -3034.0547, the g loss is: 3099.0469, the ae loss is: 0.0045850794, the jacobian loss is:0.08140444\n",
            "This is the iter 9596, the d1 loss is: 2965.2656, the d2 loss is: -3059.1172, the g loss is: 3043.1094, the ae loss is: 0.0062373765, the jacobian loss is:0.10427869\n",
            "This is the iter 9597, the d1 loss is: 2989.3594, the d2 loss is: -3078.875, the g loss is: 3026.4219, the ae loss is: 0.005499628, the jacobian loss is:0.07674743\n",
            "This is the iter 9598, the d1 loss is: 2875.2969, the d2 loss is: -2944.2578, the g loss is: 2981.75, the ae loss is: 0.006065781, the jacobian loss is:0.11597954\n",
            "This is the iter 9599, the d1 loss is: 3171.0469, the d2 loss is: -3282.1406, the g loss is: 3226.1172, the ae loss is: 0.004849592, the jacobian loss is:0.08270872\n",
            "This is the iter 9600, the d1 loss is: 2974.9766, the d2 loss is: -3072.1484, the g loss is: 3037.75, the ae loss is: 0.0059109684, the jacobian loss is:0.12709731\n",
            "0.24945267\n",
            "0.914122\n",
            "This is the iter 9601, the d1 loss is: 2778.4844, the d2 loss is: -2876.7344, the g loss is: 2828.039, the ae loss is: 0.005611567, the jacobian loss is:0.09020053\n",
            "This is the iter 9602, the d1 loss is: 3149.2969, the d2 loss is: -3235.4375, the g loss is: 3222.3516, the ae loss is: 0.0070437836, the jacobian loss is:0.09735811\n",
            "This is the iter 9603, the d1 loss is: 2866.4219, the d2 loss is: -2950.6797, the g loss is: 3015.4453, the ae loss is: 0.0034564445, the jacobian loss is:0.09586192\n",
            "This is the iter 9604, the d1 loss is: 2797.3594, the d2 loss is: -2891.2031, the g loss is: 2852.0, the ae loss is: 0.005452879, the jacobian loss is:0.13487856\n",
            "This is the iter 9605, the d1 loss is: 2860.0781, the d2 loss is: -2956.5, the g loss is: 2969.0156, the ae loss is: 0.0076119066, the jacobian loss is:0.07431554\n",
            "This is the iter 9606, the d1 loss is: 2973.1016, the d2 loss is: -3089.8516, the g loss is: 3047.336, the ae loss is: 0.0063641025, the jacobian loss is:0.07545873\n",
            "This is the iter 9607, the d1 loss is: 2999.8047, the d2 loss is: -3104.0781, the g loss is: 3073.5547, the ae loss is: 0.006039156, the jacobian loss is:0.10043711\n",
            "This is the iter 9608, the d1 loss is: 2924.1875, the d2 loss is: -3020.3438, the g loss is: 2992.1094, the ae loss is: 0.0064879744, the jacobian loss is:0.08565903\n",
            "This is the iter 9609, the d1 loss is: 2728.711, the d2 loss is: -2833.4297, the g loss is: 2808.961, the ae loss is: 0.0048631555, the jacobian loss is:0.088503435\n",
            "This is the iter 9610, the d1 loss is: 2926.3438, the d2 loss is: -2976.3438, the g loss is: 3101.8594, the ae loss is: 0.0045008548, the jacobian loss is:0.084917895\n",
            "This is the iter 9611, the d1 loss is: 2868.2969, the d2 loss is: -2969.0625, the g loss is: 3059.5234, the ae loss is: 0.0067625237, the jacobian loss is:0.095104285\n",
            "This is the iter 9612, the d1 loss is: 2944.1094, the d2 loss is: -3038.8438, the g loss is: 3010.8047, the ae loss is: 0.00732042, the jacobian loss is:0.08420028\n",
            "This is the iter 9613, the d1 loss is: 2879.4531, the d2 loss is: -2972.9375, the g loss is: 2952.836, the ae loss is: 0.005941756, the jacobian loss is:0.09702521\n",
            "This is the iter 9614, the d1 loss is: 2913.0625, the d2 loss is: -2998.2344, the g loss is: 2958.461, the ae loss is: 0.007611908, the jacobian loss is:0.1310886\n",
            "This is the iter 9615, the d1 loss is: 3120.3906, the d2 loss is: -3220.9531, the g loss is: 3271.9219, the ae loss is: 0.0068911333, the jacobian loss is:0.1025031\n",
            "This is the iter 9616, the d1 loss is: 2792.1172, the d2 loss is: -2880.6875, the g loss is: 2937.5312, the ae loss is: 0.0062155286, the jacobian loss is:0.10074837\n",
            "This is the iter 9617, the d1 loss is: 2727.539, the d2 loss is: -2813.8516, the g loss is: 2768.0156, the ae loss is: 0.0070361802, the jacobian loss is:0.08549513\n",
            "This is the iter 9618, the d1 loss is: 3014.3906, the d2 loss is: -3078.789, the g loss is: 3085.039, the ae loss is: 0.0076703234, the jacobian loss is:0.11763147\n",
            "This is the iter 9619, the d1 loss is: 2791.1484, the d2 loss is: -2872.8438, the g loss is: 2830.6875, the ae loss is: 0.007083113, the jacobian loss is:0.07332724\n",
            "This is the iter 9620, the d1 loss is: 2934.4062, the d2 loss is: -3022.6484, the g loss is: 2981.125, the ae loss is: 0.00683925, the jacobian loss is:0.075915426\n",
            "This is the iter 9621, the d1 loss is: 2867.1328, the d2 loss is: -2949.6328, the g loss is: 2921.2031, the ae loss is: 0.0066373893, the jacobian loss is:0.10607408\n",
            "This is the iter 9622, the d1 loss is: 2969.0625, the d2 loss is: -3046.6094, the g loss is: 3026.5938, the ae loss is: 0.006226126, the jacobian loss is:0.10340103\n",
            "This is the iter 9623, the d1 loss is: 2937.789, the d2 loss is: -3027.75, the g loss is: 3033.0938, the ae loss is: 0.005596858, the jacobian loss is:0.074982174\n",
            "This is the iter 9624, the d1 loss is: 3020.7188, the d2 loss is: -3085.8594, the g loss is: 3076.6953, the ae loss is: 0.0076425937, the jacobian loss is:0.06914905\n",
            "This is the iter 9625, the d1 loss is: 2832.375, the d2 loss is: -2905.9531, the g loss is: 2878.7969, the ae loss is: 0.0074678753, the jacobian loss is:0.06907918\n",
            "This is the iter 9626, the d1 loss is: 3074.2969, the d2 loss is: -3161.375, the g loss is: 3116.5781, the ae loss is: 0.005575994, the jacobian loss is:0.08074015\n",
            "This is the iter 9627, the d1 loss is: 2925.2734, the d2 loss is: -3007.0469, the g loss is: 2991.5469, the ae loss is: 0.008392696, the jacobian loss is:0.07595398\n",
            "This is the iter 9628, the d1 loss is: 3031.1875, the d2 loss is: -3113.6484, the g loss is: 3135.8594, the ae loss is: 0.007369267, the jacobian loss is:0.08742705\n",
            "This is the iter 9629, the d1 loss is: 3005.5, the d2 loss is: -3073.664, the g loss is: 3056.8438, the ae loss is: 0.0058953296, the jacobian loss is:0.11085031\n",
            "This is the iter 9630, the d1 loss is: 2968.9062, the d2 loss is: -3046.9531, the g loss is: 3040.2188, the ae loss is: 0.0067005455, the jacobian loss is:0.124311864\n",
            "This is the iter 9631, the d1 loss is: 2982.3594, the d2 loss is: -3066.2656, the g loss is: 3063.9531, the ae loss is: 0.0065936893, the jacobian loss is:0.07752465\n",
            "This is the iter 9632, the d1 loss is: 3103.2344, the d2 loss is: -3175.8281, the g loss is: 3124.7422, the ae loss is: 0.0067476034, the jacobian loss is:0.0783525\n",
            "This is the iter 9633, the d1 loss is: 2867.5781, the d2 loss is: -2941.4844, the g loss is: 2926.8594, the ae loss is: 0.0080923755, the jacobian loss is:0.102584\n",
            "This is the iter 9634, the d1 loss is: 2956.3125, the d2 loss is: -3044.9766, the g loss is: 3023.875, the ae loss is: 0.007883054, the jacobian loss is:0.096021056\n",
            "This is the iter 9635, the d1 loss is: 3045.5938, the d2 loss is: -3131.4375, the g loss is: 3075.1875, the ae loss is: 0.0055892305, the jacobian loss is:0.0701408\n",
            "This is the iter 9636, the d1 loss is: 3056.1797, the d2 loss is: -3143.7734, the g loss is: 3127.5781, the ae loss is: 0.005873497, the jacobian loss is:0.08400565\n",
            "This is the iter 9637, the d1 loss is: 3079.3281, the d2 loss is: -3174.5938, the g loss is: 3132.6875, the ae loss is: 0.0064649517, the jacobian loss is:0.07597488\n",
            "This is the iter 9638, the d1 loss is: 3077.9297, the d2 loss is: -3162.9297, the g loss is: 3199.586, the ae loss is: 0.007159011, the jacobian loss is:0.07528937\n",
            "This is the iter 9639, the d1 loss is: 2867.7656, the d2 loss is: -2966.6562, the g loss is: 2974.164, the ae loss is: 0.004964005, the jacobian loss is:0.12196381\n",
            "This is the iter 9640, the d1 loss is: 2797.6797, the d2 loss is: -2892.2969, the g loss is: 2836.7969, the ae loss is: 0.006015975, the jacobian loss is:0.11653115\n",
            "This is the iter 9641, the d1 loss is: 2765.7734, the d2 loss is: -2860.375, the g loss is: 2870.8594, the ae loss is: 0.0069334395, the jacobian loss is:0.08410886\n",
            "This is the iter 9642, the d1 loss is: 2976.0156, the d2 loss is: -3066.2656, the g loss is: 3068.414, the ae loss is: 0.0065645995, the jacobian loss is:0.057302073\n",
            "This is the iter 9643, the d1 loss is: 2997.75, the d2 loss is: -3078.375, the g loss is: 3053.5156, the ae loss is: 0.0053996244, the jacobian loss is:0.1013601\n",
            "This is the iter 9644, the d1 loss is: 2785.8281, the d2 loss is: -2879.1016, the g loss is: 2973.289, the ae loss is: 0.008628549, the jacobian loss is:0.08604368\n",
            "This is the iter 9645, the d1 loss is: 2921.6719, the d2 loss is: -2997.0625, the g loss is: 3073.2578, the ae loss is: 0.0043933536, the jacobian loss is:0.061215013\n",
            "This is the iter 9646, the d1 loss is: 2958.3047, the d2 loss is: -3039.9531, the g loss is: 3067.0312, the ae loss is: 0.0067208726, the jacobian loss is:0.08347285\n",
            "This is the iter 9647, the d1 loss is: 2955.375, the d2 loss is: -3045.3672, the g loss is: 3099.3125, the ae loss is: 0.0067870966, the jacobian loss is:0.097366616\n",
            "This is the iter 9648, the d1 loss is: 3010.9297, the d2 loss is: -3101.2656, the g loss is: 3110.8672, the ae loss is: 0.0073446995, the jacobian loss is:0.09998855\n",
            "This is the iter 9649, the d1 loss is: 2848.0781, the d2 loss is: -2927.9844, the g loss is: 3064.9297, the ae loss is: 0.0058621448, the jacobian loss is:0.0739101\n",
            "This is the iter 9650, the d1 loss is: 2888.0469, the d2 loss is: -2987.0781, the g loss is: 3000.4453, the ae loss is: 0.0080315145, the jacobian loss is:0.057009738\n",
            "This is the iter 9651, the d1 loss is: 2900.7578, the d2 loss is: -2978.9844, the g loss is: 2958.25, the ae loss is: 0.0062801787, the jacobian loss is:0.08417453\n",
            "This is the iter 9652, the d1 loss is: 2890.2578, the d2 loss is: -3000.75, the g loss is: 3013.164, the ae loss is: 0.0065893456, the jacobian loss is:0.08274783\n",
            "This is the iter 9653, the d1 loss is: 2963.1406, the d2 loss is: -3032.6172, the g loss is: 2997.2344, the ae loss is: 0.0063650766, the jacobian loss is:0.08410158\n",
            "This is the iter 9654, the d1 loss is: 2908.3672, the d2 loss is: -3005.5781, the g loss is: 2952.9844, the ae loss is: 0.0065915324, the jacobian loss is:0.0716338\n",
            "This is the iter 9655, the d1 loss is: 2915.664, the d2 loss is: -2994.164, the g loss is: 3011.1016, the ae loss is: 0.0045913015, the jacobian loss is:0.09583934\n",
            "This is the iter 9656, the d1 loss is: 3267.9219, the d2 loss is: -3340.4219, the g loss is: 3384.2656, the ae loss is: 0.0075717526, the jacobian loss is:0.09135049\n",
            "This is the iter 9657, the d1 loss is: 3056.8438, the d2 loss is: -3159.5156, the g loss is: 3143.375, the ae loss is: 0.005279823, the jacobian loss is:0.09174166\n",
            "This is the iter 9658, the d1 loss is: 3072.2266, the d2 loss is: -3174.039, the g loss is: 3228.7031, the ae loss is: 0.0062393094, the jacobian loss is:0.094874516\n",
            "This is the iter 9659, the d1 loss is: 2969.0469, the d2 loss is: -3056.6953, the g loss is: 3116.0469, the ae loss is: 0.010209458, the jacobian loss is:0.10635157\n",
            "This is the iter 9660, the d1 loss is: 3001.3438, the d2 loss is: -3100.625, the g loss is: 3073.5, the ae loss is: 0.0052926093, the jacobian loss is:0.083367124\n",
            "This is the iter 9661, the d1 loss is: 3164.5703, the d2 loss is: -3231.7031, the g loss is: 3331.7188, the ae loss is: 0.006598246, the jacobian loss is:0.06919419\n",
            "This is the iter 9662, the d1 loss is: 2895.9922, the d2 loss is: -2992.5156, the g loss is: 3008.1406, the ae loss is: 0.007331695, the jacobian loss is:0.06629476\n",
            "This is the iter 9663, the d1 loss is: 2931.164, the d2 loss is: -3017.3438, the g loss is: 3069.7031, the ae loss is: 0.0058136405, the jacobian loss is:0.07091894\n",
            "This is the iter 9664, the d1 loss is: 3058.414, the d2 loss is: -3139.7266, the g loss is: 3188.125, the ae loss is: 0.006473126, the jacobian loss is:0.07602061\n",
            "This is the iter 9665, the d1 loss is: 3238.0078, the d2 loss is: -3322.9297, the g loss is: 3303.6875, the ae loss is: 0.0053309146, the jacobian loss is:0.08538378\n",
            "This is the iter 9666, the d1 loss is: 2804.7969, the d2 loss is: -2910.5156, the g loss is: 2921.8906, the ae loss is: 0.0059951907, the jacobian loss is:0.06542847\n",
            "This is the iter 9667, the d1 loss is: 2924.0547, the d2 loss is: -3017.2266, the g loss is: 3028.4062, the ae loss is: 0.0044890447, the jacobian loss is:0.09067172\n",
            "This is the iter 9668, the d1 loss is: 3010.664, the d2 loss is: -3102.6094, the g loss is: 3088.4219, the ae loss is: 0.0071299723, the jacobian loss is:0.064209454\n",
            "This is the iter 9669, the d1 loss is: 3245.7969, the d2 loss is: -3346.6016, the g loss is: 3319.25, the ae loss is: 0.005350648, the jacobian loss is:0.107172705\n",
            "This is the iter 9670, the d1 loss is: 3019.9531, the d2 loss is: -3130.0078, the g loss is: 3119.0078, the ae loss is: 0.007210725, the jacobian loss is:0.06753389\n",
            "This is the iter 9671, the d1 loss is: 2674.5469, the d2 loss is: -2768.8906, the g loss is: 2758.5078, the ae loss is: 0.006623259, the jacobian loss is:0.08265746\n",
            "This is the iter 9672, the d1 loss is: 3101.3281, the d2 loss is: -3192.9844, the g loss is: 3124.9688, the ae loss is: 0.008953128, the jacobian loss is:0.062490378\n",
            "This is the iter 9673, the d1 loss is: 2699.3672, the d2 loss is: -2773.8281, the g loss is: 2805.6719, the ae loss is: 0.0062830662, the jacobian loss is:0.13134252\n",
            "This is the iter 9674, the d1 loss is: 2885.5703, the d2 loss is: -2981.4688, the g loss is: 2955.1094, the ae loss is: 0.0079297125, the jacobian loss is:0.10237314\n",
            "This is the iter 9675, the d1 loss is: 2937.3281, the d2 loss is: -3034.6797, the g loss is: 3065.0938, the ae loss is: 0.009388606, the jacobian loss is:0.08806697\n",
            "This is the iter 9676, the d1 loss is: 2990.375, the d2 loss is: -3081.5312, the g loss is: 2996.125, the ae loss is: 0.0068260785, the jacobian loss is:0.11097843\n",
            "This is the iter 9677, the d1 loss is: 2971.7031, the d2 loss is: -3054.0, the g loss is: 3008.2031, the ae loss is: 0.005927341, the jacobian loss is:0.10235453\n",
            "This is the iter 9678, the d1 loss is: 2972.2578, the d2 loss is: -3074.1094, the g loss is: 3081.2188, the ae loss is: 0.0057931934, the jacobian loss is:0.078856565\n",
            "This is the iter 9679, the d1 loss is: 3061.625, the d2 loss is: -3159.4062, the g loss is: 3208.875, the ae loss is: 0.006011742, the jacobian loss is:0.08577869\n",
            "This is the iter 9680, the d1 loss is: 3162.7344, the d2 loss is: -3243.461, the g loss is: 3273.3828, the ae loss is: 0.006165333, the jacobian loss is:0.09504048\n",
            "This is the iter 9681, the d1 loss is: 3145.1562, the d2 loss is: -3240.2344, the g loss is: 3251.7656, the ae loss is: 0.0060480237, the jacobian loss is:0.071463846\n",
            "This is the iter 9682, the d1 loss is: 3010.1328, the d2 loss is: -3108.375, the g loss is: 3101.1406, the ae loss is: 0.0045804116, the jacobian loss is:0.14520542\n",
            "This is the iter 9683, the d1 loss is: 2849.7344, the d2 loss is: -2937.5234, the g loss is: 2916.9688, the ae loss is: 0.006856503, the jacobian loss is:0.08194655\n",
            "This is the iter 9684, the d1 loss is: 3040.1406, the d2 loss is: -3137.914, the g loss is: 3117.9219, the ae loss is: 0.00790636, the jacobian loss is:0.10627559\n",
            "This is the iter 9685, the d1 loss is: 3028.4062, the d2 loss is: -3127.164, the g loss is: 3069.5938, the ae loss is: 0.004422378, the jacobian loss is:0.09615633\n",
            "This is the iter 9686, the d1 loss is: 3025.3125, the d2 loss is: -3107.2734, the g loss is: 3091.0469, the ae loss is: 0.0065186303, the jacobian loss is:0.08734816\n",
            "This is the iter 9687, the d1 loss is: 3025.5, the d2 loss is: -3094.2812, the g loss is: 3073.5625, the ae loss is: 0.008307816, the jacobian loss is:0.08546927\n",
            "This is the iter 9688, the d1 loss is: 3057.8594, the d2 loss is: -3153.6484, the g loss is: 3138.7578, the ae loss is: 0.008658437, the jacobian loss is:0.09742694\n",
            "This is the iter 9689, the d1 loss is: 2951.9844, the d2 loss is: -3051.1719, the g loss is: 3046.7812, the ae loss is: 0.007472436, the jacobian loss is:0.087816626\n",
            "This is the iter 9690, the d1 loss is: 2854.211, the d2 loss is: -2952.5625, the g loss is: 2927.461, the ae loss is: 0.007842103, the jacobian loss is:0.07524888\n",
            "This is the iter 9691, the d1 loss is: 3025.6094, the d2 loss is: -3122.4922, the g loss is: 3080.9844, the ae loss is: 0.00539663, the jacobian loss is:0.06626692\n",
            "This is the iter 9692, the d1 loss is: 2700.2266, the d2 loss is: -2791.7344, the g loss is: 2805.8203, the ae loss is: 0.007600026, the jacobian loss is:0.06798995\n",
            "This is the iter 9693, the d1 loss is: 3151.164, the d2 loss is: -3244.0703, the g loss is: 3201.3281, the ae loss is: 0.005161091, the jacobian loss is:0.06619528\n",
            "This is the iter 9694, the d1 loss is: 2950.2031, the d2 loss is: -3023.3906, the g loss is: 3084.1172, the ae loss is: 0.0060314676, the jacobian loss is:0.08438789\n",
            "This is the iter 9695, the d1 loss is: 2945.8594, the d2 loss is: -3033.5703, the g loss is: 3087.414, the ae loss is: 0.0059336773, the jacobian loss is:0.06719491\n",
            "This is the iter 9696, the d1 loss is: 2756.711, the d2 loss is: -2845.6094, the g loss is: 2836.6562, the ae loss is: 0.007732993, the jacobian loss is:0.07099357\n",
            "This is the iter 9697, the d1 loss is: 2924.336, the d2 loss is: -3009.8594, the g loss is: 2973.6094, the ae loss is: 0.0058983546, the jacobian loss is:0.089722365\n",
            "This is the iter 9698, the d1 loss is: 2751.1719, the d2 loss is: -2855.2969, the g loss is: 2853.8672, the ae loss is: 0.007207297, the jacobian loss is:0.09774292\n",
            "This is the iter 9699, the d1 loss is: 2999.9375, the d2 loss is: -3070.914, the g loss is: 3095.75, the ae loss is: 0.0074878884, the jacobian loss is:0.07259978\n",
            "This is the iter 9700, the d1 loss is: 2965.3828, the d2 loss is: -3044.6016, the g loss is: 3014.3594, the ae loss is: 0.007942628, the jacobian loss is:0.0793168\n",
            "0.25385094\n",
            "0.9249702\n",
            "This is the iter 9701, the d1 loss is: 3046.75, the d2 loss is: -3139.4844, the g loss is: 3129.2344, the ae loss is: 0.008048596, the jacobian loss is:0.070103936\n",
            "This is the iter 9702, the d1 loss is: 2702.1562, the d2 loss is: -2805.211, the g loss is: 2792.3438, the ae loss is: 0.0071198465, the jacobian loss is:0.07199109\n",
            "This is the iter 9703, the d1 loss is: 2966.711, the d2 loss is: -3054.0, the g loss is: 3011.25, the ae loss is: 0.0073268963, the jacobian loss is:0.06776756\n",
            "This is the iter 9704, the d1 loss is: 2892.1094, the d2 loss is: -2984.375, the g loss is: 2989.3906, the ae loss is: 0.0068749534, the jacobian loss is:0.12838218\n",
            "This is the iter 9705, the d1 loss is: 2980.4844, the d2 loss is: -3075.4297, the g loss is: 3133.2812, the ae loss is: 0.0058709793, the jacobian loss is:0.06629581\n",
            "This is the iter 9706, the d1 loss is: 2931.0234, the d2 loss is: -3042.5, the g loss is: 3028.2031, the ae loss is: 0.006433596, the jacobian loss is:0.08223607\n",
            "This is the iter 9707, the d1 loss is: 2935.3828, the d2 loss is: -3013.7031, the g loss is: 3100.6719, the ae loss is: 0.009463115, the jacobian loss is:0.07758525\n",
            "This is the iter 9708, the d1 loss is: 2685.5781, the d2 loss is: -2764.6094, the g loss is: 2771.0781, the ae loss is: 0.008035818, the jacobian loss is:0.08852801\n",
            "This is the iter 9709, the d1 loss is: 2921.5938, the d2 loss is: -3006.75, the g loss is: 2972.0625, the ae loss is: 0.0063201576, the jacobian loss is:0.10903475\n",
            "This is the iter 9710, the d1 loss is: 2944.5781, the d2 loss is: -3026.461, the g loss is: 3031.6562, the ae loss is: 0.0058105025, the jacobian loss is:0.061236914\n",
            "This is the iter 9711, the d1 loss is: 2922.6797, the d2 loss is: -3006.0, the g loss is: 3001.3906, the ae loss is: 0.006927687, the jacobian loss is:0.06781378\n",
            "This is the iter 9712, the d1 loss is: 2926.7188, the d2 loss is: -3019.4922, the g loss is: 3067.2422, the ae loss is: 0.0059201145, the jacobian loss is:0.08467353\n",
            "This is the iter 9713, the d1 loss is: 3137.3125, the d2 loss is: -3229.7969, the g loss is: 3224.1406, the ae loss is: 0.0077875606, the jacobian loss is:0.06749389\n",
            "This is the iter 9714, the d1 loss is: 3197.5781, the d2 loss is: -3276.2422, the g loss is: 3272.0156, the ae loss is: 0.0071467184, the jacobian loss is:0.09135819\n",
            "This is the iter 9715, the d1 loss is: 2962.7422, the d2 loss is: -3049.914, the g loss is: 3076.9062, the ae loss is: 0.0061356532, the jacobian loss is:0.09448014\n",
            "This is the iter 9716, the d1 loss is: 3110.2422, the d2 loss is: -3208.1797, the g loss is: 3227.9844, the ae loss is: 0.0053805197, the jacobian loss is:0.123411156\n",
            "This is the iter 9717, the d1 loss is: 3020.9922, the d2 loss is: -3130.5312, the g loss is: 3102.9688, the ae loss is: 0.007444727, the jacobian loss is:0.08679882\n",
            "This is the iter 9718, the d1 loss is: 3255.6719, the d2 loss is: -3340.1562, the g loss is: 3323.75, the ae loss is: 0.004599505, the jacobian loss is:0.053346463\n",
            "This is the iter 9719, the d1 loss is: 2833.039, the d2 loss is: -2940.7031, the g loss is: 2949.5781, the ae loss is: 0.005270695, the jacobian loss is:0.0683624\n",
            "This is the iter 9720, the d1 loss is: 3237.1094, the d2 loss is: -3347.1094, the g loss is: 3307.0703, the ae loss is: 0.0053611994, the jacobian loss is:0.07715827\n",
            "This is the iter 9721, the d1 loss is: 2973.7734, the d2 loss is: -3065.9062, the g loss is: 3059.5156, the ae loss is: 0.008903472, the jacobian loss is:0.14552796\n",
            "This is the iter 9722, the d1 loss is: 3167.5156, the d2 loss is: -3270.4219, the g loss is: 3294.7578, the ae loss is: 0.0050565507, the jacobian loss is:0.05906952\n",
            "This is the iter 9723, the d1 loss is: 2971.1797, the d2 loss is: -3075.4062, the g loss is: 3080.8047, the ae loss is: 0.007043814, the jacobian loss is:0.068410605\n",
            "This is the iter 9724, the d1 loss is: 2909.961, the d2 loss is: -3031.25, the g loss is: 3034.8516, the ae loss is: 0.007150138, the jacobian loss is:0.061987985\n",
            "This is the iter 9725, the d1 loss is: 2922.1562, the d2 loss is: -3006.4844, the g loss is: 2966.9688, the ae loss is: 0.0074106697, the jacobian loss is:0.09113849\n",
            "This is the iter 9726, the d1 loss is: 2739.625, the d2 loss is: -2838.039, the g loss is: 2864.586, the ae loss is: 0.0073312176, the jacobian loss is:0.093168184\n",
            "This is the iter 9727, the d1 loss is: 2964.7031, the d2 loss is: -3058.4375, the g loss is: 3087.3281, the ae loss is: 0.0057244976, the jacobian loss is:0.081052534\n",
            "This is the iter 9728, the d1 loss is: 2905.4219, the d2 loss is: -2995.5156, the g loss is: 3047.5781, the ae loss is: 0.008504402, the jacobian loss is:0.06407476\n",
            "This is the iter 9729, the d1 loss is: 2781.2969, the d2 loss is: -2876.289, the g loss is: 2831.7188, the ae loss is: 0.0049170475, the jacobian loss is:0.11093413\n",
            "This is the iter 9730, the d1 loss is: 3362.8984, the d2 loss is: -3446.7344, the g loss is: 3438.1797, the ae loss is: 0.006781997, the jacobian loss is:0.07320289\n",
            "This is the iter 9731, the d1 loss is: 3044.625, the d2 loss is: -3115.7969, the g loss is: 3169.5156, the ae loss is: 0.008358496, the jacobian loss is:0.07670571\n",
            "This is the iter 9732, the d1 loss is: 3043.3281, the d2 loss is: -3115.6719, the g loss is: 3134.164, the ae loss is: 0.006005079, the jacobian loss is:0.0847601\n",
            "This is the iter 9733, the d1 loss is: 2975.9062, the d2 loss is: -3062.25, the g loss is: 3059.6016, the ae loss is: 0.0073750783, the jacobian loss is:0.07468844\n",
            "This is the iter 9734, the d1 loss is: 2889.7734, the d2 loss is: -2974.0, the g loss is: 3008.9375, the ae loss is: 0.0077786054, the jacobian loss is:0.076076776\n",
            "This is the iter 9735, the d1 loss is: 2996.2031, the d2 loss is: -3075.4219, the g loss is: 3062.9531, the ae loss is: 0.00760427, the jacobian loss is:0.0933117\n",
            "This is the iter 9736, the d1 loss is: 3045.5469, the d2 loss is: -3113.5625, the g loss is: 3105.375, the ae loss is: 0.0072546233, the jacobian loss is:0.098487094\n",
            "This is the iter 9737, the d1 loss is: 3236.25, the d2 loss is: -3324.836, the g loss is: 3340.5156, the ae loss is: 0.0066072983, the jacobian loss is:0.07705105\n",
            "This is the iter 9738, the d1 loss is: 3021.1875, the d2 loss is: -3110.664, the g loss is: 3083.914, the ae loss is: 0.007956026, the jacobian loss is:0.058280982\n",
            "This is the iter 9739, the d1 loss is: 3205.2188, the d2 loss is: -3290.6562, the g loss is: 3307.4844, the ae loss is: 0.0070389165, the jacobian loss is:0.0920618\n",
            "This is the iter 9740, the d1 loss is: 3219.164, the d2 loss is: -3289.0312, the g loss is: 3298.7422, the ae loss is: 0.006328367, the jacobian loss is:0.07223333\n",
            "This is the iter 9741, the d1 loss is: 2689.1562, the d2 loss is: -2788.3984, the g loss is: 2790.2422, the ae loss is: 0.00629678, the jacobian loss is:0.08380648\n",
            "This is the iter 9742, the d1 loss is: 2998.539, the d2 loss is: -3075.961, the g loss is: 3091.9297, the ae loss is: 0.008173179, the jacobian loss is:0.06727522\n",
            "This is the iter 9743, the d1 loss is: 3118.9062, the d2 loss is: -3216.8047, the g loss is: 3179.5547, the ae loss is: 0.0059811585, the jacobian loss is:0.123984545\n",
            "This is the iter 9744, the d1 loss is: 3064.6875, the d2 loss is: -3172.9062, the g loss is: 3229.2344, the ae loss is: 0.0073631937, the jacobian loss is:0.07106768\n",
            "This is the iter 9745, the d1 loss is: 2960.789, the d2 loss is: -3024.6016, the g loss is: 3033.539, the ae loss is: 0.004464201, the jacobian loss is:0.058239557\n",
            "This is the iter 9746, the d1 loss is: 2832.3281, the d2 loss is: -2941.7969, the g loss is: 2968.836, the ae loss is: 0.0069792196, the jacobian loss is:0.08235956\n",
            "This is the iter 9747, the d1 loss is: 2919.1328, the d2 loss is: -3026.8906, the g loss is: 3146.25, the ae loss is: 0.0051499577, the jacobian loss is:0.06405702\n",
            "This is the iter 9748, the d1 loss is: 2993.2656, the d2 loss is: -3082.2969, the g loss is: 3073.0781, the ae loss is: 0.008769142, the jacobian loss is:0.07540198\n",
            "This is the iter 9749, the d1 loss is: 3276.6484, the d2 loss is: -3369.8516, the g loss is: 3365.6406, the ae loss is: 0.009580389, the jacobian loss is:0.10341219\n",
            "This is the iter 9750, the d1 loss is: 3210.625, the d2 loss is: -3306.086, the g loss is: 3281.2969, the ae loss is: 0.006488337, the jacobian loss is:0.08881127\n",
            "This is the iter 9751, the d1 loss is: 2905.3047, the d2 loss is: -2968.1562, the g loss is: 2957.875, the ae loss is: 0.0066694682, the jacobian loss is:0.10897082\n",
            "This is the iter 9752, the d1 loss is: 3128.711, the d2 loss is: -3211.7656, the g loss is: 3156.8047, the ae loss is: 0.0061986186, the jacobian loss is:0.07832762\n",
            "This is the iter 9753, the d1 loss is: 3128.3828, the d2 loss is: -3223.1406, the g loss is: 3204.2734, the ae loss is: 0.0050952123, the jacobian loss is:0.067360625\n",
            "This is the iter 9754, the d1 loss is: 3055.8047, the d2 loss is: -3151.5547, the g loss is: 3191.1094, the ae loss is: 0.007385292, the jacobian loss is:0.071939066\n",
            "This is the iter 9755, the d1 loss is: 2890.5, the d2 loss is: -2980.3047, the g loss is: 2953.8828, the ae loss is: 0.005772293, the jacobian loss is:0.055699144\n",
            "This is the iter 9756, the d1 loss is: 2984.914, the d2 loss is: -3085.6406, the g loss is: 3030.5547, the ae loss is: 0.0052136453, the jacobian loss is:0.072638325\n",
            "This is the iter 9757, the d1 loss is: 2933.1562, the d2 loss is: -3009.7344, the g loss is: 3043.8594, the ae loss is: 0.0058973324, the jacobian loss is:0.14502713\n",
            "This is the iter 9758, the d1 loss is: 2845.1016, the d2 loss is: -2941.6562, the g loss is: 2957.4219, the ae loss is: 0.0066499244, the jacobian loss is:0.060174692\n",
            "This is the iter 9759, the d1 loss is: 2881.3125, the d2 loss is: -2987.625, the g loss is: 2958.7969, the ae loss is: 0.004708477, the jacobian loss is:0.07538489\n",
            "This is the iter 9760, the d1 loss is: 2813.6406, the d2 loss is: -2901.6172, the g loss is: 2922.5312, the ae loss is: 0.0048526265, the jacobian loss is:0.06833511\n",
            "This is the iter 9761, the d1 loss is: 3033.875, the d2 loss is: -3109.0156, the g loss is: 3119.2656, the ae loss is: 0.0049324636, the jacobian loss is:0.08689047\n",
            "This is the iter 9762, the d1 loss is: 2916.7344, the d2 loss is: -3009.0625, the g loss is: 2955.2266, the ae loss is: 0.0073650703, the jacobian loss is:0.06407791\n",
            "This is the iter 9763, the d1 loss is: 3086.7969, the d2 loss is: -3180.9375, the g loss is: 3157.2578, the ae loss is: 0.006444978, the jacobian loss is:0.07941464\n",
            "This is the iter 9764, the d1 loss is: 3052.289, the d2 loss is: -3137.2734, the g loss is: 3133.9688, the ae loss is: 0.0075812098, the jacobian loss is:0.091980144\n",
            "This is the iter 9765, the d1 loss is: 3088.2812, the d2 loss is: -3200.5156, the g loss is: 3214.9375, the ae loss is: 0.007906089, the jacobian loss is:0.07462542\n",
            "This is the iter 9766, the d1 loss is: 2963.3984, the d2 loss is: -3056.0781, the g loss is: 3028.7422, the ae loss is: 0.0054959217, the jacobian loss is:0.07437199\n",
            "This is the iter 9767, the d1 loss is: 2910.711, the d2 loss is: -2999.4688, the g loss is: 3024.1953, the ae loss is: 0.007880145, the jacobian loss is:0.0914539\n",
            "This is the iter 9768, the d1 loss is: 3190.2656, the d2 loss is: -3281.4297, the g loss is: 3243.4531, the ae loss is: 0.010435897, the jacobian loss is:0.07780699\n",
            "This is the iter 9769, the d1 loss is: 2797.0625, the d2 loss is: -2897.4062, the g loss is: 2865.7266, the ae loss is: 0.0072910776, the jacobian loss is:0.051508464\n",
            "This is the iter 9770, the d1 loss is: 3119.8281, the d2 loss is: -3220.1953, the g loss is: 3204.8281, the ae loss is: 0.006696516, the jacobian loss is:0.06677376\n",
            "This is the iter 9771, the d1 loss is: 2758.1094, the d2 loss is: -2836.289, the g loss is: 2793.5625, the ae loss is: 0.009281738, the jacobian loss is:0.12126392\n",
            "This is the iter 9772, the d1 loss is: 3159.3594, the d2 loss is: -3244.2969, the g loss is: 3276.0156, the ae loss is: 0.00649808, the jacobian loss is:0.05389299\n",
            "This is the iter 9773, the d1 loss is: 3387.5312, the d2 loss is: -3471.336, the g loss is: 3415.1016, the ae loss is: 0.0056729754, the jacobian loss is:0.086332105\n",
            "This is the iter 9774, the d1 loss is: 3289.1406, the d2 loss is: -3380.3594, the g loss is: 3430.7031, the ae loss is: 0.0047476618, the jacobian loss is:0.079306394\n",
            "This is the iter 9775, the d1 loss is: 2957.9922, the d2 loss is: -3047.6875, the g loss is: 3077.25, the ae loss is: 0.0070599588, the jacobian loss is:0.06534983\n",
            "This is the iter 9776, the d1 loss is: 3119.9219, the d2 loss is: -3216.0469, the g loss is: 3156.2422, the ae loss is: 0.005546142, the jacobian loss is:0.08346655\n",
            "This is the iter 9777, the d1 loss is: 2699.5781, the d2 loss is: -2814.6328, the g loss is: 2837.1172, the ae loss is: 0.008838864, the jacobian loss is:0.09413868\n",
            "This is the iter 9778, the d1 loss is: 3159.9453, the d2 loss is: -3253.8984, the g loss is: 3229.9453, the ae loss is: 0.006483089, the jacobian loss is:0.07440174\n",
            "This is the iter 9779, the d1 loss is: 2855.6562, the d2 loss is: -2918.7578, the g loss is: 2885.4766, the ae loss is: 0.0076093017, the jacobian loss is:0.13265258\n",
            "This is the iter 9780, the d1 loss is: 3015.0938, the d2 loss is: -3110.4375, the g loss is: 3101.6719, the ae loss is: 0.009687623, the jacobian loss is:0.059196748\n",
            "This is the iter 9781, the d1 loss is: 3009.1719, the d2 loss is: -3106.0, the g loss is: 3101.9062, the ae loss is: 0.010614937, the jacobian loss is:0.083676286\n",
            "This is the iter 9782, the d1 loss is: 2990.3438, the d2 loss is: -3088.6172, the g loss is: 3072.1797, the ae loss is: 0.0067942897, the jacobian loss is:0.05957386\n",
            "This is the iter 9783, the d1 loss is: 3353.7266, the d2 loss is: -3423.5078, the g loss is: 3397.336, the ae loss is: 0.0065266383, the jacobian loss is:0.11850803\n",
            "This is the iter 9784, the d1 loss is: 2868.3281, the d2 loss is: -2974.6328, the g loss is: 2944.9453, the ae loss is: 0.011935903, the jacobian loss is:0.07026751\n",
            "This is the iter 9785, the d1 loss is: 3142.75, the d2 loss is: -3223.25, the g loss is: 3209.6562, the ae loss is: 0.005620789, the jacobian loss is:0.06935928\n",
            "This is the iter 9786, the d1 loss is: 3085.6094, the d2 loss is: -3184.6562, the g loss is: 3211.0703, the ae loss is: 0.0066147204, the jacobian loss is:0.065348685\n",
            "This is the iter 9787, the d1 loss is: 2900.211, the d2 loss is: -2967.9062, the g loss is: 3053.5, the ae loss is: 0.006150934, the jacobian loss is:0.072023034\n",
            "This is the iter 9788, the d1 loss is: 3180.1406, the d2 loss is: -3251.414, the g loss is: 3279.2188, the ae loss is: 0.007802241, the jacobian loss is:0.078831725\n",
            "This is the iter 9789, the d1 loss is: 2825.8125, the d2 loss is: -2920.2969, the g loss is: 2895.7812, the ae loss is: 0.0073274765, the jacobian loss is:0.14587687\n",
            "This is the iter 9790, the d1 loss is: 3001.1016, the d2 loss is: -3101.4219, the g loss is: 3165.2031, the ae loss is: 0.007181799, the jacobian loss is:0.07901736\n",
            "This is the iter 9791, the d1 loss is: 2798.3594, the d2 loss is: -2877.3906, the g loss is: 2964.2656, the ae loss is: 0.00576722, the jacobian loss is:0.07447004\n",
            "This is the iter 9792, the d1 loss is: 3054.8516, the d2 loss is: -3139.1562, the g loss is: 3110.125, the ae loss is: 0.005488269, the jacobian loss is:0.0615532\n",
            "This is the iter 9793, the d1 loss is: 2908.8438, the d2 loss is: -2988.0469, the g loss is: 2998.4844, the ae loss is: 0.0049677445, the jacobian loss is:0.06072034\n",
            "This is the iter 9794, the d1 loss is: 2975.586, the d2 loss is: -3077.2188, the g loss is: 3033.5547, the ae loss is: 0.008653232, the jacobian loss is:0.07082673\n",
            "This is the iter 9795, the d1 loss is: 2806.8672, the d2 loss is: -2891.9219, the g loss is: 2881.1719, the ae loss is: 0.0064144284, the jacobian loss is:0.06216915\n",
            "This is the iter 9796, the d1 loss is: 3097.8125, the d2 loss is: -3188.2031, the g loss is: 3139.336, the ae loss is: 0.008988421, the jacobian loss is:0.062201425\n",
            "This is the iter 9797, the d1 loss is: 2943.4531, the d2 loss is: -3042.586, the g loss is: 3080.5938, the ae loss is: 0.0066488027, the jacobian loss is:0.052804407\n",
            "This is the iter 9798, the d1 loss is: 3308.8516, the d2 loss is: -3416.6797, the g loss is: 3402.164, the ae loss is: 0.0077383555, the jacobian loss is:0.05956643\n",
            "This is the iter 9799, the d1 loss is: 2874.0, the d2 loss is: -2959.2031, the g loss is: 2959.6562, the ae loss is: 0.006663482, the jacobian loss is:0.09548692\n",
            "This is the iter 9800, the d1 loss is: 3159.1328, the d2 loss is: -3238.3438, the g loss is: 3227.125, the ae loss is: 0.0068889996, the jacobian loss is:0.100816175\n",
            "0.24923404\n",
            "0.90954167\n",
            "This is the iter 9801, the d1 loss is: 2983.2734, the d2 loss is: -3089.0703, the g loss is: 2997.8438, the ae loss is: 0.007820442, the jacobian loss is:0.06640369\n",
            "This is the iter 9802, the d1 loss is: 3118.375, the d2 loss is: -3228.8047, the g loss is: 3218.6484, the ae loss is: 0.006729801, the jacobian loss is:0.07616884\n",
            "This is the iter 9803, the d1 loss is: 3108.4922, the d2 loss is: -3189.0156, the g loss is: 3164.0, the ae loss is: 0.0071056476, the jacobian loss is:0.11513697\n",
            "This is the iter 9804, the d1 loss is: 2894.2812, the d2 loss is: -2978.7969, the g loss is: 2952.375, the ae loss is: 0.008025672, the jacobian loss is:0.057687704\n",
            "This is the iter 9805, the d1 loss is: 2980.8516, the d2 loss is: -3067.3516, the g loss is: 3131.289, the ae loss is: 0.0061207237, the jacobian loss is:0.055578966\n",
            "This is the iter 9806, the d1 loss is: 3136.039, the d2 loss is: -3222.8672, the g loss is: 3238.1016, the ae loss is: 0.0054114396, the jacobian loss is:0.050644908\n",
            "This is the iter 9807, the d1 loss is: 2795.7969, the d2 loss is: -2870.3047, the g loss is: 2851.2656, the ae loss is: 0.00552178, the jacobian loss is:0.061153896\n",
            "This is the iter 9808, the d1 loss is: 3093.2812, the d2 loss is: -3176.2031, the g loss is: 3169.8203, the ae loss is: 0.007981215, the jacobian loss is:0.06112042\n",
            "This is the iter 9809, the d1 loss is: 2722.8516, the d2 loss is: -2804.6172, the g loss is: 2767.2344, the ae loss is: 0.010448473, the jacobian loss is:0.11740082\n",
            "This is the iter 9810, the d1 loss is: 3203.5625, the d2 loss is: -3276.2812, the g loss is: 3319.7578, the ae loss is: 0.005657284, the jacobian loss is:0.08911713\n",
            "This is the iter 9811, the d1 loss is: 2823.5781, the d2 loss is: -2916.0625, the g loss is: 2913.7578, the ae loss is: 0.005525427, the jacobian loss is:0.07437468\n",
            "This is the iter 9812, the d1 loss is: 3087.2422, the d2 loss is: -3174.0234, the g loss is: 3192.7656, the ae loss is: 0.008870493, the jacobian loss is:0.089736335\n",
            "This is the iter 9813, the d1 loss is: 2938.3203, the d2 loss is: -3036.8047, the g loss is: 3019.3672, the ae loss is: 0.0060941363, the jacobian loss is:0.077533804\n",
            "This is the iter 9814, the d1 loss is: 3145.8125, the d2 loss is: -3240.5, the g loss is: 3256.539, the ae loss is: 0.004480681, the jacobian loss is:0.049138833\n",
            "This is the iter 9815, the d1 loss is: 2855.9375, the d2 loss is: -2933.3125, the g loss is: 2857.7969, the ae loss is: 0.008769619, the jacobian loss is:0.093069434\n",
            "This is the iter 9816, the d1 loss is: 2871.9922, the d2 loss is: -2955.1094, the g loss is: 2945.3516, the ae loss is: 0.0064729387, the jacobian loss is:0.05022248\n",
            "This is the iter 9817, the d1 loss is: 2936.6406, the d2 loss is: -3039.3672, the g loss is: 3001.4219, the ae loss is: 0.0069080302, the jacobian loss is:0.07050945\n",
            "This is the iter 9818, the d1 loss is: 3073.0234, the d2 loss is: -3134.3828, the g loss is: 3191.875, the ae loss is: 0.007600519, the jacobian loss is:0.07318183\n",
            "This is the iter 9819, the d1 loss is: 3031.6875, the d2 loss is: -3132.7812, the g loss is: 3108.3906, the ae loss is: 0.008558633, the jacobian loss is:0.086955234\n",
            "This is the iter 9820, the d1 loss is: 2949.8984, the d2 loss is: -3038.8516, the g loss is: 3065.4062, the ae loss is: 0.011703618, the jacobian loss is:0.08450056\n",
            "This is the iter 9821, the d1 loss is: 2805.1562, the d2 loss is: -2900.8672, the g loss is: 2885.6953, the ae loss is: 0.0049349903, the jacobian loss is:0.088300414\n",
            "This is the iter 9822, the d1 loss is: 3082.2656, the d2 loss is: -3190.789, the g loss is: 3216.3203, the ae loss is: 0.008351419, the jacobian loss is:0.051924244\n",
            "This is the iter 9823, the d1 loss is: 2946.336, the d2 loss is: -3038.2188, the g loss is: 3018.3828, the ae loss is: 0.0071323407, the jacobian loss is:0.07497043\n",
            "This is the iter 9824, the d1 loss is: 3000.1016, the d2 loss is: -3095.6094, the g loss is: 3037.5703, the ae loss is: 0.007469427, the jacobian loss is:0.07858807\n",
            "This is the iter 9825, the d1 loss is: 2932.1875, the d2 loss is: -2997.875, the g loss is: 3092.4688, the ae loss is: 0.005303869, the jacobian loss is:0.080364235\n",
            "This is the iter 9826, the d1 loss is: 3031.4531, the d2 loss is: -3111.5547, the g loss is: 3137.3203, the ae loss is: 0.0084103905, the jacobian loss is:0.07762158\n",
            "This is the iter 9827, the d1 loss is: 2883.289, the d2 loss is: -2986.289, the g loss is: 3017.414, the ae loss is: 0.0060397973, the jacobian loss is:0.05875915\n",
            "This is the iter 9828, the d1 loss is: 3168.5938, the d2 loss is: -3241.8047, the g loss is: 3263.0703, the ae loss is: 0.007605029, the jacobian loss is:0.07116176\n",
            "This is the iter 9829, the d1 loss is: 3075.8281, the d2 loss is: -3159.7266, the g loss is: 3105.7734, the ae loss is: 0.004106006, the jacobian loss is:0.12526727\n",
            "This is the iter 9830, the d1 loss is: 3252.9375, the d2 loss is: -3325.3594, the g loss is: 3284.0, the ae loss is: 0.0063478616, the jacobian loss is:0.08845815\n",
            "This is the iter 9831, the d1 loss is: 2912.0234, the d2 loss is: -2987.3438, the g loss is: 3022.0, the ae loss is: 0.006317049, the jacobian loss is:0.07987025\n",
            "This is the iter 9832, the d1 loss is: 2910.211, the d2 loss is: -2988.8594, the g loss is: 3000.2812, the ae loss is: 0.006216445, the jacobian loss is:0.058240924\n",
            "This is the iter 9833, the d1 loss is: 3207.5, the d2 loss is: -3302.3516, the g loss is: 3342.1094, the ae loss is: 0.0045664255, the jacobian loss is:0.0628989\n",
            "This is the iter 9834, the d1 loss is: 2809.5156, the d2 loss is: -2891.5781, the g loss is: 2892.664, the ae loss is: 0.006006728, the jacobian loss is:0.07753844\n",
            "This is the iter 9835, the d1 loss is: 3060.3828, the d2 loss is: -3130.9922, the g loss is: 3142.0, the ae loss is: 0.006005678, the jacobian loss is:0.063566655\n",
            "This is the iter 9836, the d1 loss is: 3300.2969, the d2 loss is: -3359.0, the g loss is: 3408.164, the ae loss is: 0.004161765, the jacobian loss is:0.068689086\n",
            "This is the iter 9837, the d1 loss is: 2972.4453, the d2 loss is: -3076.539, the g loss is: 3081.6797, the ae loss is: 0.0062794746, the jacobian loss is:0.062306326\n",
            "This is the iter 9838, the d1 loss is: 3043.9219, the d2 loss is: -3125.5703, the g loss is: 3092.3203, the ae loss is: 0.0057736523, the jacobian loss is:0.070539415\n",
            "This is the iter 9839, the d1 loss is: 3083.2188, the d2 loss is: -3159.7969, the g loss is: 3155.7422, the ae loss is: 0.0048408834, the jacobian loss is:0.09136336\n",
            "This is the iter 9840, the d1 loss is: 3074.0938, the d2 loss is: -3167.9531, the g loss is: 3080.8125, the ae loss is: 0.007222605, the jacobian loss is:0.08246624\n",
            "This is the iter 9841, the d1 loss is: 3033.9375, the d2 loss is: -3093.289, the g loss is: 3108.461, the ae loss is: 0.0068565807, the jacobian loss is:0.08752717\n",
            "This is the iter 9842, the d1 loss is: 2905.9766, the d2 loss is: -2989.1328, the g loss is: 3015.1875, the ae loss is: 0.0049838126, the jacobian loss is:0.08126126\n",
            "This is the iter 9843, the d1 loss is: 3033.2188, the d2 loss is: -3116.6562, the g loss is: 3180.7969, the ae loss is: 0.0063349903, the jacobian loss is:0.050642207\n",
            "This is the iter 9844, the d1 loss is: 3059.3594, the d2 loss is: -3132.6875, the g loss is: 3087.8047, the ae loss is: 0.007771846, the jacobian loss is:0.06299054\n",
            "This is the iter 9845, the d1 loss is: 3120.3594, the d2 loss is: -3197.789, the g loss is: 3198.9062, the ae loss is: 0.0054160203, the jacobian loss is:0.06493652\n",
            "This is the iter 9846, the d1 loss is: 2952.5078, the d2 loss is: -3042.6562, the g loss is: 3038.9219, the ae loss is: 0.0068666968, the jacobian loss is:0.04892623\n",
            "This is the iter 9847, the d1 loss is: 3042.461, the d2 loss is: -3127.0938, the g loss is: 3111.75, the ae loss is: 0.0063461647, the jacobian loss is:0.07231003\n",
            "This is the iter 9848, the d1 loss is: 3087.9844, the d2 loss is: -3153.5312, the g loss is: 3160.0547, the ae loss is: 0.006832175, the jacobian loss is:0.083376385\n",
            "This is the iter 9849, the d1 loss is: 3326.2656, the d2 loss is: -3403.6797, the g loss is: 3437.2344, the ae loss is: 0.0056845592, the jacobian loss is:0.08525503\n",
            "This is the iter 9850, the d1 loss is: 2929.5625, the d2 loss is: -3021.8438, the g loss is: 3009.4922, the ae loss is: 0.00626372, the jacobian loss is:0.070625015\n",
            "This is the iter 9851, the d1 loss is: 3106.6172, the d2 loss is: -3211.4453, the g loss is: 3192.4531, the ae loss is: 0.00794116, the jacobian loss is:0.10517571\n",
            "This is the iter 9852, the d1 loss is: 2872.1094, the d2 loss is: -2959.5, the g loss is: 2922.164, the ae loss is: 0.006357412, the jacobian loss is:0.05550866\n",
            "This is the iter 9853, the d1 loss is: 3117.5312, the d2 loss is: -3210.7656, the g loss is: 3172.8281, the ae loss is: 0.0077727195, the jacobian loss is:0.12916009\n",
            "This is the iter 9854, the d1 loss is: 3015.8281, the d2 loss is: -3095.086, the g loss is: 3195.9531, the ae loss is: 0.0073589245, the jacobian loss is:0.06528921\n",
            "This is the iter 9855, the d1 loss is: 3071.5, the d2 loss is: -3165.6172, the g loss is: 3151.9219, the ae loss is: 0.0060949004, the jacobian loss is:0.073084995\n",
            "This is the iter 9856, the d1 loss is: 2977.5078, the d2 loss is: -3064.1094, the g loss is: 3149.6562, the ae loss is: 0.008841696, the jacobian loss is:0.07121659\n",
            "This is the iter 9857, the d1 loss is: 3124.5156, the d2 loss is: -3187.6953, the g loss is: 3182.6875, the ae loss is: 0.0066407407, the jacobian loss is:0.06406993\n",
            "This is the iter 9858, the d1 loss is: 2912.1719, the d2 loss is: -2991.3594, the g loss is: 2937.375, the ae loss is: 0.0052569937, the jacobian loss is:0.05202398\n",
            "This is the iter 9859, the d1 loss is: 2983.2266, the d2 loss is: -3091.1953, the g loss is: 3088.5938, the ae loss is: 0.0071293022, the jacobian loss is:0.12204945\n",
            "This is the iter 9860, the d1 loss is: 3063.375, the d2 loss is: -3153.8203, the g loss is: 3117.6562, the ae loss is: 0.00368211, the jacobian loss is:0.061300058\n",
            "This is the iter 9861, the d1 loss is: 2939.3438, the d2 loss is: -3029.2266, the g loss is: 2984.9297, the ae loss is: 0.008750422, the jacobian loss is:0.07177849\n",
            "This is the iter 9862, the d1 loss is: 3301.8047, the d2 loss is: -3390.336, the g loss is: 3447.2188, the ae loss is: 0.0075043202, the jacobian loss is:0.07355111\n",
            "This is the iter 9863, the d1 loss is: 2956.9297, the d2 loss is: -3042.6016, the g loss is: 3045.289, the ae loss is: 0.007866744, the jacobian loss is:0.062069897\n",
            "This is the iter 9864, the d1 loss is: 2777.1719, the d2 loss is: -2848.5469, the g loss is: 2843.586, the ae loss is: 0.008481672, the jacobian loss is:0.19925272\n",
            "This is the iter 9865, the d1 loss is: 3099.5781, the d2 loss is: -3188.7734, the g loss is: 3122.0625, the ae loss is: 0.008366061, the jacobian loss is:0.06694182\n",
            "This is the iter 9866, the d1 loss is: 2844.7031, the d2 loss is: -2918.961, the g loss is: 2992.9219, the ae loss is: 0.0053118193, the jacobian loss is:0.081200436\n",
            "This is the iter 9867, the d1 loss is: 3155.6797, the d2 loss is: -3242.6094, the g loss is: 3218.2656, the ae loss is: 0.009144865, the jacobian loss is:0.06937489\n",
            "This is the iter 9868, the d1 loss is: 2708.4219, the d2 loss is: -2792.75, the g loss is: 2877.2812, the ae loss is: 0.0062337616, the jacobian loss is:0.074562654\n",
            "This is the iter 9869, the d1 loss is: 3122.7344, the d2 loss is: -3205.1484, the g loss is: 3253.3828, the ae loss is: 0.0056573926, the jacobian loss is:0.09295699\n",
            "This is the iter 9870, the d1 loss is: 3090.5938, the d2 loss is: -3182.3125, the g loss is: 3306.7422, the ae loss is: 0.007066941, the jacobian loss is:0.0807054\n",
            "This is the iter 9871, the d1 loss is: 3111.7812, the d2 loss is: -3208.2188, the g loss is: 3199.336, the ae loss is: 0.004475047, the jacobian loss is:0.08428532\n",
            "This is the iter 9872, the d1 loss is: 2870.75, the d2 loss is: -2963.5469, the g loss is: 2931.3984, the ae loss is: 0.0050477413, the jacobian loss is:0.06671327\n",
            "This is the iter 9873, the d1 loss is: 3105.6797, the d2 loss is: -3177.0312, the g loss is: 3217.1094, the ae loss is: 0.0065066568, the jacobian loss is:0.06832939\n",
            "This is the iter 9874, the d1 loss is: 2868.0703, the d2 loss is: -2967.9219, the g loss is: 2979.8594, the ae loss is: 0.0068751266, the jacobian loss is:0.08604057\n",
            "This is the iter 9875, the d1 loss is: 3269.3516, the d2 loss is: -3365.2188, the g loss is: 3370.0078, the ae loss is: 0.0072258012, the jacobian loss is:0.05342317\n",
            "This is the iter 9876, the d1 loss is: 3097.2969, the d2 loss is: -3184.4766, the g loss is: 3147.8672, the ae loss is: 0.0108727515, the jacobian loss is:0.07565779\n",
            "This is the iter 9877, the d1 loss is: 2895.039, the d2 loss is: -2999.1406, the g loss is: 3028.0781, the ae loss is: 0.006074537, the jacobian loss is:0.06440192\n",
            "This is the iter 9878, the d1 loss is: 3147.4844, the d2 loss is: -3227.875, the g loss is: 3184.8594, the ae loss is: 0.0061961226, the jacobian loss is:0.06383895\n",
            "This is the iter 9879, the d1 loss is: 3112.3984, the d2 loss is: -3200.9844, the g loss is: 3205.8125, the ae loss is: 0.007219978, the jacobian loss is:0.06457891\n",
            "This is the iter 9880, the d1 loss is: 3090.7422, the d2 loss is: -3185.2656, the g loss is: 3158.1484, the ae loss is: 0.008592662, the jacobian loss is:0.06631238\n",
            "This is the iter 9881, the d1 loss is: 3011.2734, the d2 loss is: -3097.75, the g loss is: 3043.9375, the ae loss is: 0.0084571075, the jacobian loss is:0.054475535\n",
            "This is the iter 9882, the d1 loss is: 3346.3203, the d2 loss is: -3440.8828, the g loss is: 3476.0703, the ae loss is: 0.004778191, the jacobian loss is:0.05182055\n",
            "This is the iter 9883, the d1 loss is: 2919.5156, the d2 loss is: -2999.7266, the g loss is: 3056.4297, the ae loss is: 0.006391065, the jacobian loss is:0.05529013\n",
            "This is the iter 9884, the d1 loss is: 3039.5625, the d2 loss is: -3144.2266, the g loss is: 3152.2344, the ae loss is: 0.006568837, the jacobian loss is:0.060565438\n",
            "This is the iter 9885, the d1 loss is: 2991.336, the d2 loss is: -3091.8828, the g loss is: 3071.0, the ae loss is: 0.008100405, the jacobian loss is:0.055868156\n",
            "This is the iter 9886, the d1 loss is: 3077.5156, the d2 loss is: -3167.375, the g loss is: 3239.6562, the ae loss is: 0.0069556106, the jacobian loss is:0.07600221\n",
            "This is the iter 9887, the d1 loss is: 2997.4375, the d2 loss is: -3080.8828, the g loss is: 3055.289, the ae loss is: 0.005702309, the jacobian loss is:0.053319566\n",
            "This is the iter 9888, the d1 loss is: 3106.289, the d2 loss is: -3200.625, the g loss is: 3209.0156, the ae loss is: 0.006507815, the jacobian loss is:0.057796553\n",
            "This is the iter 9889, the d1 loss is: 3077.8984, the d2 loss is: -3176.0938, the g loss is: 3106.125, the ae loss is: 0.006807089, the jacobian loss is:0.050875857\n",
            "This is the iter 9890, the d1 loss is: 3217.6797, the d2 loss is: -3309.2188, the g loss is: 3327.8438, the ae loss is: 0.0076345354, the jacobian loss is:0.10713194\n",
            "This is the iter 9891, the d1 loss is: 3166.0938, the d2 loss is: -3255.0078, the g loss is: 3158.6406, the ae loss is: 0.010712557, the jacobian loss is:0.091936775\n",
            "This is the iter 9892, the d1 loss is: 3101.4062, the d2 loss is: -3196.7188, the g loss is: 3145.0625, the ae loss is: 0.006670419, the jacobian loss is:0.15527947\n",
            "This is the iter 9893, the d1 loss is: 3371.1406, the d2 loss is: -3474.0938, the g loss is: 3536.2812, the ae loss is: 0.008828024, the jacobian loss is:0.06014592\n",
            "This is the iter 9894, the d1 loss is: 3131.8672, the d2 loss is: -3207.125, the g loss is: 3256.8203, the ae loss is: 0.0076635364, the jacobian loss is:0.07303794\n",
            "This is the iter 9895, the d1 loss is: 3118.8984, the d2 loss is: -3205.8594, the g loss is: 3186.4531, the ae loss is: 0.008616666, the jacobian loss is:0.05709345\n",
            "This is the iter 9896, the d1 loss is: 3147.3125, the d2 loss is: -3240.8672, the g loss is: 3206.6719, the ae loss is: 0.00696591, the jacobian loss is:0.05901069\n",
            "This is the iter 9897, the d1 loss is: 3449.1875, the d2 loss is: -3521.1875, the g loss is: 3464.289, the ae loss is: 0.0060580065, the jacobian loss is:0.090080485\n",
            "This is the iter 9898, the d1 loss is: 3224.3906, the d2 loss is: -3293.8594, the g loss is: 3267.25, the ae loss is: 0.008206095, the jacobian loss is:0.07244823\n",
            "This is the iter 9899, the d1 loss is: 2863.4375, the d2 loss is: -2956.4453, the g loss is: 2965.4766, the ae loss is: 0.008085272, the jacobian loss is:0.08968517\n",
            "This is the iter 9900, the d1 loss is: 3065.6484, the d2 loss is: -3146.2188, the g loss is: 3132.0938, the ae loss is: 0.0070295073, the jacobian loss is:0.07733983\n",
            "0.24916819\n",
            "0.9091447\n",
            "This is the iter 9901, the d1 loss is: 3136.9375, the d2 loss is: -3205.211, the g loss is: 3204.6562, the ae loss is: 0.0079783015, the jacobian loss is:0.085047565\n",
            "This is the iter 9902, the d1 loss is: 3244.4219, the d2 loss is: -3340.1562, the g loss is: 3322.1562, the ae loss is: 0.008116006, the jacobian loss is:0.08479138\n",
            "This is the iter 9903, the d1 loss is: 2851.6719, the d2 loss is: -2918.0781, the g loss is: 2898.9688, the ae loss is: 0.0066297175, the jacobian loss is:0.0696378\n",
            "This is the iter 9904, the d1 loss is: 3131.7188, the d2 loss is: -3235.1875, the g loss is: 3202.7188, the ae loss is: 0.0073606186, the jacobian loss is:0.078034125\n",
            "This is the iter 9905, the d1 loss is: 3178.336, the d2 loss is: -3247.711, the g loss is: 3210.289, the ae loss is: 0.01057868, the jacobian loss is:0.1241811\n",
            "This is the iter 9906, the d1 loss is: 3407.8203, the d2 loss is: -3511.7266, the g loss is: 3465.125, the ae loss is: 0.0067539937, the jacobian loss is:0.065133326\n",
            "This is the iter 9907, the d1 loss is: 3164.539, the d2 loss is: -3228.375, the g loss is: 3216.664, the ae loss is: 0.006430909, the jacobian loss is:0.13179964\n",
            "This is the iter 9908, the d1 loss is: 2741.375, the d2 loss is: -2826.8594, the g loss is: 2881.1562, the ae loss is: 0.0070185685, the jacobian loss is:0.0760412\n",
            "This is the iter 9909, the d1 loss is: 3274.375, the d2 loss is: -3372.2969, the g loss is: 3359.711, the ae loss is: 0.0067322357, the jacobian loss is:0.056100987\n",
            "This is the iter 9910, the d1 loss is: 3189.4531, the d2 loss is: -3262.4531, the g loss is: 3263.25, the ae loss is: 0.0055907005, the jacobian loss is:0.068792135\n",
            "This is the iter 9911, the d1 loss is: 3038.5781, the d2 loss is: -3118.9375, the g loss is: 3108.9531, the ae loss is: 0.006480176, the jacobian loss is:0.07899068\n",
            "This is the iter 9912, the d1 loss is: 3205.9922, the d2 loss is: -3281.0625, the g loss is: 3284.8984, the ae loss is: 0.005139795, the jacobian loss is:0.06332065\n",
            "This is the iter 9913, the d1 loss is: 3191.2734, the d2 loss is: -3280.2812, the g loss is: 3215.7031, the ae loss is: 0.008270828, the jacobian loss is:0.080695465\n",
            "This is the iter 9914, the d1 loss is: 3151.461, the d2 loss is: -3230.5, the g loss is: 3207.7656, the ae loss is: 0.006746933, the jacobian loss is:0.059279267\n",
            "This is the iter 9915, the d1 loss is: 3169.539, the d2 loss is: -3279.0625, the g loss is: 3304.1953, the ae loss is: 0.0066080643, the jacobian loss is:0.07047712\n",
            "This is the iter 9916, the d1 loss is: 3113.0312, the d2 loss is: -3180.1562, the g loss is: 3183.3828, the ae loss is: 0.005347548, the jacobian loss is:0.078718975\n",
            "This is the iter 9917, the d1 loss is: 3076.9219, the d2 loss is: -3138.6875, the g loss is: 3216.0469, the ae loss is: 0.0062174886, the jacobian loss is:0.10349881\n",
            "This is the iter 9918, the d1 loss is: 2858.6484, the d2 loss is: -2956.4531, the g loss is: 2935.414, the ae loss is: 0.007437626, the jacobian loss is:0.06666959\n",
            "This is the iter 9919, the d1 loss is: 3081.5703, the d2 loss is: -3174.5781, the g loss is: 3130.1562, the ae loss is: 0.0070787575, the jacobian loss is:0.09483303\n",
            "This is the iter 9920, the d1 loss is: 3218.5312, the d2 loss is: -3307.6172, the g loss is: 3311.7188, the ae loss is: 0.0062274863, the jacobian loss is:0.09049396\n",
            "This is the iter 9921, the d1 loss is: 2878.9766, the d2 loss is: -2955.0234, the g loss is: 2991.1172, the ae loss is: 0.007905344, the jacobian loss is:0.1300659\n",
            "This is the iter 9922, the d1 loss is: 2959.2344, the d2 loss is: -3057.0469, the g loss is: 3036.7578, the ae loss is: 0.008018885, the jacobian loss is:0.07974532\n",
            "This is the iter 9923, the d1 loss is: 3234.3672, the d2 loss is: -3318.164, the g loss is: 3335.0938, the ae loss is: 0.007682389, the jacobian loss is:0.13519965\n",
            "This is the iter 9924, the d1 loss is: 3116.0078, the d2 loss is: -3195.4453, the g loss is: 3188.9375, the ae loss is: 0.009716219, the jacobian loss is:0.12027333\n",
            "This is the iter 9925, the d1 loss is: 3285.664, the d2 loss is: -3358.2812, the g loss is: 3407.3281, the ae loss is: 0.006729667, the jacobian loss is:0.06900364\n",
            "This is the iter 9926, the d1 loss is: 2960.8125, the d2 loss is: -3052.3438, the g loss is: 3106.086, the ae loss is: 0.005074124, the jacobian loss is:0.078967415\n",
            "This is the iter 9927, the d1 loss is: 3154.3438, the d2 loss is: -3240.125, the g loss is: 3221.6953, the ae loss is: 0.008260529, the jacobian loss is:0.122362964\n",
            "This is the iter 9928, the d1 loss is: 2901.9219, the d2 loss is: -3002.086, the g loss is: 2998.2188, the ae loss is: 0.006957419, the jacobian loss is:0.099887975\n",
            "This is the iter 9929, the d1 loss is: 3282.25, the d2 loss is: -3379.539, the g loss is: 3350.3828, the ae loss is: 0.0076951124, the jacobian loss is:0.10123149\n",
            "This is the iter 9930, the d1 loss is: 3053.7422, the d2 loss is: -3151.4062, the g loss is: 3149.7031, the ae loss is: 0.00954898, the jacobian loss is:0.07341734\n",
            "This is the iter 9931, the d1 loss is: 3184.3203, the d2 loss is: -3250.5625, the g loss is: 3243.0781, the ae loss is: 0.0063173324, the jacobian loss is:0.05931535\n",
            "This is the iter 9932, the d1 loss is: 3005.6094, the d2 loss is: -3093.3203, the g loss is: 3092.9688, the ae loss is: 0.0075245444, the jacobian loss is:0.10873953\n",
            "This is the iter 9933, the d1 loss is: 3088.0547, the d2 loss is: -3175.3594, the g loss is: 3161.5781, the ae loss is: 0.0048224796, the jacobian loss is:0.086423576\n",
            "This is the iter 9934, the d1 loss is: 3219.6406, the d2 loss is: -3301.7266, the g loss is: 3277.4844, the ae loss is: 0.008281222, the jacobian loss is:0.08523308\n",
            "This is the iter 9935, the d1 loss is: 3127.2266, the d2 loss is: -3221.375, the g loss is: 3231.1094, the ae loss is: 0.004771317, the jacobian loss is:0.108459964\n",
            "This is the iter 9936, the d1 loss is: 3126.9688, the d2 loss is: -3218.0156, the g loss is: 3268.7188, the ae loss is: 0.0073766564, the jacobian loss is:0.064561956\n",
            "This is the iter 9937, the d1 loss is: 3106.4297, the d2 loss is: -3198.4922, the g loss is: 3171.3438, the ae loss is: 0.0059016217, the jacobian loss is:0.08611031\n",
            "This is the iter 9938, the d1 loss is: 2912.9844, the d2 loss is: -2999.0703, the g loss is: 2963.9375, the ae loss is: 0.009120651, the jacobian loss is:0.092938475\n",
            "This is the iter 9939, the d1 loss is: 3011.9688, the d2 loss is: -3105.0234, the g loss is: 3087.8281, the ae loss is: 0.007235862, the jacobian loss is:0.07066529\n",
            "This is the iter 9940, the d1 loss is: 3154.461, the d2 loss is: -3235.9062, the g loss is: 3168.0938, the ae loss is: 0.00645397, the jacobian loss is:0.06838097\n",
            "This is the iter 9941, the d1 loss is: 3018.0469, the d2 loss is: -3104.6094, the g loss is: 3115.8828, the ae loss is: 0.005538664, the jacobian loss is:0.08026972\n",
            "This is the iter 9942, the d1 loss is: 2865.9922, the d2 loss is: -2969.75, the g loss is: 3009.8047, the ae loss is: 0.008039301, the jacobian loss is:0.07271219\n",
            "This is the iter 9943, the d1 loss is: 3199.6094, the d2 loss is: -3302.5469, the g loss is: 3258.7266, the ae loss is: 0.004661802, the jacobian loss is:0.06986201\n",
            "This is the iter 9944, the d1 loss is: 3153.1562, the d2 loss is: -3234.6562, the g loss is: 3249.9062, the ae loss is: 0.008652196, the jacobian loss is:0.07611682\n",
            "This is the iter 9945, the d1 loss is: 2812.9062, the d2 loss is: -2882.3281, the g loss is: 2933.836, the ae loss is: 0.0057886643, the jacobian loss is:0.08894844\n",
            "This is the iter 9946, the d1 loss is: 3168.5703, the d2 loss is: -3236.5703, the g loss is: 3277.8672, the ae loss is: 0.009042665, the jacobian loss is:0.12468258\n",
            "This is the iter 9947, the d1 loss is: 2959.9219, the d2 loss is: -3054.2422, the g loss is: 3094.9453, the ae loss is: 0.005360889, the jacobian loss is:0.086182564\n",
            "This is the iter 9948, the d1 loss is: 3164.1484, the d2 loss is: -3260.2656, the g loss is: 3260.4453, the ae loss is: 0.008486529, the jacobian loss is:0.080436334\n",
            "This is the iter 9949, the d1 loss is: 3123.7969, the d2 loss is: -3225.0234, the g loss is: 3219.3906, the ae loss is: 0.005292068, the jacobian loss is:0.07060438\n",
            "This is the iter 9950, the d1 loss is: 3149.086, the d2 loss is: -3234.9844, the g loss is: 3213.6719, the ae loss is: 0.0067885444, the jacobian loss is:0.07188696\n",
            "This is the iter 9951, the d1 loss is: 3425.7344, the d2 loss is: -3521.4062, the g loss is: 3510.2344, the ae loss is: 0.007161754, the jacobian loss is:0.11897975\n",
            "This is the iter 9952, the d1 loss is: 3099.3281, the d2 loss is: -3199.125, the g loss is: 3216.7734, the ae loss is: 0.009955242, the jacobian loss is:0.09302596\n",
            "This is the iter 9953, the d1 loss is: 3002.9688, the d2 loss is: -3097.539, the g loss is: 3137.5625, the ae loss is: 0.0070425724, the jacobian loss is:0.07632119\n",
            "This is the iter 9954, the d1 loss is: 3301.8125, the d2 loss is: -3392.6797, the g loss is: 3337.9531, the ae loss is: 0.006119748, the jacobian loss is:0.07732103\n",
            "This is the iter 9955, the d1 loss is: 3064.039, the d2 loss is: -3149.5625, the g loss is: 3149.8125, the ae loss is: 0.0076855067, the jacobian loss is:0.10934895\n",
            "This is the iter 9956, the d1 loss is: 2935.9453, the d2 loss is: -3033.789, the g loss is: 3017.875, the ae loss is: 0.004984243, the jacobian loss is:0.07048732\n",
            "This is the iter 9957, the d1 loss is: 3398.7422, the d2 loss is: -3495.7031, the g loss is: 3525.9766, the ae loss is: 0.008256538, the jacobian loss is:0.058224566\n",
            "This is the iter 9958, the d1 loss is: 3238.5703, the d2 loss is: -3325.1562, the g loss is: 3264.0625, the ae loss is: 0.005963673, the jacobian loss is:0.06987078\n",
            "This is the iter 9959, the d1 loss is: 3476.3125, the d2 loss is: -3555.5938, the g loss is: 3501.4844, the ae loss is: 0.007862616, the jacobian loss is:0.09428794\n",
            "This is the iter 9960, the d1 loss is: 3145.414, the d2 loss is: -3244.1953, the g loss is: 3249.4688, the ae loss is: 0.0059139817, the jacobian loss is:0.08212107\n",
            "This is the iter 9961, the d1 loss is: 3257.4531, the d2 loss is: -3355.5312, the g loss is: 3291.2266, the ae loss is: 0.0055019828, the jacobian loss is:0.07260932\n",
            "This is the iter 9962, the d1 loss is: 2957.125, the d2 loss is: -3041.8281, the g loss is: 3062.6406, the ae loss is: 0.0070407107, the jacobian loss is:0.06293671\n",
            "This is the iter 9963, the d1 loss is: 3179.2422, the d2 loss is: -3267.3281, the g loss is: 3315.6094, the ae loss is: 0.005560189, the jacobian loss is:0.06877748\n",
            "This is the iter 9964, the d1 loss is: 3274.5703, the d2 loss is: -3353.6797, the g loss is: 3410.2422, the ae loss is: 0.0059334943, the jacobian loss is:0.05133311\n",
            "This is the iter 9965, the d1 loss is: 3135.7266, the d2 loss is: -3224.8125, the g loss is: 3248.0, the ae loss is: 0.0058649317, the jacobian loss is:0.06827626\n",
            "This is the iter 9966, the d1 loss is: 3306.5078, the d2 loss is: -3405.8594, the g loss is: 3340.5469, the ae loss is: 0.004956287, the jacobian loss is:0.058699172\n",
            "This is the iter 9967, the d1 loss is: 3271.2734, the d2 loss is: -3375.4922, the g loss is: 3344.7422, the ae loss is: 0.005585673, the jacobian loss is:0.069126755\n",
            "This is the iter 9968, the d1 loss is: 3113.0078, the d2 loss is: -3209.711, the g loss is: 3221.0469, the ae loss is: 0.006874369, the jacobian loss is:0.12716758\n",
            "This is the iter 9969, the d1 loss is: 3217.0234, the d2 loss is: -3324.2812, the g loss is: 3249.875, the ae loss is: 0.008049016, the jacobian loss is:0.13000025\n",
            "This is the iter 9970, the d1 loss is: 3015.4375, the d2 loss is: -3109.0703, the g loss is: 3089.6875, the ae loss is: 0.008000929, the jacobian loss is:0.06799831\n",
            "This is the iter 9971, the d1 loss is: 2977.75, the d2 loss is: -3070.6406, the g loss is: 3041.1406, the ae loss is: 0.0061784647, the jacobian loss is:0.068297\n",
            "This is the iter 9972, the d1 loss is: 3188.4844, the d2 loss is: -3270.664, the g loss is: 3235.3047, the ae loss is: 0.0062062554, the jacobian loss is:0.08146869\n",
            "This is the iter 9973, the d1 loss is: 3108.5156, the d2 loss is: -3188.2734, the g loss is: 3161.6953, the ae loss is: 0.006297522, the jacobian loss is:0.06612106\n",
            "This is the iter 9974, the d1 loss is: 3249.3438, the d2 loss is: -3337.8594, the g loss is: 3280.6719, the ae loss is: 0.007908355, the jacobian loss is:0.079576105\n",
            "This is the iter 9975, the d1 loss is: 3178.3125, the d2 loss is: -3260.0078, the g loss is: 3349.8438, the ae loss is: 0.007445476, the jacobian loss is:0.07916479\n",
            "This is the iter 9976, the d1 loss is: 3079.0469, the d2 loss is: -3172.4062, the g loss is: 3180.6719, the ae loss is: 0.0076085962, the jacobian loss is:0.05557343\n",
            "This is the iter 9977, the d1 loss is: 3151.0781, the d2 loss is: -3251.3672, the g loss is: 3220.8516, the ae loss is: 0.0072022052, the jacobian loss is:0.06284257\n",
            "This is the iter 9978, the d1 loss is: 3168.7969, the d2 loss is: -3262.8125, the g loss is: 3217.2188, the ae loss is: 0.0078047104, the jacobian loss is:0.06972626\n",
            "This is the iter 9979, the d1 loss is: 3027.0312, the d2 loss is: -3111.4297, the g loss is: 3121.2344, the ae loss is: 0.006011885, the jacobian loss is:0.07141312\n",
            "This is the iter 9980, the d1 loss is: 3204.2188, the d2 loss is: -3309.336, the g loss is: 3286.8281, the ae loss is: 0.004533912, the jacobian loss is:0.06889006\n",
            "This is the iter 9981, the d1 loss is: 2915.4922, the d2 loss is: -2989.586, the g loss is: 3026.6875, the ae loss is: 0.010509111, the jacobian loss is:0.06714137\n",
            "This is the iter 9982, the d1 loss is: 3349.5781, the d2 loss is: -3438.9219, the g loss is: 3425.5547, the ae loss is: 0.006085458, the jacobian loss is:0.06786086\n",
            "This is the iter 9983, the d1 loss is: 2948.461, the d2 loss is: -3048.6328, the g loss is: 3041.7578, the ae loss is: 0.0067125917, the jacobian loss is:0.089321345\n",
            "This is the iter 9984, the d1 loss is: 3143.0, the d2 loss is: -3230.5547, the g loss is: 3283.9844, the ae loss is: 0.0062232, the jacobian loss is:0.057826318\n",
            "This is the iter 9985, the d1 loss is: 2986.0938, the d2 loss is: -3087.1328, the g loss is: 3025.211, the ae loss is: 0.0067068003, the jacobian loss is:0.09502356\n",
            "This is the iter 9986, the d1 loss is: 3072.6328, the d2 loss is: -3164.2656, the g loss is: 3086.0938, the ae loss is: 0.0051884484, the jacobian loss is:0.0703152\n",
            "This is the iter 9987, the d1 loss is: 3224.4453, the d2 loss is: -3333.2031, the g loss is: 3301.4844, the ae loss is: 0.006817577, the jacobian loss is:0.09303568\n",
            "This is the iter 9988, the d1 loss is: 3057.8125, the d2 loss is: -3154.961, the g loss is: 3134.9375, the ae loss is: 0.008177944, the jacobian loss is:0.11423732\n",
            "This is the iter 9989, the d1 loss is: 3086.5781, the d2 loss is: -3164.4297, the g loss is: 3165.8438, the ae loss is: 0.008180513, the jacobian loss is:0.07527577\n",
            "This is the iter 9990, the d1 loss is: 3085.1016, the d2 loss is: -3175.9922, the g loss is: 3261.1016, the ae loss is: 0.0066821324, the jacobian loss is:0.07306264\n",
            "This is the iter 9991, the d1 loss is: 3075.039, the d2 loss is: -3167.1484, the g loss is: 3244.6016, the ae loss is: 0.004719677, the jacobian loss is:0.110294335\n",
            "This is the iter 9992, the d1 loss is: 3101.6016, the d2 loss is: -3161.3906, the g loss is: 3289.9688, the ae loss is: 0.009203746, the jacobian loss is:0.07195918\n",
            "This is the iter 9993, the d1 loss is: 3162.5547, the d2 loss is: -3260.789, the g loss is: 3274.8594, the ae loss is: 0.005994496, the jacobian loss is:0.05442396\n",
            "This is the iter 9994, the d1 loss is: 3216.3906, the d2 loss is: -3313.6406, the g loss is: 3425.8438, the ae loss is: 0.0074243657, the jacobian loss is:0.09762107\n",
            "This is the iter 9995, the d1 loss is: 3039.0703, the d2 loss is: -3139.9453, the g loss is: 3153.3828, the ae loss is: 0.007178855, the jacobian loss is:0.0866906\n",
            "This is the iter 9996, the d1 loss is: 3185.3906, the d2 loss is: -3273.625, the g loss is: 3256.789, the ae loss is: 0.0075932597, the jacobian loss is:0.08275936\n",
            "This is the iter 9997, the d1 loss is: 3234.336, the d2 loss is: -3324.2969, the g loss is: 3217.289, the ae loss is: 0.0056545786, the jacobian loss is:0.07452251\n",
            "This is the iter 9998, the d1 loss is: 3084.1875, the d2 loss is: -3174.4922, the g loss is: 3144.0078, the ae loss is: 0.006470019, the jacobian loss is:0.16242756\n",
            "This is the iter 9999, the d1 loss is: 3194.0, the d2 loss is: -3308.0781, the g loss is: 3285.6797, the ae loss is: 0.0058983616, the jacobian loss is:0.09232576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBVWocd18_xU"
      },
      "source": [
        "aa=tape.gradient([fake_batch_p,fake_batch_q,fake_batch_vm,fake_batch_va],tf_real_batch_demand)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "cVtWsXIflal4",
        "outputId": "d6617a3c-5d06-4e5b-d366-48a407791cd1"
      },
      "source": [
        "print(tf_real_batch_demand)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-174ecea8f733>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_real_batch_demand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf_real_batch_demand' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "r_KdLvkjjMHY",
        "outputId": "e1cd69f5-8d7e-49f9-d7d9-925ac40a2144"
      },
      "source": [
        "#What I am doing now is to adjust the discriminator, the dimension has problem\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-374e9cca62e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#What I am doing now is to adjust the discriminator, the dimension has problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'discriminator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYG8hSy-fR_c",
        "outputId": "93bb1a1a-b1ca-4ceb-e653-b07d29bf135e"
      },
      "source": [
        "K.mean(pb).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.818477"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "3wDASHM9KpZ5",
        "outputId": "349f9df1-8b2b-429c-9798-f70d2ef9d48b"
      },
      "source": [
        "plt.plot(gp_list)\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "XB8cN3-G_llQ",
        "outputId": "5a202920-81b0-4636-ddde-ef3fca249f6a"
      },
      "source": [
        "plt.plot(dy_dx_list[:10000])\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "5HMvSiKm_pKW",
        "outputId": "c8edf99b-7037-471a-d63b-714a3ab2213f"
      },
      "source": [
        "plt.plot(ae_list[:])\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "Fihxsh1Thihb",
        "outputId": "1414d3d4-3e88-4639-c599-3abaca9b8aa6"
      },
      "source": [
        "#plt.plot(loss_g_list)\n",
        "lst = np.array(loss_d1_list)\n",
        "plt.plot(list(-lst))\n",
        "plt.plot(loss_d2_list)\n",
        "plt.legend(['D_loss_on_real_data','D_loss_on_fake_data'])\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('loss value')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "UZEv2i-uHZTS",
        "outputId": "c29ea22e-0631-4b18-bbf1-1a1d2cb58a50"
      },
      "source": [
        "plt.plot(loss_g_list)\n",
        "plt.legend(['Generator_loss'])\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('norm value')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'norm value')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8zF8ljWUo8T",
        "outputId": "c5a40320-944b-4e23-d6f5-962189086837"
      },
      "source": [
        "generator.save('/content/drive/MyDrive/Phd/opf_gan/118_generator.h5')\n",
        "discriminator.save('/content/drive/MyDrive/Phd/opf_gan/118cgan_discriminator.h5')\n",
        "qnetwork.save('/content/drive/MyDrive/Phd/opf_gan/118cgan_qnetwork.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMUWpJnvrOPe",
        "outputId": "47bd3806-c2fe-40ab-a7de-85228c6128c5"
      },
      "source": [
        "K.mean(qb)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.08903221>"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGCiTvLxsg8P",
        "outputId": "eefa44bd-f46c-47ff-87fa-6940b63609c7"
      },
      "source": [
        "K.mean(pb)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.025738597>"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD5uNXfpTuB2"
      },
      "source": [
        "pp_list=[]\n",
        "for p in pbalance_list:\n",
        "  blc=K.mean(p)\n",
        "  pp_list.append(blc)\n",
        "qq_list=[]\n",
        "for q in qbalance_list:\n",
        "  blc=K.mean(q)\n",
        "  qq_list.append(blc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "_XbP4-oZTew4",
        "outputId": "75d6ea35-9fef-4e6b-8eeb-771c04038ffb"
      },
      "source": [
        "plt.plot(pbalance_list[:100])\n",
        "plt.plot(qbalance_list[:100])\n",
        "plt.legend(['active_balance','reactive_balance'])\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('loss value')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyL81mm33PhG",
        "outputId": "4c08caa4-6b18-4e0d-9c7e-e75660315cdc"
      },
      "source": [
        "min(qbalance_list[:])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.27055508"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-CjyGXS_esM",
        "outputId": "6c5ef2ff-ef8c-4ddc-adab-831c5b84e5db"
      },
      "source": [
        "np.mean(q_balance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.993875"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "Je1lkVOU_Tpc",
        "outputId": "39fe11c7-2ed1-40fe-b484-08219e0d49fe"
      },
      "source": [
        "P_out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-ad9875994460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mP_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'P_out' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcjud32b0QMi"
      },
      "source": [
        "#def feasibility_verification(Y_bus,vm,va):\n",
        "Y_bus_tf = tf.convert_to_tensor(Y_bus)\n",
        "v_r = tf.math.multiply(vm,tf.cos(tf.math.multiply(va,tf.constant(math.pi/180,dtype='float32'))))\n",
        "v_i = tf.math.multiply(vm,tf.sin(tf.math.multiply(va,tf.constant(math.pi/180,dtype='float32'))))\n",
        "V = tf.reshape(tf.complex(v_r,v_i),[batch_size,3,1])\n",
        "vj_vi = V-tf.reshape(V,[batch_size,1,3])\n",
        "y_vj_vi = tf.math.multiply(Y_bus_tf,vj_vi)\n",
        "v_expand = tf.repeat(V,3,axis=2)\n",
        "s_ij = tf.math.multiply(v_expand,tf.math.conj(y_vj_vi))\n",
        "S_in = tf.reduce_sum(s_ij,axis=2)\n",
        "P_in = tf.math.real(S_in)\n",
        "Q_in = tf.math.imag(S_in)\n",
        "I = tf.matmul(Y_bus_tf,V)\n",
        "#S_out = tf.reshape(tf.math.multiply(V,tf.math.conj(I)),[batch_size,3])\n",
        "#P_out = tf.math.real(S_out)\n",
        "#Q_out = tf.math.imag(S_out)\n",
        "P_out = p_bus_tf\n",
        "Q_out = q_bus_tf\n",
        "\n",
        "first_constraint = tf.reduce_mean(tf.square(P_in+P_out),axis=0)\n",
        "second_constraint = tf.reduce_mean(tf.square(Q_in+Q_out),axis=0)\n",
        "#return p_balance,q_balance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crBCX49tLmRR"
      },
      "source": [
        "vm_gen_fake6=np.zeros([generated_size,5])\n",
        "vm_gen_fake6[:,0] = fake_vm[:,1]\n",
        "vm_gen_fake6[:,1] = fake_vm[:,21]\n",
        "vm_gen_fake6[:,2] = fake_vm[:,26]\n",
        "vm_gen_fake6[:,3] = fake_vm[:,22]\n",
        "vm_gen_fake6[:,4] = fake_vm[:,12]\n",
        "\n",
        "fake_p_6 = np.zeros([generated_size,6])\n",
        "fake_p_6[:,1] = fake_p[:,1]\n",
        "fake_p_6[:,2] = fake_p[:,21]\n",
        "fake_p_6[:,3] = fake_p[:,26]\n",
        "fake_p_6[:,4] = fake_p[:,22]\n",
        "fake_p_6[:,5] = fake_p[:,12]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozeL4Tv2WgZ2"
      },
      "source": [
        "pd.DataFrame(fake_demand[:,0:30]).to_csv('p_demand_test.csv')\n",
        "pd.DataFrame(fake_demand[:,30:]).to_csv('q_demand_test.csv')\n",
        "pd.DataFrame(fake_p_6).to_csv('p_supply_test.csv')\n",
        "pd.DataFrame(vm_gen_fake6).to_csv('vm_gen_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZFDAo_ig7AQ",
        "outputId": "5f614649-334f-4f2e-a78d-94508e817b39"
      },
      "source": [
        "np.sum(fake_demand[:,0:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.9265641222201484"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOUaxMf1WiY-",
        "outputId": "61ff6005-8701-4a18-d8ce-19a795273ba8"
      },
      "source": [
        "fake_p"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.49755725, 0.5038785 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.1830803 , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.29379764, 0.0864872 , 0.        , 0.        ,\n",
              "        0.        , 0.3854256 , 0.        , 0.        , 0.        ]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYT6c4qlhxLe",
        "outputId": "bc413e37-b40a-4a65-c881-313632110d9c"
      },
      "source": [
        "fake_q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.07549758, 0.15891723, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.2660091 , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.5072156 , 0.15104723, 0.        , 0.        ,\n",
              "        0.        , 0.2731502 , 0.        , 0.        , 0.        ]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOCfroVskS_k",
        "outputId": "515fbcf9-a057-4cfd-cba9-b29c608e38d8"
      },
      "source": [
        "opf_real_conditions_validation[0,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.        , 0.2247287 , 0.02861889, 0.06342201, 0.        ,\n",
              "       0.        , 0.20798738, 0.24388678, 0.        , 0.05189385,\n",
              "       0.        , 0.09081932, 0.        , 0.07285945, 0.06689444,\n",
              "       0.03501143, 0.10266188, 0.02595863, 0.09509666, 0.02374388,\n",
              "       0.17385561, 0.        , 0.03296214, 0.09885429, 0.        ,\n",
              "       0.02855109, 0.        , 0.        , 0.02501366, 0.08579894,\n",
              "       0.        , 0.10438835, 0.01160139, 0.01670866, 0.        ,\n",
              "       0.        , 0.10647208, 0.2823335 , 0.        , 0.01905765,\n",
              "       0.        , 0.08904267, 0.        , 0.01729766, 0.02900119,\n",
              "       0.01551684, 0.05661812, 0.0078438 , 0.03360168, 0.00786441,\n",
              "       0.09881533, 0.        , 0.01308805, 0.06942913, 0.        ,\n",
              "       0.01911108, 0.        , 0.        , 0.0105829 , 0.01575471])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_qX1SoxkCIU"
      },
      "source": [
        "sample_result = np.zeros([1000,30])\n",
        "# generate points in latent space\n",
        "#n_samples = opf_real_conditions_validation.shape[0]\n",
        "for ii in range(0,1000):\n",
        "  z_input, labels_input = generate_latent_points(100, 2)\n",
        "  # predict outputs\n",
        "  images = generator.predict([z_input, opf_real_conditions_validation[0:2,:]])\n",
        "  sample_result[ii,:]=images[0][0,:]\n",
        "  print(ii)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbLhcwSmdlfA",
        "outputId": "135ff761-690a-4996-f243-60c8c82a1700"
      },
      "source": [
        "generator.save('/content/drive/MyDrive/Phd/opf_gan/gen_118_113c.h5')\n",
        "discriminator.save('/content/drive/MyDrive/Phd/opf_gan/dis_118_113c.h5')\n",
        "qnetwork.save('/content/drive/MyDrive/Phd/opf_gan/q_118_113c.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    }
  ]
}